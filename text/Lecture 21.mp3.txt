{"text":"Welcome to operating systems lecture 21. Last time we were looking at concurrency problems and we looked at this code which inserts an element data into a list. Let's say this is a linked list. So the way this works is you declare a local variable, you allocate some node or type list from the heap, you set its data, you set its next pointer to the list that exists, that's given as the first argument and you update the list. So you are basically appending the data in the front of the list. Now and we said that look, these two statements, let's call this statement A and let's call this statement B. These two statements need to execute in an atomic fashion. So while they are executing, they should not get interrupted or nobody else should be touching this list while in the middle. So it shouldn't happen that in the middle of this, another thread starts executing insert. If it does that, then bad things can happen. And the bad thing in this case would be that the list structure is no longer preserved. So for example, we saw last time that if it so happens that there are two threads, T1 and T2, one thread executes A1, the other thread executes A2 and let's say it executes B2 after that and then this one executes B1, then what can happen? You had a list and thread one will insert its data, let's say this is B1, data one and thread two will insert its data to the same list and you will have something like this. So the list will be pointing here and this is no longer a list now. So bad things like these can happen. Similarly, any other schedule which has A1, A2, A1 and B1 getting interrupted and another insert getting called on that list, bad things can happen. So another bad schedule is let's say A1, A2, B1, B2 or anything of this sort. So these are all bad schedules and these basically corrupt your data structure or violate your invariance. So what did you want? You really wanted that A1 and B1 should execute in a sort of non preemptible way or in an atomic way. So if it's possible that while A1 and B1 are executing, no other call to insert should start or no other call to A and B should start. So A2 should either be here or A2 and B2 should have been before A1 and B1. That's what you want. So in other words, you want to say that these two statements A and B should execute atomically and they should execute atomically with respect to something, with respect to themselves. So there can be other code that's running simultaneously but as long as that code is not touching this list, it's okay. But if that code is likely to touch this list, then it shouldn't be. That code and this code should be atomic with respect to each other. And so we said that there's an abstraction called a lock that we're going to discuss today. So when we are designing such an abstraction, what do we need to take care of? Firstly, in this function, these are the only two statements that need to be atomic. It doesn't matter what happens here. So it's only A and B that you need to protect. So you need some way of specifying that this is the area that's atomic. You don't need to make the entire function atomic, number one. Number two, it will be nice if you could say that operations on this list need to be atomic with respect to each other but operations on list one and operations on list two do not need to be atomic with respect to each other. So insert on list one can happen concurrently with operations on insert on list two. It doesn't matter. So you would like to have that kind of flexibility that allows better concurrency and better performance in general. And thirdly, it may not be just this function insert. So insert should be atomic with respect to each other or these two statements should be atomic with respect to themselves but these two statements may need to be atomic with respect to some other statements. Let's say there's another function called delete. So if delete and insert are happening concurrently, then similar situations can happen where the list becomes corrupted. So it's not just these statements being atomic with respect to themselves. These statements may need to be atomic with respect to some other code in your system. So one has to design an abstraction that allows you all these different flexibilities. So many different abstractions exist to solve this problem. And as you go along in this course or in future courses, you're going to come across different kinds of abstractions. Different programming languages have different abstractions. C will have a different kind of... so Java or something like that would have different kind of support and different languages will have different kind of support. But locking or locks are one of the most common types of abstractions used to solve this problem. So what is a lock? So a lock is defined by a type. Let's say the type is struck lock or any other type. There should be a type that says it's a lock and there should be two functions defined on this lock. One is called acquire, which basically represents that the lock has been acquired. So I'm going to discuss what that means and release. So this variable, this structure lock is a stateful structure and it represents two states. Either it is locked or it is unlocked. The other way to say it is either it is acquired or it is released. So lock has one bit state which says whether it's locked or not. The function acquire changes the state of that lock from unlocked to locked. But the precondition being that the state should have been unlocked earlier. So if it is already locked then acquire will just wait till it becomes unlocked and as soon as it becomes unlocked, it will turn it to locked. So that is semantical acquire once again. Acquire will set this state of this lock to acquired state or locked state but it requires that earlier it should have been in the unlocked state or released state. If it is already locked then it will keep waiting till it becomes unlocked and after it has become unlocked, it will set it to locked and release will just make set it to unlocked. So that's basically it and let's assume that these functions are you know are implemented in some way. How they are implemented we are going to discuss this later but so what happens is if two threads try to acquire the same lock simultaneously and let's say initially the lock was unlocked. So let's say initially the lock is unlocked and two threads try to acquire this lock simultaneously, only one of them will be able to lock it and the second one will keep waiting. So that's abstraction. The second one will keep waiting till what time? Till the first one called release. So in other words there is an invariant. The invariant is only a lock can only be acquired by at most one thread at any time. So only one thread could have acquired the lock at any time. The second thread will have to wait. Only out of the first letter that five threads or n threads then other n minus one threads will have to wait. Only when the first thread called release somebody else is going to get a chance to get the lock. So what is it doing? It's basically doing if multiple threads call acquire then it's serializing these threads. It's saying multiple threads try to call acquire. So if multiple threads let's say call acquire and let's say they come at the same acquire point. From here on let's say only one thread will get to run, only when it calls release then the other thread will get to run, when it calls release the third thread is going to run. So it has serialized the execution but it has serialized the execution only for the region which was within that acquire and release calls. So let's see what will happen in this case. So we had this code, we had this function called insert that inserts the data into a list and we were not happy with the fact that this code could be executed concurrently by multiple threads. So what do you want? You want to serialize the execution of this code. So one way to do that is define a lock and initially let's say the state of the lock is unlocked and just put lock calls to acquire the lock and release the lock. So I think there is, let's call it lock. So acquire lock and release lock. So this makes the code correct because if there is one thread it will come here, the first thread will be able to acquire it. While it is inside this another thread who tries to execute the same code will not be able to acquire the lock. So you have to wait. So you have disabled concurrency in this region. So this region which is protected by locks is also called the critical section. So critical section is a region of code that needs to be serialized in its execution and the locks are basically allowing it to get serialized. Yes, question. Right, okay, good question. So what are these concurrent threads which I am talking about? Are these on the same CPU or are these on different CPUs? Same? Okay, 10-15 people. Different? Right, actually it doesn't matter. It can be on same CPU, it can be on different CPU. Let's look at how threads work, right. So let's look at what happens. Let's say there is one CPU and let me just draw the diagram that we have. This is the bus and let's say this is the memory. In the memory there is a list and what is happening was that this CPU was trying to update the list and while it was in the middle of updating the list, it is possible that a timer interrupt comes, right and the thread that was executing gets preempted or it gets switched out and another thread gets to run in which case what will happen is while I was executing at this line, an interrupt comes, I get context switched out, the other thread gets to run and I have the same problem as earlier, right. So this problem exists even with one CPU. If you have multiple CPUs, the problem still exists. It maybe becomes a little bit, you know, the probability becomes higher because it is possible that one thread is executing here and another thread is executing on this CPU. They are both trying. Now there is no interrupt involved, right. So there is no interrupt involved but both these threads are trying to simultaneously access list and the same thing, the same problem can happen. So the problem is really about interleaving. The interleaving can happen either because of switching out on the same CPU or the interleaving can happen either because of physical concurrency of physical CPUs on physical memory, right. In either case, the nature of the problem remains the same, right. Okay, I have a question for you. Does the problem exist for user level threads or does it only exist for kernel level threads? It exists for both, right. It does not matter whether it is user level thread or kernel level thread. It exists for both because at the end of the day, the abstraction is that a thread can be switched out any time and another thread can get to run. At every instruction boundary, a thread is potentially switchable, all right. So it does not matter, okay. All right, question. Okay so now, so let us say what happens with the lock, right. So we said there is a list living in memory. Now in our new abstraction, we also have this extra variable called lock living in memory, all right. And now one of these, both these CPUs, if both these CPUs try to acquire the lock at the same time, somehow this acquire function has been implemented such that only one of them will succeed, the other one will start have to wait, all right. How it is implemented, we are going to discuss in a minute. But let us say that there is this magic function called acquire that allows us to do it, okay. All right, so assuming that there is such a function, you know, life becomes easier. I can put a lock around my critical section, all right. In fact, notice that this abstraction allows you to say that this is the critical section. I do not need to say the entire function is a critical section. So I can, you know, I can just protect this area and these lines can execute concurrently. So I do not need to, you know, impede performance on the other lines. The other thing is, if let us say there is another function called delete and delete is going to do some manipulation on the list also, then what do you need to do? Wherever you are doing those manipulations, you need to use the same lock there, right. Then you make sure that you are using the same lock in both, in certain delete. So that way you can prevent, you know, you can make sections of code atomic with respect to each other by using the same lock around those critical sections, right. Thirdly, the third thing I said was, I may, I may want to say that look, this list and this list do not need to be mutual, do not need to be aware of each other. They can proceed concurrently. So in this case, you know, am I preventing that? So if I insert in one list and if I insert in another list, will they get serialized? Yes, right. They will get serialized because I have one lock and I am calling insert on L1 and I am calling insert on L2. The same lock is going to get taken and so only one of them is going to go forward. Even though the lists were different, I did not need to serialize, but it will get serialized. Is it wrong to over serialize? It is not wrong. Program remains correct, but performance gets affected, right. So what would have been a better way? Better way would have been, let us have a lock per list, right. Let us say I have, you know, let us say I have an array of locks, right. Or better still, in the list structure itself, I have a lock, right. So instead of saying acquire lock, I say acquire list dot lock or something, right. So in the list structure itself, I have a lock. So this is a lock for this list. So if you are manipulating this list, take this lock. If you are manipulating that list, take that lock. But these two locks are separate locks, so you can do these concurrently, right. So that is the nice thing about locks. They give you a lot of flexibility, right. Okay, now let us see how locks are implemented. So what do we need to implement? We need to implement a structure called lock, right. And let us say I implement it by having a field inside the structure which says that I am locked or not. I said this is a stateful abstraction. It basically has one bit of state, it says that I am locked or not. So let us say this is the state. Can we have a lock per subset of the data structure? For example, if I have a tree, then I want to only lock one subtree and not the other side of the tree. Is it possible? Yes, it is possible. Okay, how it is done, I mean, we are going to see as we go along, all right. So let us see void acquire struct lock for L, right. So I need to implement this. What is one way to implement it? Well, one very simple way is just check, right. So just check is L dot locked is equal to zero. If it is zero, then very good, just check L dot locked is equal to one and return, right. But if it is not zero, then keep waiting. So let us say to wait, I just put a while loop here. So basically I check if L dot locked is equal to zero. If it is zero, then actually let us say not equal to zero, right. So while L dot locked is not equal to zero, I keep spinning. As soon as I find it to be equal to zero, I set it to locked, right. But if it is not equal to zero, it means it is already taken. I cannot take it. I need to wait, right and the way I am waiting is I am just checking it over and over again in a while loop, right. And how do I implement release? Let us say L dot locked is equal to zero, right. Release is simple. I do not need to do any waiting. I just need to revert the state back to zero, right. So that is acquire and that is release. Is this correct? Okay. Alright, so the question is how do we define the preference, right. So preference, let us say there are two threads. One of them is calling insert, the other thread is calling delete. Who should get first, right? Well, the order can be completely arbitrary, right. The lock abstraction does not say that you know does not give an order. In general, you would just from a performance standpoint, what is the best policy? Well, one big one good policy is first come first served. Whoever comes first, give him the lock, alright. Whoever comes second, you know has to wait irrespective of what function you are executing. Then things would not be predictable from what will happen in the sense of way. Yes, so there is still some non-determinism. Even after you have locks, it is possible that this happens, right. Thread 1 executes before thread 2 or it is possible that this happens, thread 2 executes before thread 1 and both of them will result in different final values of your memory, right. So this is called non-determinism. There is still some non-determinism in your system. It depends on who gets there first but the important thing is it is correct. It is not wrong. Earlier it was wrong because I was allowing this and this is wrong. It was corrupting my data structure. This was right, this was right and I am allowing both of them and that is there is still some non-determinism in your system, right. In general, it is a very interesting thing that systems need to be non-deterministic for performance, right. It is a very fundamental thing. If you make a system very deterministic, you lose performance, right. So let us take an example. Let us say I wanted to make my system deterministic. So I say oh you know this lock will always be taken in strict round robin order. For thread 1, I know how many threads there are. Let us say there are two threads. So first I will give it to thread 1, then I will give it to thread 2 and so on, right. Thread 1, thread 2, thread 1, thread 2. So if thread 2 reaches first, then I just have to make it wait even though thread 1 is lagging behind for whatever reason. Let us say thread 1 is executing on a slower CPU. Let us say thread 1 did not get enough time to execute while I was doing it. So for all these or any other reason, thread 1 in general has more code to execute than thread 2, right. All these are possible cases. In either case, if thread 1 is lagging behind thread 2 and thread 2 reaches the lock first, if you want to have complete determinism, then you will have to make thread 2 wait which is just waste, right because there is nobody who is actually executing in that region. You could have executed it correctly. You could have let thread 2 execute first but you have to make it wait. So non-determinism allows performance and lock abstraction in general does not thwart non-determinism. It allows you to provide correctness but there is still some non-determinism in your system, okay. Okay, is it possible that one thread acquires the lock while the other thread releases the lock. So is it possible for a thread to release a lock without acquiring it? It is possible but you know we are assuming that because they are threads, they are trusting, mutually trusting each other, right. So it is the programmer's mistake that he has you know he has called release without acquiring the lock. It is his problem. He will pay for it by some error in his program unless he is doing it very carefully, all right. So notice that because we are operating in the threads environment, we are basically saying that we are trusting other threads. So threads must acquire and release for put around all critical sections appropriately. They should not call acquire twice without releasing. One thread should not call acquire twice without releasing. Release should not be called without calling acquire and all that. So these are all invariants that a programmer needs to maintain. So the headache is programmers, all right. So firstly a lock cannot be acquired by multiple threads simultaneously, right. A lock has to can be acquired only by one thread at a time, right. So it is not a list of threads that have acquired the lock. It is only one thread that could have acquired the lock at any time, all right and the other thing is when you are using the lock, it is better to not rely on the implementation of the lock so that you can change the implementation underneath. Just look at the abstraction. The abstraction is there is a structure called lock and there are these two functions acquire and release. Do not make assumptions about how they acquire and how they release are implemented. In general that is how it is done so that you know the implementation of the abstraction can be changed later at will, all right. So this is the standard practice that you will rely on the abstraction. Okay, let us come back to this implementation. Is this implementation correct? Yeah, go ahead. Can I context switch while I am holding the lock? Yes, right, all right. Okay, so yeah very interesting question. If I have a lock, I have taken a lock, I am in the middle of a critical section, a context switch happens, another thread gets to run. If that thread also tries to take a lock, take the same lock then you will just have to wait, all right till the next context switch and there is some wastage of performance. Absolutely true. How these things are handled, let us talk about it in some time. Actually this question about whether a context switch can happen while you have taken a lock or not really depends on your implementation of how you have implemented the lock, all right. So but in any case it is correct, right. So let us talk about correctness first and then we are going to talk about performance. Okay, good. So you are talking about a problem in this implementation, yes. So everybody understands there is a problem in this implementation? Right, okay. The problem is one thread comes here, it sees that lock is equal to 0, it is here, a context switch happens or forget about a context switch, let us say it is a multiprocessor system, another thread also comes here and both of them are here, both of them take the lock, you have broken your abstraction, right. The abstraction is no longer true. So what did you want? You wanted that this entire thing should have been atomic, right. So what have I done? I have just, you know I had some critical section that I wanted to make atomic. I said I am going to use locks to protect that critical section but now I have changed the problem to say now I have to implement this lock but now to implement the lock also I need some atomic invariant, right. So I have not really made any progress except I made some progress in the sense that I have reduced the critical section from that large area which can be arbitrarily large to this small thing here, right. And now the question comes how does one implement this in an atomic fashion, right. Okay, all right. So the answer is you need hardware support, okay. So there should be some instruction in the hardware that allows you to read and write a variable in an atomic fashion, right. So I am going to take the example of x86. On the x86 there is this instruction called exchange, right and exchange takes two operands, a register and a memory address, okay. So for example I could say and the semantics are that it is going to exchange. So this is exchange instruction and it will exchange the contents of register and memory, all right. So if you know let us say initially, so before r is equal to 0, m is equal to 1, then after you will basically see r is equal to 1 and m is equal to 0, right. So that is the semantics of the exchange instruction. Notice that this instruction is doing two memory accesses in one instruction, right. It is reading the old value of m and it is writing the new value of m. So it is both reading and writing in one instruction, right. So that is important. So in that sense this instruction is slightly special because it needs to read and write memory. So it needs to make two memory accesses in some sense, all right, okay. And it is possible to make an instruction atomic in hardware. So on x86 there is this lock prefix. So if you say lock exchange rm, then this instruction will atomically read the value of m and overwrite it with the new value, such that no other CPU can intervene in the middle, right. So what does it mean that if let us say there is CPU 0 and CPU 1 that both execute exchange on address let us say 100, right, and let us say I am saying so this is address and this is value, let us say r, and let us say r was equal to 1 and this one is saying exchange to the same address and let us say this was 0, then either this will happen before this or this will happen after this. It is not possible that the two reads and writes of one instruction get interleaved, right. So for example, right, so let us say initially the 100th location had value minus 1, then and let us say CPU 0 executes before, so minus 1 comes to r and 1 goes to m, right. So this was initial of, right, so let us look at this again in a bigger context, right. So let us say initially m is equal to minus 1. If CPU 0 executes, then m will become 1 and if CPU 1 executes after that, m will become 0, right, and the final value will be 0. On the other hand, if CPU 1 executes first, then it will become 0 and CPU 2 executes later, then it becomes 1, right. What is not possible is that they get interleaved, right. So what can happen if they get interleaved? Both of them read the old value of m and so in this case you know CPU, so r0 would be equal to minus 1 and r0 will be equal to 1 in case of CPU 1, right. In this case r1 will be equal to minus 1, so r0 of CPU 1 will be equal to minus 1, let us not call it r0. Let us say r is equal to minus 1, r is equal to minus 1, r is equal to 0, right. So these are the only two possibilities. If two threads, two CPUs try to execute exchange, they are atomic with respect to each other. What cannot happen is that both of them read the old value of m. So it is not possible that CPU 0's r is equal to minus 1 and CPU 1's r is also equal to minus 1, not possible, right. This could have happened if both of them read the old value, right and then they try to overwrite the new value and so in that case exchange would not have been atomic with respect to each other, right. Is this clear? Basically what I am saying is that the exchange operation itself is atomic with respect to itself and with respect to any other operation on that particular memory array, okay, all right. So with that let us see how I am going to implement acquire. I am going to say void acquire and I am going to you know use some register, let us say I will use register A X and I am going to put a value called 1 in the A X and I am going to do exchange A X with L dot locked, whatever the memory address of L dot locked and I am going to do it in atomic fashion. So I use a lock prefix here and then I am going to check if A X is not equal to 0, do what, pin, right else return, right. So I have taken the old value of lock into A X and put the new value of lock A X into lock. So in one instruction in one go I have read the old value and I have put the new value into lock. If the old value was 0 which means it was unlocked then I am done. If the old value was 1, it means it was already locked. So I did not I did not change the value of lock either, so I just keep retrying this, is this clear. So this is my register, I have set it to 1, I am trying to push this value 1 into lock but I am but I am also making sure that the old value of lock should have been 0, right. So that is what I am checking here. If the old value of lock was not 0 then it basically means that the lock was already locked, so I keep trying again. So what is the difference between the old code and the new code? In the new code I am using this exchange instruction to read and write the value of lock in one go, right. So if there are two threads which are trying to acquire this lock at the same time, one of them is going to hit the exchange instruction first, whoever hits it first is going to get the lock and is going to return. The second one is going to see the value of lock to be 1. Yes, okay, so yeah good question. The thread which is not able to acquire the lock will just keep executing exchange operation again and again, isn't it wasteful. The answer is it depends and I am going to discuss but let us first talk about correctness, right. So how many understand this implementation? Okay, so everybody, great. So good, so you just check if it was already locked and this is correct, right because you have made sure that you are reading and writing this lock variable at once. Okay, good, right. So as you have pointed out, one CPU will keep trying and other CPU will fall through. So the one CPU that is trying is waiting and it is waiting in a loop, all right. So it is also called spinning, right and this type of a lock implementation is called a spin lock. By the way, how will you implement release? The previous one is okay, right. It doesn't matter, just set it to 0 and that is fine. Okay, so release is fine. Acquired, only acquired needs to change, release was okay, right and this is called a spin lock, right. Okay, so in this example what will happen? If two threads come on acquire, one of them is going to start spinning, the other thread is going to start running this code. Only when the first thread releases, the second thread which was spinning here is also going to get the lock and the second thread is going to do it. So you basically serialized this operation of A and B and you are basically doing it using the exchange instructions, right. So now there is a question, isn't spin wasteful? Okay, so what is the alternative? All right, okay, so there is one answer which says can we disable interrupts? Would that be better just disable interrupts and then enable it at release. So disable interrupts at acquire, enable interrupts at release. Okay, all right, so here is one suggestion. The suggestion is in this code here, right enable disable interrupts here, so call ply here, CLI clear interrupts and then re-enable interrupts here, right. So on x86 the instruction is tie, set interrupts, right. So call CLI before and call tie after and that should that should solve the problem. Okay, so the one answer is this will solve the problem only if there is a single processor. If there are multiple processors, it does not solve the problem, perfectly fine, right. Okay, that is that is perfectly valid answer. Let us assume there was a single processor, is this correct? I have cleared the interrupts, I am spinning and let us say log was equal to 0, it was equal to 1. So I am just waiting for it to get 0. If there is just one processor, will it ever become 0? No, because I have cleared the interrupts, right. Timer interrupts cannot occur and so you know what is the hope? I am just basically dead locked, right. So that is not correct. Secondly you know a user program can definitely not do CLI and STI, right. These are privileged operations. So this this privilege is only for the kernel but even for the kernel this implementation is not correct. What could have been correct? Well if you really want to do it in this way, one way to do it is inside this loop of while right. So just do STI and CLI here, all right. So you are just disabling, enabling, disabling, enabling interrupts so that there is another thread who gets a chance. At least there is no dead lock, right. This is actually a valid implementation of a lock on a uniprocessor. It is not not a very very nice implementation, right. This is also equally wasteful, right. Okay and it does not work on a multiprocessor, all right. But in either case, in both cases we are doing spinning, right. What is another what is another way? Yield, yes, good. So the other way is yield. So if one thread has figured out that it is not getting the lock, you know I have checked the value of the log variable. It is zero. It is one. It needs it knows that you have to wait. Why do you, why do you, why are you still holding on to CPU, right. So one way to do it is you could say that okay in that loop just call yield, right and so yield is going to sustain you and so that way you may let other processes run. But is it really a better way? Let us look at this example, right. In this case, the critical section is really small. It is just two, two C statements which may be you know maximum of ten instructions, all right or let us say maximum of you know 100 to 500 nanoseconds to execute these two statements, right. The other thread, how long will it have to spin? It will probably have to spin for around that time which is you know 100 to 500 nanoseconds. How long will it take to actually call yield? Let us say a thousand or two, you know thousand instructions which is much larger. It is much better to just wait for ten instructions rather than call yield which is going to actually change my process structures and all that and it is going to cause much better. So in this case, it is actually better to use spin lock. In fact, yielding is wasteful, right. With yield, there is a cost to yield. Imagine if there were two threads that try to acquire the lock, one thread got it, the other thread called yield. Now this thread will very soon release it, right and the second thread should have actually now got the lock but now it is in this separate code path where it is calling yield and then later it will get scheduled after you know the next time interrupt or whatever and so it is wasteful. Spinning would have been better. On the other hand, if the size of the critical section was large, you know let us say you were executing ten thousand instructions or if you were making some external access like a disk access or some other slower device. If your critical section is large in time, then it would be better to call yield because for that amount of time, you are just unnecessarily holding up the CPU, right. So isn't spin wasteful? The answer is depends and depends on what? On critical section size, right and basically the logic you want to use is if the critical section is likely to be large, then use yield, right and the lock which uses yield is basically called a blocking lock. In fact, instead of using yield, you could actually also use block, right. So what is block? Block basically just says that I want to block myself because I know that this lock is not available. So yield just says, yield just puts you in the runnable state which means on the next scheduling quantum, you will again get to run. Block basically tells the operating system that I am not even in the runnable state, right. I am in the block state which means do not even schedule me and at the release time, the thread can say look at all the block threads and make them runnable, right. So that is another way. One is yield, the other is block. Block is even more conservative in the sense that if one thread is not able to get the lock, it blocks. The thread which has the lock when it calls release is going to unblock all the block threads, right and again there is some cost to block and there is some cost to unblock, right and that kind of a lock is called a blocking lock, right. So we saw a spin lock. There is another lock called a blocking lock. Blocking lock will block the thread which is called acquire and the release function becomes a little more complicated because it unblocks the threads that have, that are blocked on that lock. So which one should you use? So typically you will have both implementations available, a spin lock and a blocking lock and depending, so as a programmer you have to make a choice and you will basically look at your critical section size, typical critical section sizes and make a choice. It is possible that your code is such that some critical sections are very small, some critical sections are very large and the same lock is protecting those critical sections. Possible, right. Some critical section is small, another critical section is large, the same lock needs to protect both those critical sections. What do you do in that case? In that case you can just say I will use a hybrid approach. I will spin for some time and then if I do not get a lock in that time then I will block, right. Even that is a valid strategy and that is a strategy that is used in many, many lock implementations which says spin for some time, right and then block. If while you were spinning you got successful, you are done. But if you spin, if you spun for some time and you have not got the lock, it probably indicates that, you know, assuming that, so it probably indicates that it is going to take a long time, so it is better to now block, right. So this is a sort of a hybrid approach, adaptive approach where basically you at the run time figure out whether you need to spin or block. So how do you decide this sometime, whether it should be 10 nanoseconds, 10 milliseconds, 10 microseconds, 10 seconds? Once again it is an engineering, it is a decision that you have to make depending on what is the cost of blocking, right. How long does it take to block? Let us say blocking cost is, let us say there is this variable, there is this quantity called the blocking cost which is how long will it take to actually block the thread. So let us say the blocking cost is, let us say 100 microseconds. If you know that the blocking cost is 100 microseconds, roughly speaking, then one approach is that spin for 100 microseconds and if you do not get the lock in those 100 microseconds, then block. So now you are going to, so the maximum amount of time you are going to waste is 100 microseconds of spinning plus 100 microseconds of blocking, a total of 200 microseconds, right. Yield or block is costly, yes. Sir, but suppose even if I am spinning. Yes. That overhead is more. So can you repeat, I did not understand your question. Sir, I am saying that. Yes. Sir, I am talking about a multiprocessor system. On a multiprocessor system, you have to make this choice between spinning and blocking. On a uniprocessor system, what is the better choice? Spinning. So how many say spinning? Three. Three. Blocking? Right, most of the others, yes. So blocking is the better choice, right, on a uniprocessor. Because if you are in a uniprocessor, you know, you are trying to get a lock. Spinning is not going to solve the problem, right. You are on a uniprocessor. If you keep spinning on a uniprocessor, you are never going to see that lock released because nobody else is getting the chance to run. It is only when you get switched out will somebody get a chance to run and he will release the lock. So anyway the switching is going to happen, so might as well just block, right. So when I am saying there is a trade-off between spinning and blocking, I am really talking about a multiprocessor system. On a uniprocessor, it is very clear, always block. Yeah, so even on a uniprocessor, it is also okay to spin because you will get preempted. So it is okay to spin, but from a performance standpoint, it is better to block, right. From a corrective standpoint, it is also okay to spin or even on a uniprocessor, right. Okay, good. All right, let's stop here."}