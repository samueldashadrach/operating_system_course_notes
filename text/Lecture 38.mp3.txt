{"text":"to Operating Systems Lecture 38. So I'm going to first continue our discussion on scheduling. And we're going to discuss an interesting case study where we'll see that bad scheduling can have very nonlinear effects in terms of performance. And then we're going to talk more about lock-free synchronization after that. So we've been looking at scheduling and what are the different scheduling policies. And we talked about priority scheduling. And we said that typical general-purpose systems use some sort of priority scheduling where they prioritize I-O-bound jobs or interactive jobs over compute-bound jobs or long-running batch jobs. And that ensures that there's a high throughput and also very low response times. But these priorities are not strict in nature in the sense that priorities keep changing. So if a job hasn't received attention of the CPU for a long time, then gradually its priority will increase. And so eventually, its priority will become maximum since there's no starvation problem. So in general-purpose systems, you avoid starvation problems of this kind. But in other systems, like real-time systems, let's say you're running an operating system in a car, there you will want to implement strict priorities. So some processes have strictly higher priority than other processes. For example, safety-related processes will have higher priority than other processes that are not so critical. But I'm going to talk about one case study which will give you a nice insight into the non-linear effects of scheduling. So if you do a bad scheduling, then actually your system can behave very, very poorly under high loads. So we also said that scheduling is a problem when the resources are saturated in their, you know. So if the number of resources are short for the amount of or the rate of requests that you are getting, at that point, scheduling becomes really important. If the number of resources are plentiful, then scheduling is less of a problem, of course. So let's take this example. Let's say this box is a computer or CPU. And let's say it's connected to the network. And this is a network card, let's say network device. And packets are coming in. So let's say it's a server. And there are packets coming in from the wire. And let's say it's a web server or some kind of a network server. And it's receiving packets, and then it's sending replies. So it's receiving a lot of requests, and then it's sending replies to those requests. Now, one way, or one of the typical ways you would implement such a server is that you will use interrupt-based mechanisms to handle incoming packets. So as soon as you receive a packet, you will configure your network device or your network card to say, interrupt me whenever you receive a packet. Why do you say that? As we know, there are two ways to handle I0. One is interrupt-based I0, and the other is polling-based I0. So there are two options here. One is I could configure the network card to say, every time you receive a packet, interrupt me. And interrupt is nothing but a very high-priority process. So an interrupt basically preempts all other lower-priority processes, assuming you are not already inside an interrupt handler. Whatever was running will get preempted, and you will get to run the interrupt handler. And the rationale behind that is that if there's a packet that has come in, you want to serve it immediately. So there's a request that has come in. You want to minimize the response time of the request. So the reason you would want to have an interrupt-based system is to minimize the response time. The other approach you could have taken was not configure the network card for interrupts, and rather said, I'm going to check the network card every few milliseconds, or some generality. So let's say I check the network card every 100 milliseconds. So in this case, the network card needs to have a buffer. And packets are coming in. Then the buffer doesn't need to get buffered in that buffer on the card. So it has nothing to do with the CPU. There has to be a buffer on the device itself. And then the CPU can check the network card for packets. The problem here is that the response times are higher than what they can be. If a packet comes, on average, it will have to wait on the buffer for 50 milliseconds. If I'm polling every 100 milliseconds, then my average response time will be at least 50 milliseconds, which is very, very large. I could have done much better if I was using interrupts. Then I could have given much, much better response times on the order of microseconds, let's say. So let's say I have an interrupt-driven kernel. So I'm going to say it's an interrupt-driven kernel, which means that each time I get a packet, I get an interrupt. And the packet gets copied from the network card to the buffer, to some buffer that I'm maintaining in the main memory. And then there is another process server, which is consuming from this buffer and doing some processing on it. Let's say it's formulating a reply and sending a reply back on the network card or some other network card, or doing something with the packet. So in an interrupt-driven kernel, I model this as a producer-consumer, where the producer is the network card and the consumer is the server thread. And in the interrupt-driven kernel, the producer is basically always a higher priority process than the consumer. So if the producer has something to do, which means it has some packet to produce inside the buffer, it will get to run. And it will preempt the consumer every time. So this has very low response time, because you will basically immediately produce and consume and things like that. But let's see what happens if there's a very high load on the network card. Let's say I have a very high-speed network link, and my CPU cannot process the packets at that rate. So for example, I'm receiving packets at the rate of 10 gigabytes per 10 Gbps link, and I'm getting lots of packets. And the processing I need to do on the CPU cannot sustain that rate of packets. So what's going to happen in this case? The packets will start getting dropped inside this buffer. So this buffer will fill up, and the packets will start getting dropped at this buffer. What will also happen is that, because you are interrupting on each packet, and the interrupt rate is so high, because you're getting packets at a very high rate, the server thread will never get to run. So the consumer will never get to run. So it's only the producer that's getting to run, and the consumer is never getting to run, which effectively means that the CPU is actually doing no useful work. All it's doing is getting the packet from the network card to your buffer, and there's no follow-up on the buffer, and eventually for the packet to get dropped. This act of actually bringing the packet from the network card to the buffer is completely wasteful in this case, because that packet is destined to get dropped eventually. Or in fact, if the buffer is full, then it will just get dropped right there. So this is an example of a very bad scheduling. So what will happen is, if I was to draw a curve between input packet rate and, let's say, output packet rate. Let's say my server was sending a reply for every packet. Let's say there was an input packet rate, an output packet rate. This curve will be pretty much linear for low rates. But at some point, when the input packet rate is so high that the server actually gets saturated, you will basically start seeing a drop in the throughput till you actually reach zero. So this is the point where the server saturates. And the reason you are basically seeing a drop is basically because after the input packet rate becomes higher than a certain limit, then the producer gets to run more often, and the consumer gets to run less often. And so because the consumer is getting to run less often, the net throughput is dependent on the consumer, which is the bottleneck. And so the higher the input packet rate, the less often the consumer gets to run, till the consumer doesn't get to run at all. So the input packet rate becomes so high that the consumer doesn't get to run at all. And at that point, you basically have a net output rate of zero. So this is the point. At this point, we say it's a live log system. We call a system live log if the system is doing a lot of work, but it's making no progress. This is definitely a case of a situation where the system is doing a lot of work. Continuously, the CPU utilization is 100%. Yet the output, net useful output of the system is actually zero. So what's the problem? How could I have fixed this problem? If the rate is very high, then I could have switched to polling. And so what's the best? Good answer. And so what's the best I can expect? So let's go in the reverse direction. So I don't like the fact that if the input packet rate is so high, then my throughput becomes zero. What would have been the ideal system? So the ideal system would have been that at this point, the CPU got saturated. Clearly, I cannot serve anything which is more than that. After all, the server cannot do anything more than that. After this, I should have had a curve like this. So after that, all the packets that are coming, extra rate is getting discarded. But the rate that I can support, at least that much is getting served. That's the ideal thing you could add in your system. And why am I not being able to achieve this ideal behavior? Because I'm doing bad scheduling. Because I'm doing bad scheduling because I'm prioritizing the producer over the consumer, irrespective of anything else. And I did that because I wanted very low response times. So here's a very nice example where because of trying to minimize the response times, I actually get a very bad throughput in my system. All right, and so what's happening is that only the producer of the queue gets to run. And the consumer never gets to run. And so you're basically degrading your throughput. So how would you reach this point from this point? You would want to reprioritize things. You would want to say that, OK, I can see that there is some problem. I want to give equal priorities to my consumer and producer at some point. It shouldn't so happen that the producer is getting to run too much, and the consumer is not getting to run at all. Or the consumer is getting to run less than the producer. That, to me, is a warning sign. If the consumer is getting to run less than the producer, or in other words, if the consumption rate is less than the production rate, then that looks like a warning sign. And what I should probably do is throttle my producer so that the net throughput remains nice. Otherwise, if the producer is just running, it's completely wasteful work. So the ideal thing that could have happened is that instead of the package getting dropped at this buffer, the package should have been dropped at the network card itself. In that case, the CPU wouldn't have done any wasteful work. What's happening is that the work that's required to bring the packet from the network device to the buffer is completely wasteful in the case of when you reach the live log stage. But if you somehow figured out that, no, I don't need to do this wasteful work, then what would happen is eventually the packet would get dropped here. And one way to think about this is that if you notice that your buffer is becoming higher, if the average buffer lengths are becoming higher than a certain threshold, then you throttle your producer. Throttling your producer basically means that you switch from interrupt mode to polling mode. And you choose your polling frequency. The nice thing about switching to polling mode is that you can choose your polling frequency. And the choice of polling frequency determines the rate of execution of the producer or the priority of the producer or the proportion of the producer. Eventually, I want that the producer and consumer rates should be similar. And so that's how I would ensure that kind of thing. So the solution to this is that which is, let's say, implemented in mainstream kernels. And so this was a problem in the Linux kernel 15 years back. It was discovered. And so the solution that was implemented after that was that you use interrupts if the rates are low. And then at some point, you switch to polling. And the rates are high. And you choose your polling frequencies depending on your average buffer lengths and the time it takes to the time the producer and consumer processes are getting to run. So here's an example where there's a tension between response time and throughput. And you want to have both of them dealt with. So in general, the thumb rule is that interrupts are better at low rates. And polling is better at high rates. With this, I'll switch to my next topic, which is lock-free synchronization. So we have discussed some lock-free synchronization before. And I'm going to discuss more of that today. So let me first introduce to you a small example. Let's say I wanted to implement a data structure called a searchable stack. The searchable stack has elements of this type. An element is a key and a value and a next pointer. And the stack is represented by a variable called top. So you can only start at the top. You push to the element by just incrementing top. Basically, you wanted to push an element e. You just say e.next is equal to top, top is equal to e.next. That's pushing an element. You pop an element by just decrementing top. You say e is equal to top, top is equal to e.next, and return e. Of course, I'm gliding over some details, like if top is not equal to null, and et cetera. You can do that. But there's a third operation, which is a search, which just goes through the stack and searches for a particular key. So you want to search for a key. You just iterate over the stack while e, if e.key is equal to key, then return e.value. Otherwise, just do e.next. If you find it, very good. If you don't find it, return minus 1. So there's a searchable stack. There are two operations, push and pop, which are write operations, read-write operations. And there's one operation, which is a read-only operation, called search, which searches the stack. Clearly, if this code is executed concurrently, if multiple threads are sharing the searchable stack, then there's a problem. We are not going to discuss that. We all know there are concurrency problems in this code. There's a concurrency problem if somebody's searching, and there's somebody pushing at the same time. And also, there's a concurrency problem if somebody's pushing. The two threads are pushing at the same time, or pushing and popping, et cetera. So what are some ways to solve it? Well, one way to solve it is you use locks, right? So you basically say, I have a lock for the entire data structure. And basically, put a lock, acquire and release around this. You put a lock, acquire and release around this. And you put a lock, acquire and release around this. That's a coarse-grained lock, right? And that will severely impact your performance. It will not scale at all. So if you have multiple threads, then you will only have single-threaded performance. Let's say this code was the only code that's running. Then you'll basically have no scalability. What's the other thing you can do? Let's say I tell you that most of the time, you're going to execute search. And push and pop are relatively very rare operations. So just as an example, 99% of times, it's search that gets called. And only 1% of times, it's either push or pop that gets called. So it's mainly search that's getting called. And search is a read-only operation. So what do you do? Read or write a lock, right? So it's simple. I'll just use a read or write a lock. I'll say, I'll have a read. So I'll declare a read or write a lock. Let's say RW lock. And then I'll have write acquire. RW lock. And release. And similarly, I'll have write acquire here. And I'll say release. And here, I'll say read acquire. And I'll say release it. So why is read or write a lock better than your normal lock? Because the semantics are that multiple, the lock can be acquired in read mode multiple times simultaneously. But the lock cannot be acquired simultaneously in write and read mode or simultaneously in write and write mode. So there can be 99% of times you're calling search, then all these threads can execute simultaneously. But is it really better than, is it really that good? So let's look at what's really happening at the CPU level or at the hardware level. So let's say I have CPU 0. And CPU 1. And let's say I have a shared bus. And I have caches here. So I have cache 0. And I have cache 1. And they're connected to the bus. Before I discuss this, let's also revise how read or write a lock will be implemented. So if I want to implement a read or write a lock, I will say, how will I implement read acquire? I will say, I will set some flag. I'll say rwlock.mode is equal to read and count plus plus. Something of this sort. And I have to make sure that it's not already held in write mode. So I'm going to, and so this, I'm going to have to write to this lock variable. I'm going to have to maintain a state in this lock variable that it's read, it's held in read mode. And I'll have to also write some kind of a count variable that says, you know, I basically acquired it in read mode so that other readers know that, or other readers and writers know that it's been acquired. So what has happened is, so what I'm going to show you is that this code, which was actually completely read-only code, has now become a read-write code. So here, I'm only doing read operations on all the shared values. What is my shared value? The shared value is my stack. And all I'm doing is I'm just doing reads on the stack. The only writes I'm doing is to my local variable, which can be my local stack or my local register, whatever. But that's not shared, so I don't care about that. So the only thing that I'm, the only operation that I'm making on the shared variables are reads. But the moment I put a read acquire and a write acquire, these operations involve a write to a shared variable. And that's a bad thing. And so why is that a bad thing? Because if I look at the hardware level, if there's shared data, let's say there's a stack living here. Let's say there's top, there's a searchable stack living here, and so on. Then if both of these were calling search, then very likely, this stack would get cached in the local caches of both these CPUs. And so these CPUs can just execute search at cache speeds. They don't need to go to the bus to be able to read any memory location. Assuming that all local variables are already in the cache or in the register, the global variables, if the accesses of the global variables are read-only accesses, then modern hardware allows you to cache both of them simultaneously in read mode only. So before, so just some background. Hardware implements what's called a cache-coherence protocol. How many of you have studied a cache-coherence protocol? OK, not really. All right. So a cache-coherence protocol basically says it allows for every memory location m, memory location m can be cached here. So it basically maintains coherence of data, which means that a memory location m can be cached here in read mode or exclusive mode. If another cache accesses it also, then if the memory location is held in read mode, then this can also cache it. And so the same location can be held, can be cached in read mode and multiple caches simultaneously. But a location can only be cached in exclusive mode in one cache at a time. So every location can either be cached in read mode or in exclusive mode. If a location can be cached in read mode in multiple caches simultaneously to increase performance. And because it's in read mode, nobody can modify it. If somebody tries to modify it, what happens is all the other read mode cache entries get invalidated. And this, the one that modified it, now holds it in the exclusive mode. And this mechanism to implement this kind of protocol is implemented in hardware and called the cache-coherence protocol. So if the CPUs are accessing the list in read-only mode, what will happen is that this list will get cached in the local caches of the CPUs in read mode eventually. And what will happen is that all these search operations will only access the local caches and never have to go to the bus. Recall that cache speeds are on the order of few nanoseconds, one or two nanoseconds. On the other hand, a bus transaction involves, depending on whether you actually go to the bus or whether the scores are within a single chip, it can range from tens of nanoseconds to hundreds of nanoseconds. So it's on that order. So it can be a 10 to 100x slowdown from just cache access. So if you only have cache hits versus if you only have cache misses, you can have a 10 to 100x performance difference in there. So earlier, all the things were getting cached in the local caches. And so both these CPUs could have executed search at full cache speeds. But now, because you have a read acquire and a write acquire, there's another shared variable here, which is the lock. And because the lock is being accessed in read-write mode, it's being modified, what will happen is that the lock will get cached here, will need to get cached here in exclusive mode. Then this CPU will try to modify it. And so it will need to get invalidated here. And now it will get cached here in exclusive mode. And so the lock will keep bouncing. The read-write lock will keep bouncing between the two caches. And because it needs to be cached in exclusive mode, because it's a write access to these variables. So each time you access, each time you write to the read-write lock, you actually need to invalidate the value in the other cache, if it exists, to get it, to get the latest value. And so what will happen is that this read-write lock will keep bouncing between the two CPUs. And each time it bounces, it involves a bus transaction. And that gives you a lot of slowdown. So what can happen is that this code will actually become, it's possible that this code actually becomes slower with read-write lock than faster. So for example, if you have two CPUs, and if I ask you to implement the fastest possible, fastest possible solution to my program, which involves accessing the read-write lock, you may say, OK, let me use a read-write lock. But if you use a read-write lock, because of the cache line bouncing, the total performance of two CPUs may actually be lower than the performance of one CPU. Because if you only ran the program with one CPU, it would have been a cache line access, cache access. But if you access it with two CPU, if you parallelize the program with read-write lock, even though there is full concurrency, because of cache line bouncing, you're actually slowing down the program by a factor. So the advantage of actually having more concurrency is gone in some sense. In other words, even though the CPUs are executing concurrently, the bus becomes a serialization bottleneck in some sense. So what was the problem? The problem was that I had a read-only function. And by using read-write lock, I converted it into a read-write function. And that made it less scalable. This problem, I have demonstrated this problem on two CPUs. But this problem becomes even worse, actually much worse, if you go to a larger number of CPUs. For example, today you can get a machine which has 80 CPUs in it, a full desktop processor like the Intel, for example. And so you can get a machine with 80 CPUs on it. And then you have a cache-coherent protocol going on between the 80 CPUs. And now this lock is bouncing between the 80 caches. And so that can actually become a huge performance bottleneck. So firstly, it clearly shows that reasoning about performance in concurrent programs is very hard. But what could I have done? Basically, it's a bad idea to convert a read-only code to a read-write code. It would be nice if I could have done this synchronization without having to do this. Why do I need to have this read-write operation inside my read-only function? Because I need to synchronize it with other write operations. That's the only problem I have. So what could I have done? Let's say I used a lock-free primitive, like compare-and-swap. We have seen compare-and-swap before. And I said that I'm going to use compare-and-swap to implement my writes. And because my compare-and-swap is going to be atomic, my reads don't need to take a lock. Let's see if this works. So what I'm going to do is I'm going to implement. Before I talk about compare-and-swap, let me talk about one alternate solution. So here's one alternate solution, what are called BR locks, big reader locks. The big reader locks have this, let's say there are n CPUs. You can generalize it to thread, but I'm going to consider n CPUs. Then you have n locks, one per CPU. Reader CPU, I'm just using the term thread and CPU interchangeably. Reader CPU acquires its own lock before executing read code. And writer CPU needs to acquire all locks before executing write code. The idea is that you have a lock, which is a structure which has now, instead of one bit or two bits, it has n bits. And the idea is that every CPU will acquire its own lock, its own bit, and the writer CPU will need to acquire all the bits. So why is this better than the reader-writer lock that we just discussed? Let's see. What will happen is that, let's say there's CPU 0 and CPU 1, so you have two locks. CPU 0 will have its own lock, so there are two locks, L0 and L1. CPU 0 will make a write operation, but it will make a write operation always to the same location, L0. And so what will happen is L0 will get cached in cache 0, and L1 will get cached in L1, both in exclusive mode. But there'll never be any bouncing between the caches. So L0 will never need to go to L1, and L1 will never need to go to L0. And so they will both live on caches. And yet, you will basically ensure that it's correct. Because readers can execute concurrently, because one CPU has acquired L0, another CPU has acquired L1. But a reader and a writer cannot execute concurrently, because the writer will acquire all locks. So the writer will need to acquire all locks. So what you've done is you have slowed down the writer, but you have really made the reader faster. And that's the case you're optimizing for. I'm saying that the reader is executing 99% of the time. I don't care about the writer performance at all. So this works. What are some problems? The writer can start. Well, not really. I mean, it just depends on your lock acquisition algorithm. So you can just say, I want to give locks on a first come, first served order. So just like a reader-writer lock, you can say that if a writer makes a request for a lock Li, then it will eventually get the lock within a finite amount of time, within a bounded amount of time. The writer cannot start, but the writer actually has to make a global operation. If you have 80 CPUs, then it takes to do a lot of work to get there. The more important thing is that this caching is not done at bit generality. Caching is done at cache line generality. So just like pages, pages are a unit between disk and main memory. The unit between main memory and the cache is a cache line. You cannot just say, I want to cache this byte. You have to say, I want to cache the cache line that contains this byte. It's a full cache line that you have to get into, the cache. So what does that mean for a big reader lock? Can Li's be sharing the same cache line? No. So all these different reader locks and locks need to be on different cache lines. So basically, you need to make sure that the variable Li is occupying one full cache line. So typical cache lines, let's say, will be 64 bytes on modern hardware. You need 64 bytes for every CPU. So if you have 80 CPUs, then you have 64 bytes into 80. So the lock structure actually becomes pretty large. Moreover, you cannot write generic code. It really depends on what the cache line is on the architecture. So you basically need to check what the cache line is and compile your code for that particular chip. Different chips for the same architecture can have different cache line sizes. And you need to optimize based on that. These are all difficult things to do. I mean, imagine if I was to use big reader locks in the Linux kernel, then I had to worry about which chip is this kernel going to run on to basically decide what the structure of my big reader lock should be. So this is an option. But let's look at a better option which doesn't have these problems. And I'm going to try to use compare and swap to basically implement lock-free synchronization, and in this case, example, in a better way. So what I'm going to do is, firstly, why do I want to avoid locks? Let's just also review why we want to avoid locks. Firstly, I've already discussed locks have performance problems, the cache line bouncing, even for read-only code, even if I use reader-guided lock. There's complexity involved in locking. You have to worry about fine-grained locks versus coarse-grained locks, et cetera. You have to worry about deadlocks. And you have to worry about priority inversion. So we have also studied scheduling, and there's a problem with locks. Locks are just resources, and people need to relate on resources. So any time you have something like that, then there's priority inversion. That's a problem. So let's look at the other way of doing lock-free synchronization, which is compare and swap, which is sort of a transactional way of doing things, where basically, let me implement the push and pop using compare and swap. So let's just revise what is compare and swap. Compare and swap is an instruction, let's say cache, that takes three arguments. One is an address, another is the old value, and the third is the new value. And the semantics are, if the contents of address are still equal to the old value, then replace them with the new value. And you do this in an atomic way. Also, this instruction returns, which means it overwrites one of these registers with the value that was read. So let's say I read a value which was, let's say, I said was is equal to star adder. Then I return was. So I read this location, and I return that, firstly, always. But I also compare it with the old value. And if it's equal, then I replace that location with the new value. And this entire operation is atomic. That's the compare and swap. So you compare, and you swap if the comparison is successful. If the comparison is not successful, then you don't swap. So that's the compare and swap. And let's see how we use it for something like push. So I say void push, element e. I'm going to say e.next is equal to top. And then I wanted to do top is equal to e.next. But I wanted to do that top is equal to e.next only if top hasn't changed between the two instructions. So what I'm going to do is I'm going to say do while compare and swap top address old value, which is what you read, e.next, and new value e is not equal to e.next. You check atomically if the value of top is still equal to what you read it earlier. And if so, you replace it with e. That's what you do. And this check with the e.next is basically saying whether this was actually successful or not. If it was successful, then you're done. You break out. If this was not successful, which means it was not equal to e.next, it was not successful, then you retry this operation. So let's say this is top. I read the old value in e.next. This becomes old. Then I execute cache on top and old. And the new value that I want is e. I want to push the element. So I want top to become e. And so I do execute this cache and top old e. This will become successful if nobody else has modified top in the middle. So your top will get replaced with e. If somebody has modified top in the middle, then this will become unsuccessful. Right? And the way to check whether it was successful or unsuccessful is to look at the return value. That's all. OK? And you keep trying until you become successful. So this is how you would do a push. And similarly, you can do pop. I'm not going to write pop. You can just write this piece of code. And pop is very similar. So now let's say, OK, let me just write pop because it's going to be important in the discussion that follows. So let's say l struct element stop. I declare a local variable e is equal to top. And then I try to compare and exchange top. The old value that I have read is e. And I want to replace it with a new value, which is e.next. Right? OK, e.next. That's what I'd want to do. And I want to do it in an atomic fashion. It's possible that this operation fails. How do I know whether this operation has failed? It has failed if the return value is not equal to e. Right? That's it. And you put it in a while loop. That's your pop. Right? OK. So that's your push and pop. What are you doing? You're basically making sure that atomically the list gets updated, either with a push or a pop. But if I do that, what happens to search? Can I say that now search can execute in a completely unsynchronized way? Search does not need any synchronization. Well, I mean, what are the guarantees that I want to give to the user? I want to give to the guarantees to the user that my data structure always remains well-formed. Right? And my search always sees a consistent data structure, which means if it starts from a top, and then it sees a location. Now, there's serializability in the access. Right? So by serializability, it means that there were about 10 operations that were given to you. Then the final result that I get for all those 10 operations look like some serial order. It seems like the operations were done in some serial order. So either a push was done first, and then there was a search, then there was a pop, then there was a search, and so on, or something. So there must be some serial order. There should be. So that's what the guarantee I want to make. Whatever final result I get, it should obey some serial order. There should be some serial order that basically will give you the same result as what you saw. So in this case, because you are atomically updating the pointer, the search, if it executed before the compare-and-exchange, we'll see a list where the push hadn't been made. And if it executed after the compare-and-exchange, it will see a list such that the push has been made. So the serial order depends on whether the search executed before, or whether the search started, whether you started with the top pointer before the push or after the push. So when you read the top pointer, whether that read of the top pointer was before the push, or before the compare-and-exchange, before the cache, or after the cache. That's what's going to make it serial. And that's going to define the serial order. So if it was after the cache, then it's as though the search executed after the push. If it was before the cache, it's as though the search executed before the push. But that's not all. So but let's say I am holding a pointer to. So firstly, let's see what are some problems with this kind of a compare-and-swap. Firstly, why was I able to do this? I was able to do this because my data structure was such that updating it required one pointer swap. I was pushing involved swapping between E and E.next. And swapping involved swapping between E.next and E, something of that sort. And so I could do this. But if my update operation required swapping two pointers, or three pointers, or n pointers, then I can't use cache. Then I use need. So let's say, for example, if I wanted to support another operation that said, remove an element in the middle of the searchable stack. So I give you a pointer to an element inside the middle of the stack. And I say, remove this element. So removing that element would involve changing the pointer from, let's say it's a doubly linked list. Then I would need to change the pointer from my previous node to the next node, and from the next node to the previous node. So I need to make two pointer changes. And I cannot do it atomically using the cache instruction. If my hardware supported a double cache instruction, which they took six arguments, then yes, or six operands, then yes, I could have supported something like that. x86, or in general, architectures don't support more than one memory location for cache. So firstly, cache can be used if the operations involve only a single pointer update. And one needs to very carefully reason about it. So that's one disadvantage. Yes? It's a pre-the-pointer. It's a pre-the-pointer. OK, good. So the second problem is memory reuse. So let's say I popped a pointer. So I popped the locations on the stack. And there was a concurrent search that was running. So let's draw a timeline. So search gets to execute, takes pointer to top. Then pop gets to execute, updates top. So we said it's OK, because it's as though the search executed before the pop. And so as long as the top pointer still exists, and the top is still pointing to the list, so what will happen is at this point, you will have a list where the top is pointing like this. But you also have search holding a pointer to a location that is pointing to this. That's what will happen. Search is holding a pointer to something. But you have popped it off, but you're still holding a pointer. And we say it's OK, because this location is still pointing here. And when the search is going to execute like this, it's still going to see a consistent list. So it's OK. But it's OK if this location is not freed. If this location was freed by the pop, then the pointer can become invalid. So memory reuse is a problem. The search is holding a pointer to a location that's no longer a part of the data structure. Search operation will remain consistent if that location is not freed. So we have a strange situation where we have a pointer to a location. That is not really a part of any global data structure. But yet, I don't want it to get freed, because some local references to it may still be hanging around. So the short answer to the question whether I need to do something to search is that, well, I mean, search will work just like that, except that you need to be careful about memory reuse. So you cannot just free the location. So the question is, when can you free the location? You have executed pop. This location is no longer used. When can you call free? Is there a guarantee to when I can call free? If there is no other thread that holds a reference to this location. How do I know if there's any other thread that's holding a reference to this location? Wonderful. So here's a suggestion that let's put inside the pointer, inside the struct element, let's put a reference count. Which says, how many people are holding a reference to it? Is that a good solution? Yeah, it will have the same cache-line bouncing problem. So each time you search, call search, you have to increment the reference count. And so you have made a write operation out of a read operation. And so reference count is a possible solution, but it brings us back to the same issue, that is cache-line bouncing with the reference count. So the answer is, really, I cannot have an answer. If I don't want to have a write operation, global write operation, then it's very difficult to know whether there could be any thread holding a reference to a location that's no longer part of the data set. So I don't have any answer on when I can free it. Well, I could say something like the following, though. I could say, let's wait for one hour, and then free the location. That makes sense, because after all, if there was a search procedure that was holding a reference to it, it must have moved on with it. How long can it hold a location to the search? But that's not a good answer, because after all, it can hold a reference to this location for one hour. There's no guarantee that it will leave that location after one hour. And how did I choose one hour? Why is one hour a good number? So we're going to see a solution to this problem in the context of the Linux kernel, which is called read-copy-update. And we're going to discuss that next time. OK, so let's stop."}