{"text":"Alright, so welcome to operating systems lecture 3. In the previous lectures we have looked at Unix and its interfaces. In particular we looked at these system calls, fork, exec. Fork was a system call which copies a process and makes a new process out of it. Exec overwrites the current process with an executable and starts it running. Open, read, write, close operate on files or other resources and use file descriptors as handles. Pipe is a way of creating an inter-process communication channel between two processes. Doop allows you to duplicate a file descriptor inside the file descriptor table. Exit is a way of telling the operating system that the process has finished and wait is a way of telling the operating system that I want one of my child processes to finish and I want to get its return value, right, or the exit code of the exiting process, alright? Okay, so once again, you know, if I were to draw a picture, here is an operating system and here are the various processes, P1 and P2. What is a process? Process is, you can think of it as a running program, a program which is, which is in action, which has state associated running, live state associated with it, okay? The abstraction of a process is that this entire space is private to it. So if I allocate something inside the process, it remains completely private to me. If I write something in this space, it remains completely private to me and nobody else can touch it, alright? So this space is also called an address space. So this is an address space. It's called an address space because if I say I want to access address 1,000,000, then the address 1,000,000 here will point somewhere here, let's say, and address 1,000,000 in P2 will point somewhere here. So the same address in different address spaces points to different locations, right? And the, and address spaces is also an abstraction that's implemented by the operating system, okay? So process also means that it will have its own address space, alright? The nice thing about this kind of isolation between processes is that different processes can run completely independently and they don't need to be worried about each other and, you know, worried about what they are going to access and whether I should protect myself with another process or not. So as an example, your web browser doesn't need to worry about your shell or your compiler or other things, right? They can all sort of execute independently because they're executing in completely independent address space. Whenever they need something which requires access to shared resources, and the shared resources could be devices, they could be files, et cetera, they use things like system calls, right? Or even memory, right? So if they want more memory, anything. So clearly, you know, you have one physical system that's being shared by all these processes. If I want to get access to these processes, these resources, the process needs to ask the OS for it. And the OS is going to decide whether this request is legal or not and appropriately arbitrate these resources among these processes and also ensure that concurrent accesses to these resources from these processes are protected. And so fork, exec, open is one way of doing these things. Fork is a way of creating a new process. The list of processes is also in some sense a resource and if you want to create a new process, you ask the OS, I want to create a new process. The OS checks if it has enough resources to actually support a new process. For example, does it have enough memory to support a new process? And if it does, it will create a new process and it will return the ID of the new process. If not, then it will return an error code which will be a negative number. And similarly, you know, if you open a resource like a file, whether you have permissions to open that file or not. So in some sense, the OS is acting as an arbitrator between the process which is untrusted and the resource, right? So a process can be thought of as an untrusted entity. It's a program that's running. You don't really trust it. You don't, in the sense that you don't trust it to, you know, the process, the program could be greedy. It may try to take too many resources. It could be malicious. It may be trying to crash the system or it may be just buggy, right? In either case, you don't want a buggy program to crash your entire system. So it's the job of the operating system to ensure that all your resources remain safe even in presence of these untrusted processes. And so when you design these interfaces, you also take one of the important things that you worry about is security. You know, you don't want untrusted processes to be able to take control of your system in unexpected ways. And the other thing you have to worry about is performance. And we saw some examples of how, you know, different interfaces have different performance tradeoffs. For example, we looked at create process versus fork and exit and other things. As we go along the course, it will become clearer why this interface is reasonably performant and that's why it's popular. So both performance and security are quite important when you're designing such an interface. Now the process P1 can communicate to the OS, but often a process needs to communicate with another process, right? And we saw some examples. We saw one example last time where one process wants to compute something on its input and then pipe its output to another process which wants to compute something on its input and so on, right? So a chain of processes with pipes connected in the middle. And this is a typical example of a situation which requires inter-process communication, and pipe is a system called to be able to do this inter-process communication efficiently. And then we have also looked at exit and wait. So if I were to draw a diagram which represents a process, the processes in a system are often represented by a hierarchy, which is the hierarchy in which they were created. So for example, you know, when you start your system and you boot your system, the operating system will typically create one process, which, you know, which is often called the init process. And this init process is going to fork more processes and more processes, and these will spawn more processes in turn. And also, you know, if needed, the init process will create pipes between these processes. And so the system looks like a tree of processes with pipes connected between them or inter-process communication channels between them in general. So for example, when the system boots up, it will spawn, let's say, a shell. It will spawn the X server. It will spawn, let's say, some default applications. It will tell the default applications that your default terminal is this shell, or it will tell the default applications that your default terminal is on the X server, and it will connect them appropriately. So these things can run simultaneously and relatively independently, right? So you have many programs that are running independently, and yet they can communicate with each other to do the things that you want them to do. I'll point out here that pipes are one way of doing inter-process communication, and there are other ways of doing inter-process communication, which are pretty much similar in their nature, where you make system calls to be able to write things to that channel. You create a channel, and then you make system calls to be able to write to that channel, and then you make system calls to be able to read from that channel, okay? So that's a review of what we have done so far, and today I'm going to talk about another abstraction, which are called signals. Often, when a process is running, you want to interrupt it and you want to say, I want to do something. The process is conceived as a sequential flow of instructions, right? So one instruction after another is running. That's the normal flow of execution of a process. But let's say I want to bring to the attention of a process a certain event. For example, a key was pressed, right? So that's done using what's called signals. So a process is called, it's signaled with a certain signal, and the semantics of a signal is that it amounts to an asynchronous function call. So let's say this is a process which is running, okay? This thing means that it's running, and now the OS wants to raise a signal. What this means is, so the semantics of a signal is that it will interrupt the current execution of the process and make a function call to the signal handler. Whatever it was doing, it suspends that, in some sense, and makes a function call to this special function, which has been registered previously, and makes a call to that function. And that special function is called a signal handler. That signal handler also executes in the address space of that process. So the signal handler executes in the address space of the process, and can touch all the memory that the process can touch, or read and write all the memory that the process can read. All right. Okay. And so when the signal handler returns, the execution continues from where it was interrupted, right? So that's the semantics of a signal. In some sense, it's like an asynchronous function call. Yes? Sir, you said that... So by not related, you mean... We registered... Right. So signal handlers are private to a process. So if process one registers a signal handler, then process two, while process two is running, and a signal occurs, process one signal handler will not get executed. Process two will have its own signal handler. So every process will have its own signal handler, and those signal handlers will execute in the context of its own process. All right? Okay. So this can be thought of as an asynchronous function call. It's basically like saying that whatever you were doing, just insert a function call right after that, right? So you call that function. That function, when it returns, it just returns exactly where you called it, and you can start executing the next instruction. All right. So the reason this kind of an abstraction is needed is to be able to handle rare events, you know, events which occur, which are sort of events, to allow a process to be able to expose an event-driven interface, like press of a key, right? Arrival of a network packet or something like that. So you want some kind of an event-driven abstraction for a process to be able to do more than just sequential execution. All right. So there are two system calls on Linux. One is called kill, and another is called signal, which support this signal—unique signal abstraction. The syntax of signal is you give him—so both of these are system calls, and signal takes two arguments. The first is a signal number, and the second is a function pointer to its handler, right? So a process can actually register handlers for signals, right? There are a set of standard signals that are pre-documented, which are part of the OS semantics. For example, there is a SIG interrupt, SIGINT, the interrupt signal, which is triggered when you press a Control-C while the process is executing, right? So if you press a Control-C, what happens is the operating system figures out that Control-C has been pressed on the standard output of this process, and so it generates a SIGINT for that process, right? What that causes is that it causes an asynchronous function call of the SIGINT handler. The default SIGINT handler will simply exit. It will simply call the exit system call, and the process stops, right? So that's why you see that most of the time when you press Control-C, the process just exits, right? A process may choose to overwrite the default signal handler, the default SIGINT handler, with its own handler. For example, it just wants to count the number of times Control-C was pressed while the process was executing. So it can do that, for example, right? Or it may say, when a user says Control-C, then free this data structure, right, or start afresh. All these are possibilities. That's just a part of the semantics. So the Unix operating system will say this will be our default signal handler, right? And it will map it in the address space of that process. All right. Yes. The signal system call is used to register a signal handler with a particular signal number, right? And SIGINT has a particular number. Let's say, you know, I don't know what it is, but let's say it's 7, hypothetically. So SIGINT stands for 7, and so with the signal interrupt number 7, the handler is set, right? Similarly, you know, there is SIGSTOP, which basically suspends the process, and, you know, Control-Z delivers the SIGSTOP signal to the process. Okay. Yes. Can we overwrite the handler? So, okay, good question. So can we overwrite? So I said the signal system call allows you to overwrite the signal handler, the default signal handler, or, you know, whatever has been previously registered with a new handler. Now the question is can I overwrite all signals? For example, there is a signal called SIGSTOP, you know, which you can invoke by typing Control-Z on the standard output, and if a process is actually allowed to overwrite this signal, then, you know, there's no—our user will completely lose control of what the process can do. So there are certain signals that the operating system will allow you to overwrite, and there are certain signals that it will not allow you to overwrite, right? And so SIGSTOP happens to be one of those signals that cannot be overwritten, right? So clearly the signal system call—the process call, the signal system call, the OS can decide whether it wants to allow this overwriting, or it doesn't want to allow this overwriting. And there are rules when it will allow and when it will not allow. Yes. No. So that handler is associated with that particular signal number. Right, right. So this is a simplification. Often what you need—often what people do is use the same function to handle multiple signals. And so this argument of this function allows you to disambiguate what was the signal, and you may want to take different actions depending on what you do. So it's just a simplification, right? So the signal handler, if you notice, takes an integer argument, which basically says what was the signal number that was generated. And so that allows you to have the same function pointer for multiple signals. So the same handler for multiple signals. All right. And okay, then there is another signal, which I'm sure a lot of you have seen, is 6xv. This is a segmentation fault, right? The segmentation fault is treated as a signal, right? So what happens is, if the process does something illegal, which means it touches a memory location that it's supposed to not touch, the operating system will figure that out in an efficient way, and we are going to see how it figures that out in an efficient way. But let's say it is able to figure this out, that the process is trying to access an address which it's not supposed to access, then what it will do is it will generate a 6xv for the process. And now the 6xv handler is going to get called. And so the default 6xv handler is just going to abort the process. And then similarly, there's a 6 floating point exception. For example, if you do a div by 0, you will divide by 0, you're going to get a 6fe, and similar things as 6xv are going to happen. There's another signal called 6child. This is a signal which is generated if one of the child processes of the process, of the current process, exits. Let's say I started a child process, and I wanted it to run concurrently. I didn't want to wait on it. I didn't want to block. So I wanted, for example, I start a browser window, and then I start and spawn a new window from inside the browser window, and I want both windows to exist simultaneously. And now one of the windows is closed, which means it calls exit. And so you may want that the parent process be notified that the child has exited to maintain certain data structures about what are all my outstanding children, for example, or anything else I didn't want to do. So the 6child signal is a way of the process, of the parent process, to know that one of its child children has exited, and now it can check what has happened, really. So for example, on the shell, in the shell example, when we tied LS, what happened was LS was spawned as a new process, and the shell implementation we saw so far used to wait on that process to exit. If instead I did LS ampersand, I don't know how many of you have used this syntax of shell, it basically means that you spawn a new process, but you don't wait for it. So you also let the shell continue execution, and let LS continue as a separate process. LS may not be such a natural example, but think of it as a browser, for example. So browser can continue on the shell, and you want to type more commands on a shell, for example. But what you also want to do is that when the child exits, you want to know what its return status was, and etc. So that's how 6child can help. And finally, the skill system call, semantics of the skill system call are that you can actually ask the OS to send a signal to another process. So you can say that send this signal, signum, to this process PID. So one process can actually send a signal to another process. Simply saying that I want to send a signal to this PID, this signum. Of course, there are rules. I cannot send a signal to a process run by another user. I can only send certain types of signals, and so on. So with those rules, the skill system call allows you to do these things. So I don't know how many of you have used this program called kill, which if the program goes rogue, you just say kill-9, that program. That program is just calling the skill system call with the PID of that program to basically, and the signal that's sending it is, you know, sigint or sickle or something. Sickle. Nine stands for sickle, I guess. Okay, so that's an introduction to Unix signals. It allows you to handle interrupt-based or signal-based computation, which is relatively rare and which requires event-driven handling to be doing efficiently. Okay. Another interesting thing about processes is that, let's say I'm a process, P1, and I spawn another process, P2. And this process finally calls exit. And while the process P2 has called exit, the question is, should the operating system delete all state which corresponds to P2, or should it hold on to the state for some more time? And the reason I ask this question is, because P1 may actually call wait some time later. And wait is supposed to return the exit code of this process. So if exit of the child was called much before the parent called wait, then it's the responsibility of the operating system to preserve the exit code of the child, so that when the parent calls wait, it can return the right value. Right? So in some sense, a process is not cleaned up just after exit. A process is truly cleaned up after its parent calls wait on it. Till the parent does not call wait on its child process, there's some state of that process that remains in the operating system. Right? A process which has called exit, but its parent has not called wait, is called a zombie. So not wait, but exit. It's called a zombie process. The return code of the zombie process is actually just lingering around in the operating system. And this kind of a bug is actually very common. Right? Because what happens is that the programmer forgets to call wait on its child, and this is called a process leak. So it's called a leak, basically. It's a leak because you created some number of processes, and you forgot to call wait on it, and so these processes keep occupying some space in the operating system's structures, which store its exit code, and they'll remain forever. And if it's a long-running system, then you can imagine that over months, you are eventually going to run out of memory space storing these things. Okay, so xv6 and Unix are, you know, slightly different, number one. The syntax of wait may be different, but the return code is definitely returned in the wait system call. Right? So I believe there's a pointer that the wait system call takes, which basically gets stuffed with the return code. Right? So there are two ways. So there's a return value of the wait system call, and then there is a pointer that it takes where, you know, the operating system is supposed to stuff the return value. So in any case, the return value needs to be preserved till the wait system call is called. Right? Okay. All right. So for example, on the shell, if I say ls and, and if I implemented my shell by saying that I'm not going to call wait on the child process, I just spawn the new process, and I forget about it. And then I take the next command, what's going to happen is that ls will finish, it will call exit, but it will remain in the system forever. Right? Because it will remain a zombie. What's needed in this case is that the shell should, one way to do it is the shell could keep checking periodically, whether, you know, all the children that have spawned so far has anybody of them exited. Question is, how often should I check? Should I check every one second? Should I check every one minute, one hour, one day? You know, that's completely configurable. The other way to do it is to use signals. Right? So signals allow you to sort of more elegant way of doing things, where you say that if a child exits, a sick child gets generated in the shell, the sick child handler checks all my, check which of my children have exited. And if somebody has exited, it calls wait on it, collects the exit status, does whatever it wants to do with the exit status, perhaps it just wants to ignore the exit status, whatever, carries on. Right? Okay. Next I want to talk about open, read, write, close, and the power of these system calls. So, so far we have looked at open, read, write, close in the context of files and in the context of devices and things like that. But actually, modern operating system uses system calls to do many more things. For example, if I wanted to find out what is the number of processes in the system, right? Or if I wanted to find out which process has how much memory allocated and in which address spaces, right? So you can imagine that, you know, an application typically may be interested in, you know, a huge amount of information from the operating system. For example, if you have ever used this command called PS, it basically lists all the processes and, you know, what are the process IDs, what are their dependencies with other processes, and how much memory they're consuming. So there are other programs like top, which tell you, you know, how much CPU they've used, how much memory they've used, etc, etc, etc. So a process may be interested in a lot of information from the operating system. Also, a process may want to configure the operating system at runtime in several ways. For example, the operating system can provide the functionality that you can on the fly change the number of running CPUs in the system. For example, you want to do some kind of, you know, energy saving computation. And you want to say, I only want the processor only, you know, I have 16 processors in this machine, but I only want the processors that I'm using to be on. Everything else can be off, right? And you want an application to be able to control that, let's say, right? So an application should be able to say, oh, just switch off 10 processors, and I just want the remaining ones to work. And so you can imagine there's a plethora of different functionalities that an operating system is burdened with to provide to the user, and none of the system calls we have seen so far really solve any of these, right? We have been talking at very macro levels, and all these different things are much more micro level. And the question is, how many system calls should an operating system provide to be able to, you know, give this kind of functionality? So it seems very daunting. The way this is done on Linux, for example, is using a pseudo file system called slash proc. What this means is, there is a slash proc live in your regular file system, you know, star prefix with a slash character, but it's not a real file system. It's a pseudo file system, right? Slash proc will have sub directories, which will be all the process IDs, for example, slash proc slash 101, 103, and so on, right? So these are all the process IDs, and these are all sub directories. And what an operating, what an application needs to do to be able to, for example, if I were to implement PS, all I need to do is open slash proc and read its contents, right? So the open and read system calls can be overloaded to be able to do any information gathering from the OS, right? So all the information that the OS wants to, wishes to expose to its applications can be made part of this pseudo file system, and an application just needs to use the regular open read write closed system calls to be able to read that information. Of course, the operating system can still interpose at an open call to see whether an application is indeed authorized to read this information. For example, I shouldn't, for example, you know, some of the simple rules are, I should only be able to read information about my own processes and not about any other user's processes, and so on, right? So once again, those kind of access controls can be made, can be enforced at the open system calls, for example, or the read system calls, for that matter, right? Similarly, if I wanted to change the number of active CPUs, that's also possible. You know, let's say the slash proc has this file called numCPUs, numActiveCPUs, and when a process says write 2 to numActiveCPUs, internally, the operating system is going to trigger the procedure to actually switch off the other, you know, 16 minus 2 CPUs, for example. So you have overloaded the file system extractions to do other things, which involves getting and setting values in the OS itself, right? So as an example, I mean, just to put things in perspective, an operating system, a full-fledged operating system like Linux would have roughly 300 system calls. That's still a lot, but, you know, it's still not in thousands or millions. All right. So, so far, we have looked at interprocesses, how processes have private address spaces, how processes have abstractions to access resources that are provided by the operating system. These resources could be hardware resources, files, or its own data structures, for that matter. And we have also looked at different processes do interprocess communication, right, using pipe. So, so let's take an example of two processes which are trying to do interprocess communication, P1 and P2. And let's say this is a pipe. So, you know, let's say this is a producer and this is the consumer. So he's going to call write on this pipe and he's going to call read on this pipe. This pipe also acts as a synchronization mechanism, because if P2 calls read and the pipe is empty, it basically means that P1 needs to run some more and, you know, call some more writes before P2 can execute. So in some sense, they both synchronize with each other, right? Synchronization means that they, they sort of communicate with each other and restrict their behavior based on the behavior of the other. That's one way of doing synchronization. This way of interprocess communication is slightly expensive in the sense that to be able to write, you need to make a system call. And we're going to see later what's the cost of a system call. So a system call means you have to tell, you have to make a function call inside the OS, which means the OS has to do a few things, get your arguments, process them. Then the other, the receiver has to call and make another system call called read, and those arguments need to be given to the, into the address space of the other process. So there's, there's some amount of overhead. There is a copy that's involved from the address space of the first process to the operating system kernel, and then a copy from the operating system kernel to the address space of the second process. And these are relatively expensive operations. On the other hand, if it were possible for two different processes to share the address space, you could have done this communication much cheaper, right? For example, if I want to send a data, send a message to, from P1 to P2, and we are sharing memory, all I need to do is set a byte in my shared memory, and the other one is going to get that byte, right? So read and write to share memory. The kernel is completely out of the picture in this case. And you can imagine write in this case is just a memory write, and read in this case is just a memory read, and both are significantly cheaper or more efficient than doing write and read system calls in a pipe, right? So this, you know, this scenario where two different processes share the address space, these entities, which, you know, these are just sort of different entities that are existing simultaneously, but they're sharing the address space, are also called threads. So a thread is an execution flow. So a thread governs an execution flow, and two separate threads can have different execution flows. A process is a thread plus an address space, right? So a process basically means it's an execution flow with its own private address space, as opposed to a thread, as opposed to two threads within the same process sharing the same address space. What are some advantages of using processes and threads? Well, we have already seen the advantages of processes. You can have different processes running different programs that are completely independent of each other. One process could be waiting on, you know, could be running on the CPU. Another process could be waiting for the disk to get some data. Yet another process could be waiting on the network, and so that way you have full utilization of your hardware resources. For example, you know, I have three processes, a browser, a compiler, and a shell. The shell may be just waiting for the keyboard input, which means it's just blocked. It doesn't need to run on the CPU until a user presses a key. You know, the compiler may be actually running on the CPU, and so it's using the CPU. It's keeping the CPU busy. So while the first process is also running, it's actually, you know, the OS has been able to multiply these two processes in a way such that CPU is used in the most efficient way. And yet, let's say the browser is waiting on the network card to receive the next packet, and so on. If I didn't have this process abstraction, being able to implement this different, you know, this multiplexing would have been much harder, right? Because I would have had to have one program that understands that, okay, this part of the program is now waiting on the network, and this part of the program is now executing the CPU, etc. When you split into processes and expose system calls to the processes, the operating system has full visibility that, okay, this process is actually looking for the network, and this process is actually executing on the CPU, and that way you have more utilization. Further, when you talk about multiprocessor systems, the only way you can use other processors is if you have multiple sets of control, right? If you just have one set of control that's running, then the other CPU will never be able to run. So you need multiple sets of control to be able to actually utilize different processes, processors in a system, right? So you need multiple processes to be able to actually keep multiple processors busy, right? Same thing with threads, right? So threads give you the same kind of advantages. You get more concurrency. You get more utilization out of a system. One thread could be waiting on the network, and yet another thread could be executing on CPU 1, and yet another thread could be executing on CPU 2, all simultaneously to give you the maximum system throughput. The advantage of threads over processes is that because they share address space, they can have very fast inter-process communication. The disadvantage of threads over processes is that because they share address space, there is no isolation. So whatever one thread does, the other thread is not protected from it, right? So in other words, two threads need to trust each other. What's more, they need to be designed such that they should know the existence of the other, right? For example, the browser and the compiler don't need to know about each other because they live in separate address spaces, but if you made them threads, then they will have to know about each other and do appropriate safeguards for doing proper protection. Okay. So how does one create threads? Let's say here's a process, and instead of calling it a process, I'm going to call it an address space. So here's an address space, right? We saw that this whole system call was actually spawning a new address space and also creating a new thread of control, right? What we now want from the OS is to be able to create a new thread of control without spawning a new address space, and the only way you can, the one way you can do it is to have another system call, which creates a new thread, right? So you could have multiple threads within the same process, within the same address space, and you would want, you would need to have some system calls on your OS to be able to do that, right? Okay. If you do something like this, then the operating system is aware that these are independent threads of control, and one of them can execute on one CPU, and another one can execute on another CPU, provided the same address space gets mapped on both CPUs, right? The same memory gets mapped on both CPUs, right? Another way of doing threads or implementing threads could be that the OS has no idea about multiple threads, so what you do is, you tell the OS that I'm only one process, but internally, you have some kind of a system which allows you to emulate multiple threads of control, right? Okay, there's a question. So in this scenario, you're saying if I want, if one thread wants to exit, then it will not be, then it will cause everybody else to exit also. That's what you're saying? Yes, true. So I just wanted, so what I'm trying to show is that there are two ways to implement threads. One is to tell the OS to create multiple threads of control within the same address space, in which case threads are a first-class entity, and the OS knows about them, and the OS is scheduling them across the multiple resources that you have in the hardware. Another way of implementing threads is to not tell the OS about it, but to be able to do it internally, and this is interesting because I'm going to tell you next about how this can be done using the abstraction that we have studied so far. As you can imagine, the OS has no idea that there are actually multiple threads being emulated within the process, and so if one thread calls exit, it basically means the entire process gets wiped out. Okay. Further, if one of these threads calls a system call called read, and let's say the read was on, it's doing a read to the disk, and so all the threads will now get blocked, right? Because one thread calls read, it's as though the entire process called read, right? And so there's no real concurrency, and so the hardware resources are not being used to its fullest. Similarly, if one thread calls a read on the network, then the entire process gets blocked. So one way to do this is you wrap the exit system call into your own system call, which is called thread exit, right? And so the thread exit will basically just be stopped here, and that's going to just free up all data structures related to this thread, and the other two threads can still remain, right? So that handles things like exit. But what if I wanted to do a read from the network? That still needs to go through OS, right? Because no abstraction provides you a way of doing read directly so far. I mean, the abstractions we have discussed so far don't allow a process to do a read directly from within the process. You have to make a system call to the OS, and the OS is going to say, oh, this is the whole process that has made a system call, and so block the process until there's a packet from the network. So clearly, firstly, there is really no concurrency in this system, right? There is a concurrency. You can only run on one CPU. You block on a resource, everybody gets blocked on the resource. So two threads can physically never run concurrently. But logically, they can be made to look like they run concurrently. And that can be done by just, for example, maintaining a queue, you know, in some sense, maintaining some kind of a scheduler inside the process. And the scheduler is going to say, okay, you know, it's going to create multiple threads. I'm going to say, okay, this thread gets to run. That thread runs, and then when it yields, then you bring it back and you say, okay, now this thread, you get to run. And then when it says, I want to yield, it yields, and then you say another thread gets to run, and so on. So this is a way that a process inside itself is implementing a scheduler and sort of emulating multiple threads. Question? Okay. Let's come to the benefits a little later. Let's just understand how this is, how can this be done using the abstractions. I'm going to talk about the benefits later. The other thing that an OS does is a process should not be able to run away with the CPU. So once a process has been scheduled on the CPU, an OS should have a way of taking it back from the CPU. And the way this is done is through what's called an interrupt. So if a process is running and an interrupt occurs, the interrupt handler is the OS's interrupt handler. And the OS's interrupt handler is going to say, oh, this process has been running for too long. Let's just suspend it and let some other process run on it. A similar method can be used at the user level using signals. So a user process can register a signal, and it can say that the signal – so there's another system called alarm, which allows a process to tell the OS that a signal, let's say, should be generated every 100 milliseconds. So every 100 milliseconds, the SIG timer signal is going to get generated. The SIG timer handler is going to get called inside the process. The SIG timer handler is maintaining this data structure of all the running threads in the system, all the active threads in the system. It takes the running thread, suspends it, puts it in the ready queue, takes another thread, sets it running, and returns. So that way you also implement what's called preemption. So you can preempt a thread while it's running, and you can emulate a scheduler inside the thread. So in some sense, if you look at the abstractions we have looked at so far, the signals abstraction of Funix is paralleling the interrupt abstraction that you see in hardware. So a hardware interrupt, like a keyboard keypress or a timer interrupt coming in, or a network packet coming in, these are all activities that occur asynchronously and relatively rarely. And you want to be able to – and an OS is supposed to handle them, and the way to handle them is what's called interrupts. So an interrupt comes, and the OS interrupt handler gets called. Similarly, inside the process, a signal comes, and the signal handler gets called. Now the question is, what's the advantage of doing something like this? Well, clearly there is no concurrency achieved by having multiple user-level threads inside. So firstly, these are called user-level threads. And these are called kernel-level threads. The advantage of kernel-level threads is actually giving you physical concurrency. The advantage of user-level threads is that context switching is much cheaper for user-level threads. If I want to context switch from one thread to another, let's say one thread says, I want to give away the CPU, I want to yield, all that will happen is it will make a function call internal to the process, and the process is going to take that, put it somewhere, and set another one running. So there's no kernel crossing, there's no system call that's required, which I'm going to see is relatively more expensive. So you can do more things at the user level, and you can do them faster. Further, you can map multiple user-level threads to multiple kernel-level threads. The scheduler could be made intelligent enough so that you basically create multiple kernel-level threads at the bottom, and let's say you have two kernel-level threads, and you have five user-level threads, and it's like saying that I have two processors, and I have five processes, and then I can map them any way I like. And I can have a completely custom scheduling policy depending on my application. So a kernel scheduling policy has to be very general, because it's going to run a variety of applications. A user-level scheduler can have a very, very custom scheduling policy which suits that particular application. And more importantly, often software is written in a certain style, and you want to be able to run it in different environments. If your environment happens to be such that the kernel doesn't support kernel-level threads, that used to be relatively common, still quite common, then one way to be able to run the same program in this environment is to be able to abstract it as a user-level thread. The same code can run on this library, where you're actually fooling the program to believe that it has threads, whereas it's actually running on a single thread, single kernel thread. So anything which involves running code that was written for assuming certain things, but running in a different environment, you can use this kind of a wrapper layer to be able to do these things. Okay. All right. So we'll stop here."}