{"text":"Welcome to operating systems lecture 34. So we are discussing file systems and we saw that one file system operation typically involves multiple disk drives, right. So examples being create of a file you need to write to the parent directory, you need to write to the inode, you need to write to the data blocks, right. Similarly append to a file you need to write to the data blocks, you need to write to the index, you need to write to the inode, truncate, same thing but this time you are removing blocks from the file. Appending was adding files to the blocks to the file, truncate is just removing files blocks from the file. So let us say you truncate just removes the last few blocks from the file and unlink similar thing you are just removing a file from a directory and once again all these operations involve multiple disk drives, typically 4 to 8 disk drives per operation. And the problem you were looking at last time was what happens if there is a power failure in the middle of one operation, right. And so the problem is that it leaves the disk in an inconsistent state and that can result in many bad things to happen. The two and you can sort of categorize them into two types, one is dangling pointers where you have a directory pointing to a file which didn't get committed to disk, which didn't get written to disk and so the directory is pointing to some free disk block, right. So that is a possibility and that is a very dangerous thing. Firstly because you know when power comes back on, there is no way to figure out whether this is a dangling pointer or whether this is a real pointer, right. So if that dangling pointer is actually pointing to some block that looks like an inode, suddenly the user will have access to this particular file that it was not supposed to have access to, right. Any logic that depended on the contents of that file for the user's program is going to cause crashes. More you know, what's worse that can happen is the user can gain access into somebody else's file, right. So dangling pointers are a very bad thing and you would ideally want to avoid dangling pointers. The other problem that can happen is disk block leaks, right. Here what can happen is that you basically initialize some data and you are about to create a pointer to this data from the index. Let's say you are appending to a file and you created some blocks at the end and now you wanted to change the index in the inode and make point to those blocks and there is a path failure in the middle. So what's happened is you have initialized some blocks, you have removed those blocks from the free list, but you haven't really created pointers to them before the path failure happened, right. And so when the path comes back again, what you want to find out is that there are some blocks that are neither present in the free list and nor are they part of your directory tree or your file tree, right. And so that's a bad thing also because there are these disk blocks that will never be used and if you keep doing this, if you just keep, you know, having path failures, unannounced path failures, then eventually you will, you can have lots of leaks, lots of space in the disk that can remain unused forever, okay. But in any case, disk block leaks are less dangerous than dangling pointers. It's not a correctness problem, it's a performance problem, right. So the solution you were looking at last time was let the file system order the writes in a way such that it avoids dangling pointers. So you say, you know, one of these has to happen, you know, after all there are multiple disk sites that are happening and, you know, I cannot make them atomic with respect to path failures, path can go, get out at any time. So I have a choice between dangling pointers and disk block leaks and I'll choose disk block leaks over dangling pointers, okay. And so let, so basically the idea is that I'll order the writes to the disk such as I'll avoid the dangling pointers and which basically means that when you're adding something, for example, I'm creating a file, I will first create the file or I'll first create the, initialize the data block and then create a pointer into the index, right. So this way there'll never be a dangling pointer because the index pointer will only be created after the disk, data block has been initialized. In general, whenever there's a, there's a data structure like this where block A is pointing to block B, then you will first do this and then do this. So I'm talking about create, so you'll first create the block B and then add an entry for it in the index A. You'll first do this and then do this for create. On the other hand, when you're doing unlink or truncate, when you're removing things, you first want to remove it from the index and then deallocate or, you know, free or uninitialize these blocks, right. So if I'm doing unlink or truncate and I have a structure like this, then if I, if I, if I deallocate this first, then I have a dangling pointer. So it's better to first write to A to remove that pointer and then deallocate B, right. So you'll first do A and then B in this case, right. So first, I've written it in the other way actually, so this is a mistake. So first child or data and then parent or index and first parent and then child or index. So that's, all right, okay. So that's actually, you know, but in all this discussion, I'm assuming that all operations have to happen synchronously, which means I'm not, I'm assuming a write through behavior, right. I'm saying that if I make an operation, it should go to the disk because after all, immediately, because if I'm writing my code, I'm saying update the index, then update the, the data, the file. Then, and I want it to happen synchronously because if it's not happening synchronously and if I'm using a write back cache, then even though I did these operations in a certain order, what will really matter is in which order was the, were these data blocks flushed to the disk, right. So it can happen that in the cache, I did this first and this later, but when it was actually flushed to disk, this happened before this, right. So all, you know, there's no use for ordering if the, if there's a cache. So in general, you know, doing sync writes is not very performant, but it has this nice safety property that you can order things to avoid dangling pointers. And this was indeed used for some years in, in many operating systems in the early days, right. But we, we know that this has a very big performance penalty, but let's also look at another thing. Is it always possible to, to do this ordering? So let's say I had an operation that said move a file foo from directory A to directory B, right. Let's say I wanted to do this. And so this is, let's say the rename operation. And I wanted this to be atomic with respect to power failures. Or I wanted to make sure that things are safe, right. So in this case, there are two, two things that need to happen. Let's say this is A and this is B. These are directories. Earlier A was pointing to foo. And now you want to, let me use another pen. Let's say now you want to remove the pointer from foo and create a pointer to foo from here. Now, in this case, you may want, you know, you, you may want to say, what should be done first? Should I remove this pointer first and then create this pointer? Or should I first create this pointer and then remove this pointer? So there are two options here. I can either. So I can either do this first and this second or I can do this first and this second. In both cases, my file system on a reboot can appear in a very inconsistent state. So it is no longer the case that one of them will only result in a leak. If I do this first, so let's say I do this first and the power failure happens before I had deleted the second pointer. I have a situation where one file is being pointed to by multiple directories. And this may not be acceptable. And there's no way an operating system, so depending on the file system semantics, this may or may not be acceptable. So let's assume that it's not acceptable. And there's no way that the file system can automatically figure out what to do. So when it comes back again and figures out there's something wrong here, it will probably want to say, I either want to remove this or I want to remove this. But it has no way to figure out which one to remove. So at that point, you know, there are a couple of options. Either you ask the user, look, I see some inconsistency, a file seems to be part of two directories or a data block seems to be part of two files, you know, it's similar. Then what do you want to do? Okay, so those are options. And these are things that you may have seen in programs like FSCheck, right? So there are these programs that run on, you know, if you have not mounted your disk cleanly, then when you bring it back again, there's a program that will do a global file system traversal and figure out if there are any inconsistencies. And the advantage that ordering gives you is that you have limited the inconsistencies to only certain types. In this case, perhaps it makes more sense to do this first before that, right? Because at least you're not losing data. You have some inconsistency in your state, but at least you're not losing data. If you had done, if you had deleted first and then created, then you would have lost some data, right? So you have to basically, you have to do some ordering. And depending on what ordering you're doing, you have guaranteed that the kind of, you have limited the types of inconsistencies that can happen on a path failure. And then you have a program that will, that will be a long running program that will probably do a full file system traversal to figure out the inconsistencies and either fix them itself or ask the user to fix them first. Yeah, question. Right. So, so, so, so let's say I'm, let's say I'm using this strategy where I'm going to first create and then remove, right? And before I did a remove, there was a path failure, right? So let's say there was a crash at this point. And so I have a, I have a file system state, which has one file being pointed to by multiple directories. Now the question is, how do I differentiate? How do I know what is going on? Actually I have no idea. Right? So there's no way to figure out what was going on at that time. All you are going to do is to try to bring it into consistent state in some way. Well, yeah, I mean, so your file system may actually, so your file system may allow this kind of a thing. And this may look like a consistent state, in which case the file system may actually not throw any warning at all. And it's for the user to actually now deal with things on his own, right? But let's, I mean, assuming that the file system does not allow this kind of a thing, and this is, this is actually an inconsistency from, in the file system for semantics, then, you know, it can throw a warning to the user. All right. Okay. So, so basically, the, the idea that I've discussed so far is that you're going to order the writes on the disk in a certain way to limit the types of inconsistency that can happen. And then you're going to run a program, in case of an ungraceful shutdown, when you come back again, that program is going to do a global scan, and it's going to figure out what inconsistencies there are, and trying to resolve it either automatically or using users. Right. So this, this has actually been used for a long time in lots of different operating systems. And perhaps you have seen this also in, in the regular operating systems. But, but this, this has a few problems. Firstly, as the disk size becomes very large, the, the time it takes to run this FS check program, which is a global scan of the structure, becomes very large. Right. So just to give you an example, you know, a random read. Yes, question. Okay. All right. So, so the question is that we have also said that for reliability, we will duplicate state. Some state will duplicate. In the case of a file allocation table, I will, I will keep two tables. Right. And so can that help in a resolution of such conflicts? See, when we duplicate state, we are basically doing it for reliability against stationary disk errors, you know, errors that can happen over years and things like that. And you don't want duplicate state, you know, you won't design a file system such that duplicate state basically involves updating twice for every update. So, you know, you won't design your file system typically to say that every time I make an update, I'll have two copies of every inode. And every time I make an inode, an update to the file, I'm going to write twice to two different inodes. Right. One could do that potentially. And you know, one of the solutions we're going to look at is similar in spirit. But but that's not that's not a very performant design in the general case. See, basically, whenever you're doing systems design, you basically want to say, I want to speed up. I want to make my general case as fast as possible and yet cover for the rare cases. Right. So it's very it's a bad design to actually slow down your general case to take care of rare case. And if you're doing that, then that's a you know, that's basically an example of that. OK. So. So recall that a random. Speed. X. Ten milliseconds. Right. And so basically, that means that if I was to do a check, then. The number of the time it is probably take me to do a full check would be somewhere somewhere like number of inodes, which is representing the number of files in the system. And let's say each inode is taking one random read. So you're going to take an inode by 100 seconds. Right. You can do 100 inode reads per second. So depending on the number of files, you will do and I know it's about 100 seconds. And that's really slow. If you have thousands of files, that's easily an hour right there. As a data point, if it's checked, it's around. Ten minutes. For 70 GB disk. With. Two million inodes. So clearly, in doing so, it's not it's instead of doing a random read per inode, the better thing to do would be if you're actually doing a full file system scan, the better thing to do would be doing a do a sequential read to read up all the inodes into memory and then do your traversal. Right. But even if you're doing that. So this is the optimized statistic. So even if you're doing that, a disk, which is roughly 100 GB, takes tens of minutes. Right. And a disk that's 10 terabytes will take hours and so on. And so as the disk started to become larger and larger, this idea of doing an FS check on a reboot started becoming less and less practical. OK. So. So. So I'm going to discuss another method to be able to do this cache recovery. But before that, let's also discuss. We have discussed one method, which is ordering. And. With sync, synchronous writes by synchronous writes, I mean, right through cache. Whenever I write, I write straight to disk. There's no there's no write back going on. The other thing about synchronous write is it's very slow. Right. So let's say I just imagine that you were to untar a tar file. So and let's say the tar file has a thousand files inside it. So just untarring the tar file requires creation of a thousand inodes and creation of each inode if it's a sync write, it's going to take 10 milliseconds. So a thousand inodes will take, you know, 10 seconds to create, to untar one small tar file, which has a thousand files only. Right. So sync writes is not very practical. On the other hand, if you had a write back cache, it would have done this whole untar operation in almost zero milliseconds or less than, you know, less than a millisecond, basically. So you want write back. And so the way this is done, if you wanted to implement ordering with write back cache, you would make the updates in the cache, but also store in the cache in memory, the ordering dependencies between disk locks. Right. So let's say I wanted to do a create. I'll first write to child inode, initialize it, and then write to parent inode, parent directory inode. That's it. And so let's say these are the two things I do to them. A few other things that have to be done, but let's say there's these two things to be done. You have to write to the child inode and then you have to write to the parent inode. So what you'll do is you will do these writes in the cache, but you will attach some ordering number to these. For example, you will say that this should be done. There's an ordering dependency between the second block and the first block. So the second block should be flushed only after the first block is flushed to this. So at the time of actually doing the replacement or flushing or writing it back in a bunch, you're going to make sure that the things are ordered. So for example, you have multiple disk blocks. Let's say this is A's disk block and this is B's disk block, and let's say A and B are directories. Then if I say move foo, A slash foo, to B slash foo, and I want to make sure that creation happens before unlink, then I'll say that I'll draw an edge from B to A, saying that B should be flushed before A is flushed. Creation should happen before unlink. On the other hand, let's say after that, somebody executed a command called mv B slash bar to A slash bar. Somebody executed yet another command, and so this time you wanted to draw an edge like this. This time you're moving in the opposite direction, so you basically want to create, you want to first create a link in bar, and then remove the link from, in B, in A, and then remove the link from B. This time you basically want to say that this should happen before this. And here's an example where this dependency graph would have a cycle. If it has a cycle, then at the time of flushing it back to disk, it's unclear which one you should flush first. Let's say you flushed A first. Then you have lost, it's possible that if there's a crash that happened after you flushed A, then you may have lost the link to foo. The foo content may have been lost. On the other hand, if you flushed B first, then bar's content could have been lost. So here's a cycle, so how do you resolve something like this? Okay, so here's a suggestion. I'm maintaining ordering at block generality. I'm saying this block should be committed to disk, or written to disk before that block. Without going into semantics of what is inside the contents of the block. And the suggestion is, instead of saying that this block should be committed before that block, say these contents in this block should be committed before those contents in that block. That's not a practical solution, right? Because there's too much semantic information that needs to be stored, and it's basically almost like writing, at flushing time you have to basically look at the semantics of what's written, at what bytes that has been written, if it's a directory or etc. I mean, these are the kind of things you don't want to worry about at flushing time. All you want to care about is, here are some disk logs that are in the cache, they are dirty, they need to be written back to disk. What's the other thing you can do? Well, here's one suggestion. When this operation gets executed, in memory, the OS figures out that, hey, there's going to be a cycle in your dependency graph. So when you see that there's a possibility of a cycle, you stop that operation, and you flush these disk logs to disk, such that this edge gets removed. Once you flush them to disk, the old edges will get removed, which means the old state will get consistent, and now you can perform this new thing. So every time you're making an operation, you check the dependency graph, if you see a cycle, you hold on, you flush the blocks that are involved to the disk, so that the old edges get removed, and so the new edges can get created without having a cycle in your dependency graph. Okay, so ordering with writeback and coupled with an FSx program has been a popular solution for a long time, but with growing disk sizes, it has become relatively unpopular. And the other way to do cache recovery is logging. Let's see how logging works. So the idea is that, let's say there's some system call, let's say there's create, and it's going to do write to block 10, then 100, and so on, right? And then, and that's it. And you want to make sure that these operations are atomic with respect to power failure. There's some operation that's going to have multiple disk writes, and you want to have them atomic with respect to power failure. Here's one way you could do it. Each time you see something like this, you basically don't write the disk loss to the file system at the time, at this point. You keep recording these operations in a log. So basically, start logging, you maintain a separate data structure called a log on disk, and each time you want to log, write something, you basically say, you basically log this operation. So you say that I want to write to block 10 with these contents, and you put that in the log. When you put it in the log, you're basically saying, you're basically indicating your intention that you want to write to this block. You have not actually written it to the disk block, right? You log each operation, log each write, I should say, to log, right, to disk. After you're done logging each of these writes, you append a commit record to log on disk. Basically, first write that I want to write to all these blocks, and then you append a commit record to the log on disk, right? And your operation is complete only after the commit record has been written. If there's a power failure before the commit record has been written, it's as though none of these writes ever happened. If there's a power failure after the commit record has been written, it's as though all these writes have happened. So let's see. I have a disk. Let's say I was to draw the disk data structure. So I have some tree-like structures, you know, different types of trees. One is a directory tree, and other is an inode tree, and so on. So there are tree-like structures on the disk. So this is, let me call this the file system. And then there is a sequential structure, which I call the log. Each time I want to make an operation, I basically start writing to the log. And then I write a commit. Recall that we made the assumption that the write of a sector is atomic to the disk. So either the entire commit will be written, or none of the commit will be written. It's not like half of the record can be written. So this operation is atomic, and you're using this operation to basically make the entire operation, the entire sequence of writes atomic. And after you've written the commit log, commit block, you're going to asynchronously make these writes to the file system, after you've written the commit log. Let's see once again. I wanted to make an atomic operation, which involved multiple disk writes. I will create a new transaction. So this entire thing can be called a transaction. So I'll create a new transaction. I will log the blocks that I need to write, and log them to the log. And then I'll write a commit record to the log. After that, I'm done. As far as I'm concerned, the disk is in a consistent state. And asynchronously, I'm going to flush, I'm going to copy the blocks in the log to their respective positions on the file system. Recall that all these logs have annotations saying block 10 contains x, block 100 contains y, and so on. And so asynchronously, I'm going to write these logs from log to the file system. Now let's see what happens if there's a crash. If there's a crash before the commit record was written, no problem. It's as though nothing happened. If there's a crash after the commit record has been written, but before you have started applying the changes to the disk, no problem. It's as though the operation has finished atomically. And now you can do the same thing. At recovery, you can just apply the log to the file system. What happens if the crash happens in the middle of application of this log to the file system? Let's say I've written the commit record. After that, I was asynchronously writing block 10. And before I could write block 100, there's a power failure. No problem. At recovery time, you will again write 10, but with the same content, so there's no problem. You'll just overwrite the 10 again, but with the same content. That's no problem either. So this is nothing but a write-ahead log. Here, whatever you want to write, you write it to the log first, and you keep doing this, and then you write a commit record. After that, you actually push all those writes asynchronously to the real file system. We are all convinced that this will ensure a consistent state, even across random crashes in terms of power failures. Notice that the write of the commit record on the log is acting as a serialization point. If a power failure happens before the commit record, it's as though the transaction didn't happen at all. If the crash happens after writing the commit record, it's as though the transaction happened in completion. Even if there's a crash that's happening in the middle of your application of the log to the file system, it's still consistent. This data structure of the file system plus log will always remain in a consistent state. There's no inconsistency that can happen. So how does one write code to do this? Let's say I have a syscall, like create. I'll just say, begin transaction. Then write, write, write, read, etc. And then I'll say, commit transaction. All I've done is that all the operations that I wanted to make atomic with respect to power failures, I've bracketed them with begin and commit transaction calls. It's very similar to your locking, acquire and release. Begin transaction is going to write to the log that I'm starting a transaction. All these operations are going to append records to the log. And then commit transaction is going to write the commit record on the log. And there's going to be another thread that's asynchronously going to apply the log to your file system. It's very easy to write. Basically, the code structure doesn't change much. You just have to enclose things that you want to make atomic with begin transaction. After you're done applying the transaction to the file system, you can delete the log. You can delete the transaction from the log so that it can get reused. But till it has not been applied to the file system, you have to keep it around. But after you have applied it to the file system, you can now free it for use for the next operation. Okay, so what's wrong? Or what don't we like about this? Performance, right? What are some things that are happening? Firstly, everything that I need to write, I need to write twice. I need to write first to the log and then to the file system. So every operation basically has a 2x overhead in some sense, right? So 2x, right? Second thing is I log whole blocks even if few bytes written. Even if I just update one byte in the block, I log the entire block in my log. So my log space overhead is larger than what you would have wanted it to be. Eager write to log. Very eager to write to the log. Every time I do a write, I basically immediately want to write to the log, right? And as soon as I do a commit, I want to basically make sure that the commit record has been written to the log. And then later I'm going to write it to the file system. So is this avoidable? Can we not have eager write to logs to the log? We'll see this in the next lecture. But as it stands now, this thing has a very bad performance. Each time I want to make a write, I actually need to go to the log and make this write. One simple optimization could be you don't do the write to the log as they are being done. You wait for the commit transaction to happen. And then all these four or five records that you've written can be written in one batch to the transaction, to the log. This is a small optimization, but this is still not good enough. Why is this not good enough? I still need to do this eager write on every commit. So this is not write back in the true sense. Every operation needs to be synchronously committed, every atomic operation. I would have wanted that even atomic operations can be written back using a write back hash. I want my write back hash to have more freedom or more batching than just four or five disk operations. Then write to file system. So write to file system, as we discussed, can be done lazily. This is okay. Here we are saying that once you've written it to the log, you basically lazily apply the log to the file system, and you can batch it and get a lot of performance. Okay, there's a question. Okay, good question. I'm saying there's an optimization that a transaction need not be written to disk, or the blocks in a transaction need not be written to disk eagerly. You can keep them in cache, and then at commit time, write the entire log transaction to the log in one go. And the question is, when you're writing it back, if the commit is written before the other blocks, then there's a problem. So at the time of touching it, you have to basically make sure that there's some ordering in which you're doing it. Typically, the way this will be done is that you will issue all the transaction records in one go. So all the transaction records happen in one go, and then the commit record happens in a second iteration. So in that way, you have two sequential writes to the disk. So you waste one full rotation. Assuming that the log is written sequentially, you first make one sequential write to write the entire transaction blocks, and then you make one sequential write to write the commit. And just to make the ordering, you have to do two instead of one, so that the disk doesn't reorder them internally. Modern disk interfaces allow you to specify that here are five writes, but make sure that this sixth write is after all these five. So in that way, you can even avoid this extra overhead of multiple writes. Okay, so we have seen logging in its very raw form. Basically, we are using the log. We are eagerly writing to the log. We are logging the whole block, and eagerly in the sense at commit time. Also, we are making another problem with this. The thing is that only one transaction at a time. Why is this true? Let's see. Let's say I have this code. Begin transaction, and then I start writing something, and then I commit transaction. To ensure atomicity between multiple accesses to the file system, the way I have discussed this so far, you basically want to make sure that only one transaction is active at any time. If the multiple transaction is active at any time, then there are more problems to be dealt with. So the way we have discussed this so far, there's only one transaction that can be active at any time, and that in itself is a very big performance problem, because if there are lots of users running lots of programs to completely different parts of the file system, they get serialized because of this common log across the entire file system. It's not a good idea at all. All right. Let's see how we can fix it. I'm going to look at the ext3 file system on Linux. The ext2 file system was basically based on the ordering and FS check program that we discussed earlier, but the ext3 file system introduced logging, but in an efficient way. We're going to see exactly how this is done. We saw some problems with the way that we have discussed so far, and let's look at what can be done. Firstly, a transaction is not one operation but multiple operations. If there are 100 creates happening at the same time, they are all made part of one transaction. So a transaction basically represents some kind of locality in time. So basically say I start a transaction, and all the file writes that are happening now will belong to this transaction. And then at some point I'm going to say stop transaction. When I say stop transaction, all the operations that are completed belong to this transaction, and all the operations that are ongoing, I'll wait for them to complete. And when they complete, all these operations together form one atomic unit. And this atomic unit gets flushed to disk in a log, and then there's one commit record for this entire big chunk of writes, possibly by multiple users, multiple processes, different parts of the file system, completely different operations. What you're doing is you're clubbing lots of different atomic operations into one large transaction. So basically what this means is that these begin transactions and commit transactions can be replaced by begin operation and commit operation. And it's not necessary that at the time of committing the operation, you actually commit the transaction on disk. You just basically say that the operation is finished. And so the invariant is that a transaction will have either the entire operation in it or none of the operation in it. And a transaction will never have half of an operation. And so you basically club lots of different operations into one transaction, and so that gets rid of some problems. So, for example, only one transaction at a time, you've solved it. You can have lots of different operations at the same time, and then you can choose to commit them at your own will. How do you choose then to commit a transaction? You can say every five seconds, every 30 seconds, completely reasonable choice. So, for example, modern operating systems don't give you any guarantee about if you're writing something, whether it's actually persistent on disk or not. But every 30 seconds, let's say, the transaction will get closed. And so because the transaction got closed, it will get written onto the disk, and at that point you can be sure that whatever you wrote 30 seconds ago is very likely on the disk now. So this interval, so you can just choose to close the transaction at will, whenever you like. And when you close the transaction, at that point you make sure that all the operations that started before the close of the transaction will get committed on disk. You also fixed eager write to log. So now you can, because you are using a transaction worth of 30 seconds of write, you can actually batch all of them and write them all together, and so it's not an eager write to log. You actually, instead of writing six operations, you're writing 6,000 operations together to the disk. So that's much more efficient. So this one is already good. Write to FS is lazy. Log whole blocks even if few bytes are written. So they still log whole blocks even if few bytes are written, because it's very hard to keep track of which bytes have been written and which not. You log the whole block. So the nice thing is because you are making a transaction so big, if there are multiple operations that wrote to the same block. You can hear me? Okay. All right. So you log the whole block even in EXE3, but the nice thing is because you're doing lots of different operations batching into one transaction, you don't have to... These blocks may have been modified many times within the same transaction, and you don't need to record all those blocks, all those different versions of the block. You only need to log the final version of the block. So let's say if this block was modified 1,000 times in this 30-second interval, all you need to log to disk is the last value. You still log the whole block, but you just absorb lots of writes. So this also becomes better. And 2x writes remains, but the nice thing is because you are batching so many writes together in log, it's a complete sequential write to the log. So that's very fast, as we know. There's only one seek and one rotation, and you do a write at 50 to 100 megabytes per second to the log. So that's very fast. Also, now all these writes now need to be applied to file system, but that's a huge transaction. So because it's a large transaction, you can have lots of IOs and writes simultaneously, so writing to the disk is also faster. You have much better disk bandwidth utilization. So essentially, the larger the transaction, the less the 2x write problem is. And because I made my transaction as large as I want, there's no problem. Okay. So let's look at how it works. Let's look at an easy transaction. Open one transaction at a time. On start op, add the op to ongoing transaction. This is all in-memory operations. Each time somebody starts an op, you basically add the op to the ongoing transaction and commit transaction every few seconds, every few tens of seconds, depending on what you want, how much reliability and performance tradeoff you want. Let's see how you commit a transaction. Firstly, open a new transaction. So when you are committing a transaction, you basically say that all new ops will now belong to the next transaction. You basically first close your current transaction, which basically means any operations that are happening after this point will now belong to the next transaction. Mark this transaction as done. Wait for in-progress ops to finish. There could be some ops that have started but have not yet committed or haven't stopped, finished. When you close the transaction, there are some partially completed ops. You just want to wait for those ops to finish before you actually write the commit record. That's how you're basically maintaining atomicity. You have basically said that I can close the transaction at any point you want, but when you close it, you also wait for all ongoing or all operations that have started before this close to finish. That's how you basically ensure the atomicity of each operation. Then write descriptors. Descriptors are basically these blocks which contain meta information about which block, which block, which version number, etc. And then what's the data? Descriptors and blocks. Log and wait and then write the commit record. And that's it. That's your commit transaction. And then asynchronously, you are going to write the contents or whatever the log is saying to the file system. After you've written the commit block, you can now allow the blocks whose contents have been logged in the transaction to now get written to the actual file system. All right, let's stop here and let's discuss the exe file system in more detail next time."}