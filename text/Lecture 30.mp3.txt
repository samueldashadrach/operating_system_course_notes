{"text":"Welcome to operating systems lecture 30, right, so last time we were talking about page replacement algorithms and we looked at random, we looked at CIFO, we also looked at a hypothetical algorithm which we called MIN, which is less than optimal, which assumed the knowledge of future and we said that assuming that past is equal to future, LRU is the best approximation of MIN. So assuming that past approximates the future, LRU approximates MIN. But then we said LRU is too difficult to implement because you need to put a time stamp on every memory access and recall that memory accesses are on the hit path and they need to execute on a, you know, very fast, so putting a time stamp on every memory access is a very, very expensive operation, so it's not really practical in general. So LRU, so implementing perfect LRU is hard because you need to put a time stamp on every memory access and that needs to be on the hit path and that, you know, so putting a time stamp on every memory access is not possible, instead CLOCK is a one bit approximation to LRU where the time stamp is just one bit, which says whether it was accessed or not, so there's this one bit and this bit is stored in the page table entry, recall that there were these extra bits in the page table entry and so one of them is used on the X86 hardware to represent the access bit and so the hardware basically sets the access bit, so the first time the page, if the access bit was 0 and the page gets accessed, then the access bit becomes 1. If the access bit is already 1 and the page gets accessed, nothing happens, right, it just remains 1. So that's a one bit approximation to LRU and we said that, you know, that basically, so we basically use FIFO except that we use this one bit to distinguish between pages that were accessed recently and that were not accessed recently. Pages that were accessed recently are not replaced, they are instead moved to the other list for the first time, so you basically examine these pages in FIFO order and if you see a page that has been accessed recently, you clear its access bit indicating that it has been moved to the lower level but if you see a page in the lower level in FIFO order, then you basically evict it, the first page you see it, right. So that, you know, another way to think about it is that it's a clock and you keep a pointer to the last page that you accessed and you move the clock in one direction and these clocks have, you know, access bits 0, something like this and each time you come across a page whose access bit is set, you set it to 0 and if you see something which is already 0, you evict it, right and because you are storing the pointer, the next time you run the page replacement algorithm, you are going to start from where you left off, so it turns, it looks like FIFO in that sense, right, basically you are just advancing the pointer from where it was last time. What is the logic behind setting the one, setting the access bit from 1 to 0? Well, I mean basically the idea is that if a page was, is 1, then I set it to 0, so that I want to observe whether it is, you know, I have seen it at once and now the next time I am more interested in seeing whether it was accessed in the last revolution or not, right. I do not want to carry over information from 10 revolutions behind, I am not interested, I am only interested in knowing whether it was accessed in the last revolution or not and to be able to do that, I need to clear it, so that it gets set again. Can we traverse all of them and set all of them to, from 1 to 0? I mean this is, this is in some sense doing that except that it is not a global operation, you are just going in a sequential way, I mean the net effect is similar, right, you are just going one by one and whichever you see as 1, you set it to 0 and so each page is getting an equal time to get accessed, which is one revolution or roughly equal time, you know, on average or it is all symmetric anyway, right and but of course, we said that you know, if the number of pages is really large and the time that a page gets to prove that it is actually being accessed recently may be too large for your, for what you need and so you may want to use a two hand clock and just say that the leading hand is going to clear the bit and the trailing hand is going to evict, so the leading hand just clears the bit if it finds a 1 and the trailing hand evicts if it finds a 0 and so for a page to not get evicted, the page should have been accessed between the leading hand and the trailing hand, that is the only way the page will survive eviction, it will not be able to you know, it will survive with the pass and so you can tune that by using and so this angle is fixed between the leading edge and the trailing edge, let us call that theta, so this angle is fixed and depending on how much time you want to give to a page to really allow it to be called accessed recently, you will choose theta, if you choose theta to be too large, you are basically using, you are giving it the page more time, if you choose theta to be too small, then you are giving page too less time and you know, extreme cases are if theta is equal to 0, you are actually not giving the page any time at all and just fee for at that point and if you make theta equal to 360, then you are basically back to one hand clock, right, all right, so there are some, so you know the clock algorithm is fairly successful and presents as actually being used for many years to do page replacement and here are some more extensions to the clock algorithm that are usually used, so one is you know replace multiple pages at once, instead of replacing one page at a time, so each time you get a page fault, you are actually just obliged to replace one page, instead of replacing one page, you may want to replace multiple pages at once, so that you would save you know, so firstly if there are any 30 pages, you can write all of them in a batch to the disk and so as we know that a disk is dominated by the seek and the latency time, rotational latency, it is better to write a bunch of pages together rather than one page at a time, also it saves, so yeah, so I think it is basically about you know, saving the number of writes that you can have, another sort of straightforward extension to the clock algorithm is what is called the nth chance algorithm, so one way to think about the clock algorithm is it is also called the second chance algorithm, where pages that have been accessed recently are given a second chance, right, so you basically process all your pages in C4 order, but if you see that a page has been accessed recently, you give it a second chance, which means you set it to 0 and you give it another chance to basically prove itself once again, right, and if it is able to prove itself again, which means it gets accessed recently, you give it yet another chance, but if it is not able to get accessed recently, then it gets evicted, so it is a second chance algorithm. The nth chance algorithm is just a straightforward extension, you say instead of giving it two chances, give it n chances, so the idea is that each time you see you know, so you go along the clock, you see one bit, you set the counter, so we maintain a counter with every page, which says how many times you have seen this page to be not accessed, right, so in how many rounds have you seen this page to be not accessed and each time you see a one bit in that page, you clear the counter to 0, basically means it was just accessed recently, so the counter is 0, if you say see it as 0, then you increment the counter, you do not replace it immediately, you increment the counter by 1, right and when the counter reaches n, which is where n is the you know, nth chance of n, then you replace it, right. So, let me just write the nth chance algorithm, make it more clear, counter per page on sweep by the clock hand, if you see a 1, then clear counter, if you see a 0, then increment n, increment counter, also if counter equals 1, replace or evict. In other words, the clock algorithm as we seen at the second chance algorithm is just nth chance with n is equal to 1, the first time you see it as a 0, you increment the counter and the counter becomes equal to 1 and you replace it, did I say m1, it should be n. The clock algorithm is the nth chance with n equal to 1 but if you want to give a page more chances to basically prove itself, then you know, you increment n, the more the higher you have the larger n, the nth, the more revolutions you have to make before you actually evict a page, I do actually see the page as not access for at least, you know, n revolutions before you actually evict it, you are giving the page more chances in that sense. Also, the higher the n, the closer you are to LRU, right, so basically let us say n was equal to 1000, you are basically saying that this page has not been accessed a 1000 times and so this is the best candidate to get evicted, right. On the other hand, if there is a page that was accessed in the last 1000 revolutions before you know, we will get priority over, so you are getting more fine grained, you are distinguishing between a page that was accessed a 1000 times and a page that was accessed, that was not accessed across a 1000 revolutions and that was not a page that was not accessed across 900 revolutions, right. The higher the n, the more fine grained you are getting, you are distinguishing between pages that have been not been accessed a 1000 times, for a 1000 revolutions and a page that has not been accessed for a 900 revolutions, so the higher the n, the more closer you are getting to LRU, the downside is the more work you have to do to find a page to replace, right, okay, alright, so you know, if this is, so the nth chance algorithm with a large n is one way to approximate LRU but in general, clock algorithm works just as well as LRU, LRU is anyways an approximation to the what is optimal and clock algorithm is an approximation to LRU and in general, it works good enough, so you do not need to do these nth chance or kind of things or you do not need to have a very large n to basically have good hit ratio, right, the whole point of doing all this is basically to have good hit ratio without having too much overhead on your hit path and also on the miss path. One common thing that is done is treat dirty pages preferentially over clean pages, so when you are doing replacement, you can imagine that if you make a dirty page, you will have to do more work because dirty page eviction requires a write to the disk, a clean page eviction does not require a write to the disk, so keeping this in mind, you may want to give more priority to your dirty pages, so you may say oh, this is a dirty page, let me give it more priority over your clean page, so if I have a choice between dirty and a clean page, I will probably pick the clean page to evict, right, so you know one common thing that sometimes done is use the nth chance algorithm with n is equal to 1, 2 for dirty pages and n is equal to 1 for clean pages, so evict the clean page the first time you see it not accessed, evict the dirty page the second time you see it not accessed. So, in this scenario, you make it clean, then n is equal to 1, right, so let us say you went through that page and you figured out that it has not been accessed in the last revolution, so you do not evict it immediately but you also say that you know, let us just write it back to disk, so that by the time I you know, hopefully by the time I come to it next, it becomes clean and you know this write back to the disk can be done in an asynchronous fashion, in a batch fashion, so lots of pages, lots of dirty pages can be collected together and written back. So, if a page has not been seen access in the last revolution which is you know, at n is equal to 1 for the dirty page, you can just put it in your queue to be written back to the disk and you know, because it is likely to get replaced in future, there is a you know, there is some indication to use there, so you make it clean at n is equal to 1 and you replace it at n is equal to 2, right, for clean pages you can just replace it at n is equal to 1, so just one example of where nth chance algorithm is actually used and to preventively treat dirty pages over clean pages, all right. So, let us just understand where LRU works and where LRU does not work, right, so LRU works very well, basically LRU captures the recency, how recently you have accessed the page and in general our workloads are actually you know, have a lot of similarity between past and future and so recency works. But here is an example, here is a common example of a pattern where LRU does not work, so let us say you have a page, you have some physical memory m and you are accessing a set of pages p such that p is greater than m, right, so you are going you know, let us say accessing 1, 2, 3 dot dot p and you are just accessing these pages in a let us say repeatedly in a cyclic manner, 1, 2, 3, 4, 5, 6 till p and then you come back to 1 and so on. Now, in this case let us see what happens, what with LRU, so you access 1, 2, 3, 4, 5, 6 till m, all of them are misses in the first iteration and so but all of them get into m, so your m basically becomes 1 dot dot m minus 1 or 1 dot dot m actually, when you access m plus 1 and which is the page that you replace, 1, right, so you replace 1 and you put m plus 1 here and you replace 2 and put m plus 2 here and so on, right, till let us say, till let us say p, p minus 1 and then you access 1 again. The problem is that you know, 1, the page that you replaced is also the page that you are going to access, so here is the case where past is not equal to future, the page that was accessed least recently is actually the page that will be accessed most closest in future, right, in this sense, in this case instead of replacing 1, it would have been in fact better to replace the page m because m is the page that is going to be accessed latest in future, right, so basically you know, instead of LRU perhaps, MRU most recently used which is you know, completely counterintuitive would have worked in this sense, in this example workload, just take the page that was most recently used, that is the page that is you know, going to be accessed farthest in the future and so that is going to have some hit rate, LRU in this workload will have completely zero hit rate, you know, all the pages will be, all the accesses will be missing. And this kind of a pattern is actually quite common for example, you know, a process going through an array of just scanning its array linearly or going through its data structure like a tree or whatever, so basically any linear scan and repeat it is quite a common scenario in real workloads and that is why we have to worry about it, okay, so what happens with the scan, so this kind of a workload is also called a scan and notice that the worst performance or the you know, LRU seems to perform the worst if p was equal to just 1 greater than m, so let us say p was equal to m plus 1, so basically for the m plus 1th page, you replace the first page, whereas the next page you are going to access is 1, so you are basically replacing the page that is most likely to be accessed next, right and so this kind of pattern is called a scan and the problem with scan is that let us say typically your operating system will have multiple processes running or multiple threads running or at least multiple access patterns, so there is somebody who is accessing it in a linear scan and there are other parts of the program that are just accessing it in a very differential way, that means they are always accessing the same locations again and again, but one scan can actually pollute the cache by bringing in lots of cold pages into your cache, so all your hot pages get flushed out and the cold pages because of the scan get placed in your cache and so it causes a lot of misses, ideally it would have been nice if your algorithm could figure out that hey, this is a scan and so I do not need to do any caching for these pages, right, anyway these are going to be misses and rather I should focus on doing caching, I should focus on retaining my hot pages, these all these P pages are anyways cold pages, so let them have misses, these hot pages should not get polluted because of the cold pages and the reason this problem is occurring is because LRU is only looking at recency, it is only looking at what was the least recently used and throwing it out, it is not looking at frequency at all, so if there is some hot page which is getting accessed 100 times and there is this cold page that was just accessed once recently, the cold page gets preference over the hot page just because it was accessed more recently, so ideally we should have some way of figuring out, some way of combining frequency also with recency, alright, so you know there is an algorithm called least frequently used but this is also a very impractical algorithm, basically say what is the least frequently used page but the problem with this is that it has a long memory, you know a page that was accessed a lot of times one hour back may just keep polluting your cache which is not known, typically your work, typical workloads will perform badly in LFU. So typically you combine recency and frequency in some way and so you know LRU is not used in its plain form and there are examples of algorithms which extend either LRU, so there is an algorithm called 2Q, there is an algorithm called clock with adaptive replacement and so on which basically and there are many others, so most operating systems will implement some variant of LRU that is scan resistant, right, so because scans are common workload patterns and any good algorithm should also be scan resistant and LRU is not, okay, alright. So one algorithm that I will just to give you an idea flavor is called LRUK, here the idea is that you do not evict the page that was least recently used, you for each page you track the last K references, right and then you evict the page which has the least time stamp for the Kth reference in the past. So in LRU we were just looking at the last reference for the last accesses, instead of the last time stamp for the last access, you look for the last time stamp for the last Kth access and that is the one you replace, so you know LRUK is basically giving some preference to frequency and but LRUK is also very expensive to implement in the context of an operating system but LRU2 is used in databases. Okay, alright, so now let us talk about how a cache replacement algorithm is implemented and there are two choices, global versus local or per process, should I do replacement for all the processes as considered as one pool or should I do replacement for each process separately, right. So there are two options, I can consider the memory used by all the processes as one global pool and so if one page process page faults, I will consider this whole pool as one replacement pool and I can take page from some other process to serve this process and the other cases I have separate pools per process, right. So clearly if you use global, then you have more optimality overall, if you have per process quotas of memory, then it is possible that one process is under utilizing its memory and another process is over committing its memory and so you know you have artificial barriers and that is causing extra page faults and you actually need to. On the other hand, global replacement has a problem that one process can actually run away with all the memory causing all other processes to become perform very poorly, right. So a typical security attack or you know performance attack on a machine could be that you spawn a process and it just touches a lot of memory, right and just keeps touching a lot of memory and so it is just bringing in a lot of memory so all other processes sees much less memory than actually available on the system and so you know the operating system has to balance optimality or efficiency with some level of fairness, okay. So in practice you basically use, so in practice there are two things that are typically used, one is you know either they use completely global replacement, so you know just global replacement assuming you have a very large memory pool and your caching parameters are correct you know it is very hard for one process to actually pollute the cache of other processes. The other way to do it is basically have per process pools, so you have per process quotas, let's say you know I have a quota for, let's say this is my complete memory and I have some quota for process C1 and I have another quota for process P2, right and then I basically if process P2 is taking lots of page faults then slowly I will move this barrier in one direction or the other, so if I find out that P2 is taking lots of page faults and P1 is under utilizing its memory then slowly maybe on the order of minutes I will basically figure out and not try to move this quota allocation one direction or the other. The important note there is that the movement of this allocation boundary is much slower than the boundary, than the actual page replacement rate, right, so that gives you both some level of fairness and yet not too much inefficiency, right, so it gives you fairness because P2 gets, P1 gets isolated from P2 to some extent, at the same time if the operating system figures out that P2 actually needs a lot of memory and P1 doesn't need that much memory then there is some adjustment at a coarser time level granularity to make, to account for that, alright, okay, good, alright, so okay, so this is you know, so we have discussed demand paging and we said that you know, pages are brought as and when they are required and this page replacement algorithm going on and in general if a process takes a page fault it, you know, it has to wait for a disk request and while the disk is actually serving this process you will probably run another process and so you know the process, the cost of page faults actually gets hidden and the page faults in some sense become free, right, but let's see what happens if memory gets over committed, right, so let's say the sum total of all the memory that is required or all the active memory that is required by all these processes is greater than the physical memory that you actually have, right, so let's say you know the process is running and you have physical memory but you know the total amount of memory is smaller than the amount of physical memory that you have, this is greater than this, so this is let's say the active set, active pages of all these processes and this is the physical memory and this is greater, right, so what's likely to happen is processes are likely to take lots of page faults, so let's say this process takes a page fault, it's likely that it will go to the disk and it will replace one of these pages and these pages and so one of the pages that are actually resident is going to be replaced and because all these pages are active, most likely you will replace an active page and so because you have replaced an active page, very soon you will get another page fault because of that replacement and so eventually what will happen is all these processes will just keep taking page faults, right, so one process takes page fault, it replaces the other process's page, that process takes a page fault, it replaces this process's page or its own pages and so you're basically spending a lot of time on page faults and so what's happening at this point is that the system is spending a lot of time taking page faults and reading and writing pages from the disk and not actually executing useful instructions. The program was written to execute useful instructions but these instructions are just page faulting and for no fault of theirs, it's actually the operating system that was trying to play tricks under the carpet and providing a very fancy version of a large address space with performance latency of physical memory but at this point what has happened is you have an address space, alright, but the access latency is not of physical memory, the access latency now becomes of the disk, right, so each memory access now becomes disk latency access at this point and so this situation is called sashing, right, so at this point the operating system is sashing or your computer system is actually sashing which means most of the time is actually being spent on reading and writing data to and from the disk and not actually executing useful instructions. And you may have seen this in real life, in practice, if you spawn too many processes, you may have seen that sometimes the computer becomes really slow depending on, you know, if your memory is not big enough to handle that many processes and, you know, let's say your flickering light for your hard disk access is continuously flickering which means the hard disk is continuously being read and written to but the computer system as a whole seems to be very slow and really at that point your system is sashing, right, okay, so and so typically what do you do to resolve the situation in practice? You close some processes, right, and so why do you expect that if you close some processes things become better? Because if you close some processes then, you know, they will reduce the set of active pages, so some processes are not active, so their pages have been removed from the process and now the set of active pages fits in the physical memory and so at this point you can now start executing from physical, at physical memory speed back again, alright, okay, so an operating system automatically also can try at least to do the same thing, so how does an operating system deal with sashing, so let's say dealing with sashing, well, if just one process is causing all the sashing then there is nothing the operating system can do about it, that one process is actually accessing so much memory or so many active pages that it cannot fit in the physical memory and there's nothing the operating system can do about it, the maximum it can do is, you know, schedule that process less often so that other processes are not affected or maybe even kill that process, right, so those are the only two options if just one process is causing all the problem, the more common case is that the sum total of all the running processes is causing all the problem and so in that case the operating system can do something like that, it can figure out which process is using how much memory and then schedule the processes in groups, so have two groups, one is active processes and other the inactive processes and so at a time you will have only some set of processes in the active processes and make sure that the sum of all the memory, active memory usage of those processes is smaller or comfortably smaller than the size of the physical memory, right, okay. So basically, you know, have set of active processes and have a set of inactive processes and keep moving processes from active to inactive, right, and make sure that sum of pages, active pages inside the, of the active processes is less than physical memory or comfortably less than physical memory. The scheduler only schedules the active processes at a time, the inactive processes remain in the sleeping or suspended state or even in the ready state, they are not actually brought up and started to run, so it is the scheduler that is playing, you know, playing in consonance with your virtual memory subsystem to figure out, you know, which processes should be run at this point, so that, you know, the system does not start crashing at this point. Now question is how do I figure out what is the size of the active pages of a process, right. So notice I am not just saying that how much memory is allocated in the process, that is not important. What is important is what is the set of active pages in a process, which pages that have been actually being used by this process on a, in an active way, right. So there is a way, so there is a term called a working set used to describe the active set of a process and this was, this term was or this method was proposed by Peter Denning, all right. And informally the working set of a process is the collection of pages that have been, that are actively being used by the process, all right, okay. And let us see how the working set is computed. So firstly if you, if you can compute the working set of every process, then it is very easy. I basically, you know, make sure that the sum of working sets of the active processes is less than physical memory and my scheduler basically does this, does this intelligently, okay. Also notice that the working set of processes may change over time. So right now the process is accessing, let us say 5 MB of, has a working set of 5 MB, you know, sometime later, let us say a few minutes later it becomes 5 KB, so I should, I should figure that out and I should account for that in my scheduling algorithm, all right. So now let us first say, see how a working set can be computed, all right. So the working set is based on a parameter called T and a working set is basically set of pages accessed in last T seconds. If I define the working set as a set of pages that have been accessed in the last T seconds, you know, that is one way of defining the set of active pages of a process. Notice that here again I am using recency as a criteria to distinguish between active and non-active, right. Just like in LRU I was using recency to distinguish between something that needs to be replaced or not. I am not using frequency at all and that is also primarily because workloads typically behave in a past is equal to future manner and so recency is more accurate guess than frequency, right. So how do I compute the set of pages that have been accessed in the last T seconds? Well one way to do this is basically extend the clock algorithm with each page keep a field called idle time per page and in the clock algorithm if you see a 1 then set idle time to 0 of that page, all right and if you see a 0 then increment the idle time with the time elapsed since the last time you saw it and so that way you can keep track of in an approximate manner not completely precise but in an approximate manner what how long this page has been idle and once you have an idle time per page your working set is basically all the pages whose idle time is greater than T, is less than T, right, okay. So you can compute the working set in an efficient way and you can and you can use it, yes question. Using the record of the time elapsed is you know you can have another field for example idle time and last time it was set, right and so you just have to make one computation to figure out you know how much time has elapsed, you know the current time, you know the last time you set it and you basically know how much time or you can use some approximation of this. In any case recall that this this movement of the clock hand is happening on the missed path, right and so this extra computation is not not a big deal because you know it's dominated by the page fault handler and the disk access and all that so the move this extra computation is actually not not too expensive. In this computation of the working set nowhere have we increased the time or increased the overhead on the hit path and that's the most important thing, all right, okay. So you know so that's the working set and using the working set you can decide which processes to keep in the active set and that way you can prevent thrashing, all right. Some difficult questions for the working set idea, well firstly how long how big should T be, what is T, well typically tens of seconds to minutes, so you use a relatively coarse grained value of T to figure out what are the set of active pages, you being a little conservative in this sense and set of active pages, you being little conservative to make sure that you your system is actually completely devoid of thrashing, so you're completely preventing thrashing by having a large parameter for T. Also you need to change handle changes in your working set, so as the working set is changing you need to figure that out on the on dynamically and based on that change the set of active processes. So you know this set of active processes is called the balance set, balance set and so the operating system make sure that the sum total of the working set of the balance set of processes is less than the size of physical memory, just a term called balance set, all right and also you know when you're doing this kind of thing you have to worry about what if processes share memory, so let's say there are two processes that share memory then you need to worry about that as well, for example one thing to do maybe that these processes are considered as one unit, so they either together go into the balance set or they together go out of the balance set, right, because otherwise you know you have to account for you know otherwise it's more inefficient because shared pages are likely to be, so it's better to account for shared pages you may want to do that depend if this number of shared pages is very large, if the number of shared page is not very large you may want to have other ways to figure out you know what the working sets are, okay, right. Another way of preventing thrashing is using the page fault frequency, so one way of preventing thrashing was using the working set approach, another way of preventing thrashing is look at the page fault frequency of every process, here basically you know you have let's say this is your physical memory, you have per process allocations of physical memory, all right, and also you basically maintain the page fault frequency of each process, so let's say this is the page fault frequency of process P3 and PSS of P2 and so on, right, and if you figure out that one process has a very high page fault frequency and another process has a very low page fault frequency, then you can adjust these boundaries based on that, right, so as opposed to the working set which has a, which also has a tunable parameter called T, page fault frequency will also have some tunable parameter saying you know what the threshold where you were going to move things here and there, also if you figure out that the sum total of all the memory pools does not fit in your main memory, then you can deactivate some process, right, okay, so this is an alternate idea to working sets to be able to manage your virtual memory and to prevent thrashing. In general, thrashing was more of a problem, so this thrashing was a big problem in the early days of shared computers, so you know let's say in the late 70s or 80s when we used to have large shared computers like mainframes and lots of people accessing these mainframes, then at that point you know you had to worry about thrashing and these kind of things to basically make sure that all the users get some level of fairness and also the maximum amount of performance that you can get out of a shared computer. With the advent of personal computers, this became less of a problem, number one you could you know memory became cheaper and so if you see thrashing, the best way, one way to do it is basically just add more memory and so you get rid of much of these problems and the OS does not need to worry about it as much as it has to worry about it in shared computers, so memory has become cheaper. The other thing is the user has more control over his system, so the user himself can manually by hand just switch off processes, but on a shared computer nobody wants to stop his process because of the other process, right, but when you have a personal computer you know the user is doing more of this management itself, so in another words basically you know in a personal computer the balance set it can be managed by hand by the user, right, okay good, so that is all I have for the virtual memory subsystem and next time I will start discussing about storage devices and what are the different kinds of storage devices we have and then and the kind of data structures we use on storage devices to implement abstractions like file systems, okay. One last thing I would like to say is that the virtual memory subsystem is also used for accessing files in modern operating systems, so how, well in your virtual address space you can, so let us say this is the VA space, you can map regions of your VA space to point to files and so for example on Unix you can do this using the mmap system call and once you do that the program, the programmer can now just treat this region as any other memory and do pointer references on it, right, so read and write can be done directly here and under the cover the operating system is managing reads and writes to the disk, notice that this is an alternate way of accessing files from the one that we have discussed so far, so far we have discussed ways of open, read, write, close, right, here I have mapped the file into my address space and then just use pointers or just use my program how just consider that space like any other memory region, right, for example I could manage a tree data structure in this space, right, and so my program doesn't need to worry about that it's actually a disk that's getting written to at the back end. To make it efficient of course the operating system is caching pages in physical memory, right, so it's not like every pointer dereference is going all the way to the disk, they are being handled by page faults and pages are being brought into physical memory so that most of your operations are fast but there are also flushing daemons and page replacement daemons that are running at the background to basically make sure that the file contents are updated, okay, so the only difference between using this virtual memory subsystem for other space and for using files is that for files the user also needs some guarantees, so for example if I write something to this address space that has been m-mapped, what is my, how do I know whether this actually has been written to disk or not, right, and I may need some guarantees over that, in case of virtual memory I didn't care about any guarantee, I didn't care if the thing was written to swap or not because when the system reboots the swap is anyway cleared, right, the swap space is anyway cleared but for the file I need some guarantees that it has to be written to the disk, so one way to do that is basically using, you know, providing some kind of system calls like, you know, flush or sync, so you basically, in general when you write there is no guarantee that the contents will be reflected on disk but if you want to have a guarantee then you can use an explicit system call to basically say that the contents should be flushed to disk and they will be flushed to disk in some batched way so that it becomes efficient also. What are the advantages of doing file accesses using m-map over doing file accesses using read or write? Well, one advantage is I don't pay the cost of the system call, if I need to access this file I just make a pointer dereference and that's it, right, I didn't have to do a system call which involves a trap, some kernel code getting to run and so on, in this case actually I can just dereference the pointer and assuming that the page is already mapped in my virtual address space, the operation will be logically done, not physically done, physically done later but logically it's done. If we read the file into a character array and write to the character array and then write it all back to the disk, won't it be the same thing? Yes, I mean this is a way of the operating system providing you that functionality under the covers that, you know, there you have to do it explicitly, so you have to read the whole file into your character array, you know, so for example if there are multiple processes who want to access the same file, so there needs to be some way of, so the file system is also giving you some sharing between multiple processes but if you read into a character array then there's no sharing, for example, you know, if there's one file that's opened by multiple processes and so one process writes to that file, the other process can immediately read from that file because it's a shared address space, it's a shared file system, so the file is the same, the abstraction is that it's being shared by multiple processes but if you read it into a character array then there's no sharing, for example, right. Also, you know, reading from a character array to into a character array requires programmer effort and programmer understanding of what it means to, you know, when to write it back and optimization, it's better to rely on the operating system to do these things for you, right, in a memory mapped environment. So, you are saving the cost of a system call, number one, but is that important? Well, you know, firstly, if you are going to take a disk access then yes, this cost of a system call is not important but if you are going to get served from the buffer cache then you know the cost of system, saving the cost of system call is good. The other thing is, you know, recently we have seen lots of different types of storage devices apart from magnetic disks that we've talked about, there are things like flash memory and other sort of persistent memory technologies that are giving you very fast sort of persistent memory. They have their own problems or, you know, quirks but in any case they, you know, they are providing alternate ways of providing secondary storage. And so accessing that, if the latency of accessing that kind of secondary storage is very small then the cost of actually doing the system call becomes large. So, let's say, hypothetically speaking, the disk was as fast as memory, right? In that case, using the re-derived system call to access the disk is very wasteful. You would want to access it with the same interface as you access memory, which is just point of dereferences, right? And so MMAP is a nice elegant way of being able to do that, all right? So, we're going to discuss this in the file system discussion, but let's say this is a disk. The disk is, the operating system sits on top of the disk, right? So no process can touch the disk directly. So if there's a process, if there are two processes that are accessing the disk, the operating system is maintaining synchronization between them, firstly. And so if this one changes something, it changes both in the buffer cache and in the disk. And so later, if somebody reads it, he'll read the latest value from the disk, okay? The abstraction is that of a shared file, right? So we are, the operating system is responsible for doing synchronization, which means that, you know, there shouldn't be any race conditions on accessing, access of the buffer cache. There shouldn't be any race conditions on access to the disk. But at the same time, the abstraction is that of a common file. It's not, you know, the abstraction is not of two independent files, right? Okay, let's stop here and start our discussion on file systems next lecture."}