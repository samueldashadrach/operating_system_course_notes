{"text":"Welcome to Operating Systems Lecture 24. Right. So last time we were talking about locking, we were talking about fine-grain locking, what kind of problems we can run into while doing fine-grain locking and how deciding which locks to use and where to put the acquire and release statements is a bit of an art. And one needs to do it by looking at the program as a whole. And we were looking at this toy program, which was maintaining bank accounts, etc. And I'm going to talk about a more realistic example today, which is a web server. And let's say, you know, a web server is implemented on a single machine, in our case, let's say, and there are multiple clients that can connect to this web server. And the clients connect over the network, so clients will send a packet which will contain the request, which will, you know, for an HTTP server, it will be a URL. And the server will reply with a page, which will be the corresponding HTML page, right? And multiple clients can connect with this server simultaneously, right? So this is, you know, the pseudocode of the web server. You have an infinite loop, and this infinite loop does just this, it reads messages from an input queue, right? So this is the incoming network queue, right? And parses the message, obtains a URL from the message, reads the file from the disk, which corresponds to that URL, right? And writes the message to an output queue. Let's say this is the output queue, or outgoing queue. So we're just focusing on the server's logic, so this is the server. And, you know, there's some logic which is basically picking packets off the wire from the network and putting it in the incoming network queue. And then there's some logic which is the network which is picking packets from the outgoing network queue and putting them on the output wire, right? Okay. All right. And last time we were discussing that in this entire operation, you know, reading message from an input queue is fast, you just need to copy some data across memory, parsing the message involves some CPU instructions. Reading URLs from the disk is expensive because it needs to access the magnetic disk. And the thing we were discussing last time is that the magnetic disk is a mechanical device, as opposed to all these other things which are electronic devices, right? And then you write the reply to the output queue, right? So we said that this one is, this step is the slowest of them all. And let's look at how slow it is. So we said that a CPU instruction that executes without having to go off-chip, right? So when I draw this diagram, and I say this is a CPU, many instructions, and let's say this is the cache. This is, you know, what's called the L1 cache for a CPU, let's say. And so all these accesses within the L1 cache and to the registers are on-chip accesses. So they don't have to go outside the chip. So these are also called on-chip accesses. And all the on-chip accesses are really fast, and they, because, you know, assuming you have a 1 to 2 gigahertz processor, they'll typically take 1 to 2 nanoseconds to execute a single instruction, assuming it's not going off-chip, right? Now main memory, on the other hand, is off-chip, right? So this is the main memory. And anything which is a cache miss in L1 has to go to the main memory, and it has to go through the bus. It has to travel a longer distance. Also it has to, it has to talk to the bus to make sure that, you know, if there are multiple devices on this bus, example, multiple CPUs or other things, then there has to be some contention protocol that has to be followed. And so it takes longer, and it takes roughly 100 nanoseconds to access the main memory. On the other hand, for something like a disk, the disk transaction also goes through this bus, but eventually the bottleneck, so let's say I draw a disk somewhere here. The bottleneck is really in the disk device itself, because the disk device is a mechanical device. So it means that the disk usually takes on the order of milliseconds as opposed to nanoseconds. So actually it's, you know, a million times slower than your CPU, right? So let's just understand a little bit about how a disk is, how a magnetic disk works, right? So a magnetic disk is organized as a cylinder with such platters on it. So these are these sort of, these circular plates are what's called platters. This axis is called the spindle, and the spindle is the axis of rotation, right? So it just rotates. It rotates at a high speed. And then there's what's called a disk head, disk head actually, so the multiple disk heads. They are these disk heads that sit on top and are able to read information out of these magnetic platters. It's a bit like the gramophone that you may have seen in old movies or, you know, in antique sort of things. So it's a similar thing. There's a head which sits on top of this magnetic disk, and the disk rotates, and the head basically reads off information off the magnetic disk and basically takes it. So this head is rotating, and to allow the magnetic, to allow the disk head to go anywhere, the disk head has a capability to move radially. If I were to draw a platter, a platter internally is just concentric rings. So this is, you know, another view of the platter. So if I just look from the platter, at the platter from the top, the platter looks something like this. And each of these concentric rings is called a track. And so the disk head moves radially to position itself on a track. And because the spindle is rotating, it will eventually be able to reach the position it wanted to reach, and it will be able to read stuff out of the disk, right? Okay. So the average, so both these, so there are two mechanical movements that need to happen for you to access a disk block. The first movement is a radial movement of the disk head itself, and the second movement is the rotational movement, which is usually happening at a continuous speed of the spindle. And both these movements define how long it takes to access a disk block, right? So typical rotation speeds, so rotation speeds of a disk are 5,000 to 15,000 RPM, revolutions per minute. So if you, you know, if you have a laptop, some kind of a mobile hard disk, then it will be slower. It will do 5,000 RPM. If it's an enterprise-grade hard drive, then it will have faster. So typical range is between 5,000 to 15,000 RPM, which translates. And so the disk head, and similarly, the time it roughly takes for the radial movement to reach the correct track depends on the distance from the edge to the center of this platter. What is, you know, typical seek times roughly range between 4 to 15 milliseconds. That's the average seek time. Once again, you know, it depends on the hard disk and the kind of environment it's set up in. If it's an enterprise environment, where, you know, it will be faster, so it will be roughly at the level of 4 milliseconds. If it's a mobile environment, then it will be slower. It will be 15 milliseconds and so on. So let's say I make a request to the disk. I want, I say I want to access disk block number X. Then it will figure out that X lives on this platter, on this track, and at this offset. And so basically, it will first position the disk head at that particular track, and because the disk is rotating, eventually it will reach X. And as soon as it reaches X, it's going to start reading off data from there. And then once it reads off the data, it just puts it in some buffer. You know, there are multiple levels of buffering that are happening. One buffer is within the disk device itself. So the disk controller, which is maintaining, it is doing all this logic. So there must be a disk controller that is receiving a request from a computer, and then, you know, implementing all this logic. And that disk controller typically also has a buffer. So, you know, all this data that's being read from the magnetic disk is stored in this electronic buffer. And the buffer has the same technology that you will have for your main memory, for example. And so you will basically store it in the main memory, and eventually, you will send it to the CPU, all right? So this radial movement is also called the seek time. The time it takes to reach the correct track through the radial movement of the head is also called the seek time of that head. And the time it takes to, so once you are on the head, and you, from, you know, once you are on the track, the time it takes to actually reach the data you were interested in is called the rotational latency. That's called the rotational latency. Rotational latency will depend on, you know, it's just a matter of chance. If you, you know, as soon as you seek the correct track, if it so happens that the data you were interested in is right there, you will just read it right off it. If it so happens that it just went by, you'll have to wait for a full revolution before you reach it, right? So it's just a matter of chance. On average, the rotational latency will be the time it takes to do half a revolution, right? And so the average rotational latency ranges between two to five milliseconds, all right? So that's average. Similarly, I'm talking about the average seek time. Once again, seek time also depends, you know, it's also a matter of chance. You know, if you happen, if the block you are seeking happens to be very close to your current position, then you'll only have to travel a few small distance, and so seek time will be fast. On the other hand, if the block happens to be very far, then you'll have to travel a long distance so that the seek time will be more, right? It's also a matter of optimization, often, you know, as we're going to discuss later in the course, layout of your file system and layout of your disk should be optimized to minimize the seek time, right? Rotational latency, you cannot do much about because, you know, it's just a matter of chance. The disk is continuously spinning, so, you know, where you reach, when you reach there is sort of completely random. But the seek time, you can optimize by making sure that things that are likely to get accessed together are close to each other, or the tracks, at least, either they are on the same track or they are on very close tracks to each other, right? This also means that random accesses to the disk are more expensive than sequential accesses to the disk, right? Let's say, I just say I want to access X, then Y, then Z, you know, these are, and X and Y and Z are completely random disk numbers, then it will, you know, there will be a lot of movement of the disk head to be able to get from X to Y, and then Y to Z, and so on. On the other hand, if you say I want X to X plus 100, that's very fast, right? Because it's likely to be on the same track, so you just say, okay, you know, you don't pay the seek time again, over and over again, you only pay the seek time for the first block. You also pay the rotational latency only for the first block. All the other 99 blocks, you only pay the data transfer time, right, which is just going that much distance. And data transfer times are actually much smaller, typically, than the rotational and seek time. So random accesses are more, are costlier than sequential accesses, and what is the exact tradeoff? We're going to look at when we are going to study the file systems. It's independent of each other, yes, yes, although I draw it like this, I mean, typically you will basically have independent, and there's a lot of optimization, you know, so these are, you know, what I've drawn is a very thick diagram kind of a thing, but there's a lot of optimization that goes on in the middle, you know, so, and this is all the speciality of people who manufacture these disk devices. All right, okay, so, so with that, we basically, you know, end up with a number of something like 1 to 20 milliseconds to do a disk access, right? And where does that leave us in terms of a web server? So in this web server, it basically means that every access, every request to the web server will take roughly 1 to 20 milliseconds, because, you know, this is by far the longest slowest step, and everything will get serialized there, and you will basically, every request will take 1 to 10, 20 milliseconds. And so how many requests can the web server service per second? So what's the throughput of the web server? One upon 10 milliseconds, that's roughly 100 requests per second, right? So with this, with this kind of a code, you will only be able to service up to 100 requests per second. What will typically happen is, let's say you're getting requests which are at a higher rate than 100 seconds, then they will start, they will eventually fill up the incoming queue, and packets will start getting dropped at the incoming queue, right? And so you're basically operating at 100 requests per second. 100 requests per second is not a very, not a good number at all. Modern web servers deal with tens of thousands of requests per second, right? So what do, what's the first optimization that comes to mind? Well, the first thing is, you know, why do you have to read from the disk every time? It's quite likely that the same pages are being requested by multiple clients, which is often the case, right? So once again, we are taking the, taking this, taking the advantage of locality, spatial and temporal locality, and we're going to use caching, and we're going to cache the contents of the disk in main memory, right? Okay. So let me rewrite this code for the server. And I'm going to say, file one, once again, read input packet, just shorthanding it a little bit, that the URL is equal to parse packet. And then if requested file in cache, then, is not in cache, is not in cache, then data is equal to read from disk, new.name is equal to URL, new.data is equal to data. And let's say, let's add some counter, which is new is equal to new plus one. And then I just say, write output queue. Basically, I have just implemented a memory cache, in-memory cache, and I'll first check if the requested file is not in the cache. Then I will read the data from the disk, allocate an entry in the cache, and I'm sort of assuming that there's a lot of free space in my cache, so let's not worry about cache replacement and other things. Let's just say, I have this pointer called new, and the new is the current empty slot, so you just fill up the empty slot with some data and its corresponding URL, and you increment the new. So this is better, because if 90% of the requests hit in the cache, then 90% of the requests can be served from the cache, 10% of the requests need to go to the disk. However, still, if you think about it, you have an input queue, right? Even if one of these requests misses, so let's say the missing one is this red one, so even if one of these requests misses, all the future ones have to wait, right? Because even if one of those misses, the thread basically gets busy accessing the disk and waiting on the disk, and this is going to take milliseconds, and so all the future requests are going to wait, right? So that, I mean, eventually the throughput of the system does not really improve even if there are a few misses in the system. And more importantly, the latency, the perceived latency, so when you type your URL on the browser, the user-perceived latency becomes very bad, because anybody who is blocking you, even though your request would have been served very fast, because there's somebody standing in front of you whose request would take a long time, you have to wait for that request to get served. Just to keep track of? Right. So one way to do this is basically maintain a new queue, and basically you say that if requested file not in cache, put it in a different queue, but then you need to start the disk and you need to have some thread that's waiting on that disk while you start taking another request, right? So there's some amount of concurrency that has to happen. Even if you make two queues, you also need two threads of control. One thread of control is going to wait on the disk, and another thread of control is going to pick up the next packet from the input queue, right? And how many threads will you maintain? So once again, the solution that has been proposed is let's have, you know, at this point, if the requested file is not in the cache, don't do all this in the context of the current thread, spawn a new thread, right? And let that thread wait on the disk, and I can go forward and pick up the next request, right? So basically we are talking about some kind of multi-threading, and question is how many? So one option is do it, you know, on demand. So every time you get something like this, spawn a new thread, as soon as it finishes, you know, join it so that, you know, or terminate it, and so you can do it in that way, right? So in either case, basically what you're saying is that you need to have multiple threads of control, and you need to have concurrency, right? So basically, this is an example where you need concurrency to improve your system utilization, and also reduce your response times, okay? And this is an example where you need concurrency because there are two devices in the system. There's a CPU, and there's a disk. And you want that both of them should be kept active. So the way to keep both of them active is to have multiple threads, one, you know, few threads waiting on the disk. Disk is much slower, so many threads can wait on the disk simultaneously, and let's say one thread waiting on the CPU. Similarly, you know, we have seen other examples where the multiple CPUs, so if you want to keep multiple CPUs busy, then also you need multiple threads, right? So in one, in another way, you can think of a disk as just another processing element, and if you want to use multiple processing elements simultaneously, then you need multiple threads. All right. So what are you going to do? You're going to say, let's say one way to do it is, let's call this the server. So let's say this is the function which is the server, and so you could basically say something like this. You say, okay, I'm going to spawn 100, so let's say, for i is equal to zero, i is less than 100, i++, fork server, all right? So I forked, and let's say this fork is basically forking a thread, or, you know, if you prefer, let's say thread fork. It starts a thread, and it basically says that this thread should be running this particular function called server. So now there are going to be 100 threads that are simultaneously executing this function called server, and now what will happen is there are 100 threads that are executing this server simultaneously. One of them, or many of them, could be executing on the queue, some of them could be waiting on the disk, and so you have higher system utilization overall, right? How do you choose this number 100? Well, if it's too small, then, you know, you can have poor system utilization because you could have, you know, you could have multiple threads waiting on the disk simultaneously, and the disk could, you know, take multiple requests simultaneously. On the other hand, if it's too large, then there's no problem from a concurrency standpoint. Actually, if you have infinite threads, that's good, you know, completely logically speaking, but you will run out of your memory resources to actually manage these threads, right? Each thread takes some memory. For example, each thread has a separate stack, right? So threads have some overhead, and so if you have too many threads, then, you know, you're going to soon run out of your memory to manage those threads. So there's a trade-off. You basically want the maximum concurrency level without having too much memory overhead, and so, you know, and so you could basically, you know, one way to do it is just spawn threads on demand, and when you hit your memory limit, then stop spawning the threads, you know. That's one, that's another way, but all these issues, all these decisions basically come in the realm of scheduling. How many threads to spawn, which thread to run when, et cetera. These are basically scheduling issues, and we're going to look at them when we talk about it later in the class. All right. So now I have multiple threads that are accessing this code. Is there a, is that okay? No, that's not okay because there are too many shared resources here, right? There's a shared, so firstly, there's a shared input queue. Multiple threads accessing the same input queue, you need synchronization, right? Similarly, there's an output queue here, right? Multiple threads accessing the output queue, you need synchronization, and none of it is present here, and bad things can happen, right? Because it's a shared data, multiple threads can be accessing it, and there are race conditions, and bad things can happen, and we know what kind of bad things can happen. Similarly, this cache itself is shared, and so it, you know, for example, it can happen that one thread comes in, it says, is the requested file not in cache? Both of them figure out that it's not in cache, and they have duplicate entries of the same file. Worse still, it can happen that, you know, some schedules of this, of this code can cause empty entries in the cache, or some, it can even happen, worse still, it can so happen that you have the URL of one page, and you have the content of another page, right? All these are possibilities, right? So what do you want? You want to use some kind of locks, right? You want mutual exclusion, basically, right? And so the way we know about how to use, how to solve mutual exclusion is locks. So what I'm going to do is, I'm going to say, let's say, let's say I do an acquire, let's say I have a global lock, and let's say this lock basically maintains, you know, mutual exclusion for the whole cache, and then I have a release lock. This will take care of concurrent access to the cache. This will not take care of concurrent accesses to the input and output queues. Let's ignore the input and output queues for some time, let's say they're special, you know, special data structures, and we're going to talk about how these input and output queues are implemented. But let's just talk about, first, the shared concurrent access to the cache. You acquire the global lock, and you release the global lock. Is this an acceptable solution? Have we made any progress from our previous case? So I see some threads nodding, so why not? What's happening? Basically, let's say, now you have multiple threads, okay? Multiple threads could be servicing multiple requests simultaneously, right? So each thread is servicing one request. But if there is even one cache miss, then you're holding the lock, and you're accessing the disk. And so for that period of time, nobody else can take the lock. So what has happened is, instead of the request having, in the previous case, the request was waiting at the input queue, now the request is waiting at the lock acquisition. That's the only difference. The waiting is still the same. I still, once again, the problem is, if I'm standing behind a cache miss, then I'll have to wait, because the cache miss has taken the lock first, and he's going to take a long time. And even though my request would have taken less time, we are basically serialized, completely serialized. So the global lock will definitely not solve the problem. In fact, you know, we have just made things more complex without increasing any performance at all. So here's an example. We would definitely need fine-grained locking to have any benefit of concurrency, all right? And so what kind of fine-grained locking are you going to have? Well, what are the shared resources? You have these cache pointers, new, which are pages in the cache. You have the disk. The disk itself is a shared resource, right? So when you access the disk, the disk shouldn't be accessed concurrently, simultaneously. So accessing the disk requires sending some commands to the disk using, let's say, in and out instructions. And so those in and out instructions need to be mutually exclusive, so the disk is exclusive. Each of these cache entries need to be exclusive. And this check needs to be atomic with respect to the other operations here, right? And so, you know, what is the exact solution? The solution will basically, you know, is basically, this is a very common scenario where you're maintaining a cache and you need to maintain consistency, et cetera. And we are going to look at this in a lot of detail when we study file systems. So file systems have a similar problem, you know, when you're basically accessing your files, not each and every request doesn't go to your disk. It's actually cached in the memory. And you have to synchronize between multiple threads, making multiple concurrent accesses to multiple files. And, you know, here's a sketch of a solution. You can basically say I'll have a lock for the whole cache here. But then I'll try to release the lock. I'll only use the lock as soon as I've done this check. But before I release this lock, I'll also have a per-entry lock. So I'll have a full cache lock, and then I'll have a per-entry lock. And after I've done the check, and before I release the global lock, I'll take the local lock. Local lock is a per-entry lock. So this area can execute concurrently for multiple threads. And so you have more concurrency. And then, you know, you have to worry about other things also. So you have to basically do fine-grain locking. And there are some ways you can do it. And I just sketched one of the ways you can do it. And we're going to see this in more detail. And so, you know, just to motivate that fine-grain locking issues are actually a little bit complex. All right. OK. So now let's look at the queues. So here's a queue. And here's another queue. And once again, having queues is a very common pattern in systems in general, and in operating systems in particular. And how are queues implemented? Well, you could implement a queue as a circular buffer. The common way of implementing a queue is a circular buffer. So let's write some code to basically implement a queue. So firstly, why do I need a queue? I need a queue because there's a network thread that is adding to the queue. And there is a server thread, or there are multiple server threads, which are removing from the queue here. Similarly, there is a server thread that's adding to this queue, and a network thread that's removing from this queue. So the logic is basically, and it's a first-in-first-out queue, right? A queue basically, by definition, means it's a FIFO, a first-in-first-out. So one thread can add to the queue, and another thread can remove from the queue. And you'll get a FIFO ordering on that. And let's see how a queue is implemented as a circular buffer. I'm sure all of you have seen this before, but I'm just still going to write the code so that when I write it in the concurrent mode, you have more context. So let's say I have an array of characters which is maintaining my queue, and the maximum size of this queue is max. And then I have two pointers inside this array, int head, initialized to zero, let's say, and a tail, also initialized to zero. All these three variables are global variables. Then you have a function called produce. Depending on who is the producer, you know, who he's going to call produce, input queue network is the producer, output queue server is the producer. In either case, you basically say produce, let's say, some character, which is your data. And you're going to say, if, right. So let me just first draw the queue, just to make sure we understand it. So let's say this is my array. This is queue, right? And this is element zero, and this is element max minus one, right? Initially, this is head. And this is also tail. Now what will happen is that when I produce something, I'm going to increment head. So producing, when I produce an element in the queue, head is going to point, head is going to advance, and tail is going to be here, and this will be the first element. Similarly, producer can keep producing, and the head will keep incrementing, till head becomes equal to tail minus one, right, in a circular way. So I just keep incrementing head, and so the consumer may also increment tail, and the queue becomes full, so the condition for full, a full queue looks something like this. Tail is here, right, and head is just behind the tail. This is a full queue. If I try to add one more data item, I cannot do it without having to overwrite something that has not yet been consumed. So this is a full queue, right? What is an empty queue? Tail and head are equal, right? So if head is equal to tail, then that's an empty queue. Basically the tail has basically hit the head, and now I cannot move any forward, anything. I cannot consume anything. There's nothing ahead that's useful. So what will produce do? It will check if the queue is full. If it's full, then it will, let's say, return an error, so it will basically say if head plus one, and I'll do it in a circular way, modulo max is equal to tail, then, no, let's say error here, right? Let me just leave that as a blank. Otherwise I just say queue head is equal to data, and head is equal to head plus one, mod max, right? And similarly you can write the code for consume, which I'm not going to write here, okay? So what should you do here? If you are just operating as a single thread, and you call produce, then you may want to say I want to give an error here, right? So you may say error here, you may say error at this point. But if you are executing in a multiple thread world, and you have a shared queue, which is shared by multiple people, you may want to say, oh, let's wait, right? Because there are other threads that are running at the same time, and so it's likely that even though the queue is full right now, it will become empty in some time, right? So one option is wait for queue to become not full. Similarly in the consume, I could have a similar thing here. If head is equal to tail, then wait for queue to become non-empty, right? And so the consume has a void argument and returns a character, right? You can say return value is equal to queue tail, and tail is equal to tail plus 1, mod max, okay? So now my question is how does, so there are multiple threads, one thread is calling consume, one thread is calling produce. It's possible that the produce finds the queue full and it waits for the queue to become not full. Similarly, it's possible for the consume to become, for the queue to become empty and for the, for a thread to wait for it to become non-empty. This kind of synchronization, this is also sort of synchronization, so I'm waiting for a condition to become true, and only when it becomes true will I be able to move forward, right? And the construct that we have studied or the abstraction that we have studied so far of locks is not enough to implement this synchronization, right? It's not possible with locks to say that, you know, I want to wait on a condition, right? Locks are only for mutual exclusion. I cannot use a lock to say I want to wait on this condition to happen, right? So what am I going to do? I'm going to define a new abstraction that's called condition variable. So let's understand what this abstraction is. Just like a lock, there is a type called struct CV, let's say, just like there was a struct lock and you could instantiate a lock with this type, similarly you have a type called CV and you could have, you could instantiate this type. And then you have two functions, void, wait, right, struct star, CV, and then there's another function called notify, struct CV star, all right? Actually the wait has two arguments and the second argument I'm not writing right now before I explain why we need the second argument, but let's just understand the spirit of this abstraction. So the spirit of this abstraction is, in the previous example, the programmer can define two condition variables and the produce, let's say, so I could define a condition variable called struct CV, the two condition variables, not full and not empty. And I could say, so this English sentence, wait for Q to become not full, I can now replace with something that means similar thing, let's say, wait on not full, on this condition variable called not full. So I've defined a condition variable, I basically say, now let's say I'm going to wait on this condition variable called not full, right? Now how am I going to know whether it has become not full? The consumer has to tell me, right? So the consumer will basically here say, notify not full. So that's how you connect the two, basically say that if you find the Q to be full, you basically wait on this condition variable, you know, that I've called, that I've named not full. And then when a consumer consumes something, then clearly after it has consumed something, the Q is not full. So he can say notify not full and the idea is that when he says notify, he's going to wake up any thread that's waiting on that condition variable, right? So the semantics are then when he calls notify, if at this point there's any thread that's waiting on this condition variable called not full, it'll get woken up and it will be able to proceed, right? Similarly, you know, symmetrically, instead of wait for Q to become not empty, I could say wait and not empty, right? And here I could say notify not empty. The effect of notify is that if there's a thread that's waiting on that particular condition variable, then it will get woken up, it'll return from wait. On the other hand, if there's nobody who's waiting, then it'll have no effect at all, right? It's just a no. It's possible that at this point, there was no other thread that's waiting on the not empty. In fact, the Q was not empty at this point. We're just calling notify unnecessarily. It was actually the Q has, let's say, 10 elements and both the producer and the consumer are happily going along. But you're just calling notify unnecessarily here, right? But that's okay. It's not incorrect. It's not, it may not be efficient, but it's not necessarily incorrect, right? So there's some problems with this code. Firstly, these are shared variables, right? Q is a shared variable, head is a shared variable, tail is a shared variable. And there are at least two threads. One is a producer and one is a consumer. And both of them are accessing these shared variables, and they're not doing any sort of mutual exclusion in their accesses. And so bad things can happen, right? Okay. So firstly, I also need locks. Apart from condition variables, I also need locks in this particular code, right? Because there's these shared variables that are being accessed. So not only do I need this kind of wait notify synchronization, I also need locks to be able to maintain mutual exclusion on my accesses to these shared variables, right? So let me rewrite this and say void produce data, and once they acquire, let's say I have a lock called Q lock, right? And then I say, if head is equal to tail, sorry, head plus one mod max is equal to tail, then wait on not full, right? And I'm going to leave it empty because I'm going to say something else later. And then I'm going to say, you know, whatever was earlier code is to produce something, and then I return. So that's my produce code, right? And of course, I release the lock here, right? Acquire the lock. I do some operations. If the operations require me to wait, I wait, and then I get notified, and then I release the lock, yes. Okay. So the question is, while waiting, do I need to release the lock? So I'm going to come to that very soon, right? So good. So let's say, similarly, void consume is, or let's say char consume, void is basically going to say, acquire, also going to acquire the Q lock, it's going to check if head is equal to tail, then wait on not empty, and then do whatever you like, and then release Q lock, right? Okay. So now, as was pointed out, there's a problem with this code. One thread acquires the Q lock, starts waiting, finds the Q full, starts waiting on this condition variable called Q not full, but it hasn't released the lock, right? So if it hasn't released the lock, there's no hope that a consumer will be able to get the lock. And so the consumer will start waiting here, the producer will start waiting here, and you have a deadlock, right? So what did you want to do? You wanted to release the lock before going to wait, all right? So one option is to call release here, right? So let's say release, let's say R here, and then do I need to reacquire the lock when I come out of wait? Yes, because, you know, I'm going to have to execute this, so I need to reacquire. So I call release before wait and call acquire after wait, right? So you know, so there's a suggestion, can we make them atomic? So firstly, this sounds okay, I release the lock, then I go to wait. Then I come out of wait, somebody called notify, I come out of wait, the first thing I try to do is acquire the lock, and when I get the lock, then I can, you know, I can just go forward. There's a problem, as was very rightly pointed out, that these operations of releasing the lock and actually going to wait are not atomic with respect to each other. Let's see what's something bad that can happen. Let's say the producer basically comes in, finds a queue to be full, he checks his condition, finds a queue to be full, he releases the lock. He releases the lock, and he's just about to go to wait. Before he goes to wait, consumer comes in, consumes the element, signal's not full, but that signal will be wasted, right? So that signal will be wasted, and so the consumer has sent its signal, or that notify, so it'll basically say notify, that notification will get wasted, and then the producer will go to sleep. So it's like saying that I have released the lock, consumers can now come in, consume the entire queue, you know, and send signals, but, you know, because I haven't yet gone to sleep, those signals or those notifies don't mean anything to me. Then I go to sleep, right? And now I'll keep waiting forever. The consumers have gone, done their job, as far as they're concerned, they are notified. The producer has not started sleeping at that point, and so, you know, he's basically... So you know, one analogy to this is that, let's say, you know, you basically release the lock, which means you allowed other people to come into your house, but now you have to go to sleep. So other people have come into their house, and they have, you know, they have changed something, and they have also said, you know, they've also triggered the wake-up alarm, but you know, you haven't really reached your bedroom, and so you haven't started sleeping. So after everybody has gone, then you go to sleep, and you will keep sleeping forever, basically. You know, so that's... And so let's say this is the lost notify problem. Yeah, that's a bad example, but at least I hope it's... I was just trying to give a physical counter analogy to this thread thing. All right, so... Okay. So this is a lost notify problem, right? So basically, the notification is getting lost, right? So the abstraction is, basically, the condition variable basically takes two arguments, a condition variable and a lock, and the semantics are that wait goes to sleep on the condition variable and releases the lock, and does this in an atomic fashion, right? So basically, it does all these three things. It releases the lock and goes to wait, right? So inside this, I'll basically give this, the second argument, Q lock, and the semantics are that it will release the Q lock, and it will go to wait, and it will make sure that nobody can call notify in the middle of these two things, right? It's not possible that when after I release the lock, and before I actually went to sleep, nobody can call notify. So it's atomic in that sense, right? So you're basically going to release the lock and wait, and it's going to be atomic. Nobody can call notify in the middle of those two operations. Similarly, the other abstraction is that as soon as it gets notified, the first thing it's going to try to do is to do a lock acquire, right? Lock acquire need not be atomic. I mean, it's just, you know, you're just going to try to acquire the lock, and you may have to wait for being able to acquire the lock, right? Okay. So let's say that this code was not complete. So before you release, you basically also say notify not empty, right? And here you say notify not full, and here you release the lock, okay? So what will happen is let's say the producer figures out that the Q is full. It goes to wait on not full, but it also releases that lock atomically. So if somebody is able to get the lock, by that time the producer would have slept, definitely, right? So only after it has gone to sleep will the consumer, and has released the lock, will the consumer be able to acquire the lock? It will do all its operations, and it will signal not full. When it signals not full, it basically looks at all the threads that are waiting on the not full. In this case, there's only one thread that's waiting on the not full, and it's going to wake it up. The thread's going to get woken up, and the first thing it's going to do is going to try to acquire the lock. Is it going to get the lock immediately? No, because, you know, in this code here, the release, you know, the producer has, the consumer hasn't released the lock yet. So even though I have notified the producer, the producer has just woken up, and the next thing it's going to try to do is to acquire the lock. In this code, it's possible that the producer, the producer gets to run before the consumer actually gets to execute the next statement, and so it's possible that the lock is still held by the consumer. So he's going to try to acquire the lock, he'll not get it immediately, but eventually he'll get it because this one is going to release it, and so now he can get it, and he can continue its operation, right? So the semantics of wait and notify are that wait takes two arguments. The first is a condition variable, so the first is a condition variable, and the second is a lock. Semantics are that the thread which calls wait on these two arguments will go to sleep on CV and release the lock L in an atomic fashion, right? The semantics of notify are that it will wake up all the threads that are sleeping on CV, right? And that's it. If there are no threads sleeping on CV, notify will have no effect, right? The other semantics of wait are that as soon as you wake up from a sleep, then the first thing you will try to do is reacquire L, right? Okay? All right, so let's stop here and continue this discussion next time."}