{"text":"Okay, let's start. So a cache line can be held in read shared mode across multiple CPUs or write exclusive mode. So the idea is if the CPUs are only accessing the location in read mode, then it can exist in multiple caches at the same time. If it is being accessed in write mode, then it has to exist in one cache at any time, right? So if some other CPU accesses it while this is cached in some other CPU's cache, then the cache line has to be brought from that CPU to my cache, that CPU's cache to my cache. So that's basically write exclusive mode. And so what happens is if there are two CPUs which are accessing the same location in write mode, then there's a lot of cache line bouncing that's going on. And this cache line bouncing is not limited by the speed of the CPU, it's limited by the speed of the bus, right? At what rate can the bus do these transactions of cache line bouncing? And so what can happen is that most of the time the CPU is just idling, waiting for the cache line to come to it. On the other hand, if all these accesses were cache hits, then the CPU could have executed at full frequency and could have been, you know, up to 10 times faster, depending on what the workload is. You know, a cache hit versus a cache miss can have up to a 10x penalty. So what would have been a better thing to do? A better thing to do would have been if I could organize my code such that most of my code is read accesses. And so all the shared variables will get cached in read shared mode in all the caches, and all the CPUs will execute at full speed. We also said that because of this cache line bouncing problem, it can so happen that your code with two processors or two threads is actually slower than the code that could, that, you know, that was just running with one thread, because, you know, you're not using your caches effectively. Okay, so we said, let's take a data structure. And let's say, you know, 99% of times you are accessing, you're calling search on it, which is a read-only access. And then, you know, 1%, you're calling, let's say, push and pop, right, which has read-write access. Ideally, you would have wanted that your search operation executes at full speed. And by full speed, I basically mean that your data structure should get cached in the local caches of each CPU in read mode. You know, that search is executing most of the time. And so all these CPUs should execute at full speed. However, you need to make sure that, you know, search is properly synchronized with write operations like push and pop. And so, you know, one way to do that is basically have a reader-writer lock. If you have a reader-writer lock, then each time you call search, you have to set the state of the reader-writer lock to locked. And that becomes a write operation. And so now, you're not bouncing on the cache lines that hold the data structure, but you would be bouncing the cache lines that hold the lock, right? And so that's not a, that's not, you know, that's not a very good performance. And it's possible that the performance is actually worse than what it could have been on a single process. So the better thing could have been to, you know, do nothing on reads. This is option one, let's say. And option two is to do nothing on reads and do writes atomically. So what does this mean? Reads don't need to do any synchronization. So read, the code for the search procedure remains exactly the same. You don't use any loss for that. But in your write, in your push and pop, you ensure that this update operation happens in one shot, atomically, right? So the read operation or any other write operation cannot see the update operation half done, right? So what this will ensure is that if there was, there was concurrent reads and writes, then either the reads could have started before the update, or the write could have started after the update. But the read couldn't have been, have started in the middle of the update, because the update is atomic, right? If you could do that, then your reads execute at full speed, which is 99% of your time. But if your writes execute, you know, roughly at the same speed, it doesn't matter. It's anywhere one percent of the time. Okay? So how did we do this? How do you do writes atomically? Well, we used this compare and swap instruction on the hardware that allows you to atomically update four bytes, right? And we said this is not always possible. It's possible when your update operation involves update of four bytes. So the way you do it is, you first, in local memory, construct, you know, you first copy the data, copy some part of the data structure locally, compute on the local copy, and then you use the compare and swap instruction to update the data structure in one shot, right? And so it's also called read-copy-update. You read the data structure, or you read some part of the data structure, let's say you read the top pointer, or the head pointer. You manipulated it, you changed head to head, you know, head to something else, if you are doing push, or if you're doing pop, you change head to head.next, in whatever case. And then you use one instruction to atomically swap head and head.next, right? And so it's read, copy, and then you atomically update. Okay, so we were looking at this last time, and we said, let's say here's my data structure, and let's say this is my top, and then somebody called search, and he took a reference to top, so he said, local is equal to top, right? So local is some local variable that search is holding. It could be a register, for example, just allocating the register. And simultaneously, let's say there was a pop operation. So pop did top is equal to top.next. What will happen is that top will now point here, but search will hold a reference to the old top. But we say it's okay, because even though search executed after pop has happened, the search will still see a well formed list. And so it's as though search happened before the pop. The only issue is that you shouldn't be reusing this memory, right? So after a thread has popped the first element in the stack, it will probably want to free this memory, right? So somewhere here, it will want to say, free, you know, whatever the old top was. So let's say old top is equal to top, so you free old top. Now here is where the problem is, right? You cannot just free the old top anymore, because you don't know whether there could be a concurrent search that's holding a reference to this old top. Question is, when can I be sure that I can free it, right? And the answer is, really, I can never be sure, in general, right? I don't know how long this other thread that's called search is going to take to actually start dereferencing it, right? Search has a copy of local, but probably, you know, as soon as it executes the next 10 instructions, it will probably lose this reference. But we don't know when it will execute these 10 instructions, and we don't have any idea of who all are holding this local, this reference to the old top, okay? So because this pattern was very common in the kernel, where you have a data structure that's being read a lot and updated only very occasionally, maybe once in an hour or once in days or something, but it's being read a lot, it was very important to optimize something like this, okay? And so the solution that was proposed was to look at the structure of the operating system. Operating system goes through cyclical activity, right? So what happens is it does some computation, and then blocks. If I look at every CPU, it runs some thread, then blocks. There's a thread computation, blocks, thread computation, blocks. If I could be sure that before blocking, all local references to any shared data structure have been given up, that would be very helpful to me, right? If I could be sure that across this blocking, I couldn't be holding a reference, a local reference in my stack or register to something that's global, it would have solved the problem for me, right? Because what I would have done is I would have waited for all CPUs to reach this block. And once a CPU has reached a block, I can be sure that, you know, it's not holding a reference to old top, right? All future accesses to the stack will start at new top. It's only the old ones that could have old top, but if it has reached the block, then I can be sure that it's not holding a reference to old top, okay? So this is true for the Linux kernel. So let's say, so Linux kernel is non-preemptible, which basically means that if a thread is executing within the kernel, and there's a timer interrupt or any other interrupt, it doesn't cause a switch of this, it doesn't cause a context switch across threads, right? It waits for the thread to, you know, yield before it actually does a context switch. It basically means that if there's a context switch from thread one to thread two, implies clean boundary. It basically means whatever was the operation that the thread had to do inside the kernel that has been completed before the context switch has happened. You cannot context switch in the middle of an operation, right? This also means that if there's a context switch, then I couldn't be holding a reference, a stale reference to a shared data structure in my local variable across a context switch, right? In other words, the search procedure could have, would have either finished completely before the context switch, or would be started after the context switch. It cannot happen that I'm in the middle of the search procedure, and a context switch happens. Okay, given this information, I can devise a strategy. I can say that if there's a push or a pop, I do this atomic update, right? I do this atomic update, and I wait to free the location till there's a context switch on all the other CPUs. If there's a context switch on all the other CPUs, I can be sure no other CPU is holding a reference to this location. I can also be sure that any thread that's not running currently on any CPU definitely does not hold a reference to old top, because if a thread is actually blocked, then, you know, it cannot be in the middle of the search. It has to be either after the search or before the search. So this period where you wait is called the grace period, and it depends on the system how you implement the grace period. In something like the Linux kernel, you could wait for all the other CPUs to do a context switch before you can be sure that I can free old top after that. So the algorithm is that push or pop or any update are going to atomically update the data structure in a read-copy-update manner, and then I'm going to wait for all the other CPUs to call a context switch. And once they have called the context switch, I can be sure that I can safely free this old top. How do I wait for all the other CPUs? Yes, question. Okay. Okay, let me just try to understand what you're saying. So you're saying that let's say there's CPU 0 and there's CPU 1, right? This one says push, or let's say this one says pop, and he has not freed the location. So question is when can he free? So let's say there was some other thread that's running on the CPU, let's say this is thread X. So my first assertion is the only other thread that could be holding a reference to old top is thread X. There's no other thread that could be holding a reference to old top. Because if there's a thread that's swapped out, that's not currently running, that's definitely not, that's either finished with search, or it didn't call search at all. And if it calls a search, when it comes back again, it's going to restart, you know, starting from the new top. So the only other thread that could be holding a reference to old top is thread X. Now if you can generalize it, if there are N other CPUs, then there are N other threads that could be holding a reference to thread top. Question is, till when can these threads hold a reference to old top? So my assertion is that because Linux kernel is non-preemptible, if there's a context switch on all these other CPUs, then I'm sure that after the context switch, nobody can be holding a reference to old top. Make sense? Right? So if there's a context switch here, then I'm sure at this point, all references to old top have been lost. Yes? Okay, so here's an interesting thing. It's possible that one of the CPUs takes a very long time to context switch, and so it slows down everything else. Does it really slow down everything else? What am I waiting for context switch, what am I waiting to do before all the other context switches happen? It's just a free. So if it takes a long time for all the other CPUs to context switch, then it just means that that location lingers on for a relatively long time. But that's okay. Assuming there's a lot of space, there's a lot of memory, you're just holding on to a location for longer than it was actually needed. Assuming memory is plentiful, that's a very, very useful trade-off to have. Okay, so here's the question. If one thread has called pop, he has made some change, so can the other threads use that change before the free has been called? Why not? I don't see why. So my assertion is that if, see, as far as the pop is concerned, it has atomically made the update. It's just a matter of freeing the location. Okay, and freeing can be delayed arbitrarily. Assuming I had infinite memory, I would say, you know, I don't even need to care about all this. Assuming I had infinite memory, I would just, you know, just execute cast to implement pop and I won't even worry about freeing. And I could be sure that's all. So it's, in a way, RCU is a way of trading space for time. You know, you're using extra space for longer, but that way, you basically make sure that your reads are very fast. You don't need any synchronization on the read path, right? So it's a very interesting idea and it's very, very practical and useful. And let me tell you a few examples that are actually very useful in the Linux kernel. So firstly, notice that in this RCU approach, I use some property of the software. In this case, I use the property of the Linux kernel that it's non-preemptible. So I need to know this kind of information to be able to, you know, implement this kind of thing. If I couldn't make any assertions about the software in which this RCU is implemented, it will be not possible to implement the scheme. So what's the property that I'm looking for? I'm looking for some cyclical activity and I need some point in that cyclical activity that can be sure that's a point where, you know, all old references have been lost, local references have been lost. So that's good enough for me. So typically, you will implement things like RCU in the kernel, where you can make these assertions. RCU at the user level is much, you know, much more complicated. I don't know. I cannot just implement RCU as a library, like I can implement pthread locks and other things, right? Pthread locks make no assumption about the application behavior. RCU makes assumptions about the application behavior. So, you know, there are user level libraries that implement RCU, but you have to tell them what the cyclical activity is and when is it safe to actually, you know, release references or free things in a safe manner. Okay. But let's look at the Linux kernel, where RCU is used a lot. And where is it used? It's used, for example, in the routing table. The routing table is an example of a data structure that has a very high rate of lookups. Each time a packet comes, you look up the routing table to figure out where it should be sent. Assuming you have multiple network interfaces to this machine, it's acting as a router. So there's a routing table. Very occasionally, you update the routing table. But you need to make sure that, you know, these updates to the routing table and the lookups to the routing table need to be synchronized with each other. If I had used redirect logs to synchronize this, then I would have severely penalized my lookup path, which needs to be very fast, because of the cache line bouncing problem. On the other hand, if I use RCU, you know, that makes my lookups full speed. And my updates are relatively slow in the sense that I cannot reclaim memory immediately. Otherwise updates are as fast as they could be. If there's a free, that becomes slow. Okay? Similarly, Linux has what's called modules. So what are modules? Modules are fragments of code and data that can be loaded into the kernel at runtime. So you can compile something, and then you can load it at runtime. And there's some interface that is defined, so it can sort of start running in kernel space. So those are called modules. You would run device drivers as modules. So you have a new device, you basically attach it, the kernel doesn't have a device driver for this, you implement your driver as a module, and you load it in. The module will, the difference between a module and a process is that a module executes in privileged mode, in kernel space. You can do everything that a kernel can do, that's all. So once again, modules are an example where you will probably be doing lots of module read accesses, where you say, you know, there's some module table that looks up whether it has this function or not, and you want to call that function. So there are lots of read accesses. And occasionally you're going to do module load and unload. So unload and load are very, very occasional operations with respect to module accesses, read accesses. And so here's another example where you could use RCU to execute your read accesses at full speed and yet synchronize correctly with your load and unload, okay. Okay, let's think about xv6, where does it make sense perhaps to use RCU instead of locks and if I was to use it, then what could I have, how would I have implemented it? Here's one suggestion. Let's say, you know, there's some buffer cache entries that are accessed too much in read-only mode and not so much in read-write mode. So let's leave that question where it can be used. It can be used anywhere where, you know, there's a lot of read accesses and very few write accesses. Let's talk about how you would implement it. Okay, so everything else is okay. I mean, it's basically saying you use cache instead of raw locks. You don't do anything on your read path. The question is, how do you decide what's the cyclical activity, all right? So in case of xv6, if you remember, you can probably say something like this, that any time a thread, so first again, just like before, if a thread is context-switched out, you can probably say that's not holding any references to shared data structures. Whereas context-switched out, it has voluntarily called the yield function, right, either because of the timer interrupt or something else. And you could probably, I mean, you could organize things in such a way that it'll never call yield without actually, you know, coming out of any accesses of the shared data. And then you can say that any time, and yield internally would call, let's say, wait. And so you could, you know, synchronize that wait boundary. So you could say a CPU goes through some computation and then wait, and then some computation and then wait. And so any time the CPU calls wait, you can be sure that's not holding any local references to shared data. Also, you could say if a thread, if the CPU ever goes to user mode, at that point, I can be sure that it's not holding any shared references to user data, shared data, local references shared data. Okay, good. So I've discussed read-copy-update. So that was a, it's commonly called read-copy-update, or RCO for short. Okay. Right, with that, I'm going to move on to my next topic, which is OS organization. So, so far, we have been looking at the UNIX model of an OS. And this model is also called a monolithic kernel. Why is it called a monolithic kernel? Well, it looks something like this. There is, let's say, kernel space. And there is user space. And, you know, there are user processes which have separate address space. But the kernel is one giant blob, single address space. And within this single address space, there are all kinds of modules like file system, virtual memory, scheduler, drivers, and so on. Okay, so this is one big sort of, one big blob. And that's why it's called monolithic. This is, this is by far one of the most popular models of an operating, organizations of an operating system, at least in desktops and server systems. And that's the model that you, for example, use in modern operating systems like Linux, Windows, DSTA. What are some bad things about this kind of organization? Firstly, because they are all sharing one address space and one protection domain, if there was a bug in your device driver, it can actually bring down all the other things. Right, if there was a security hole in your device driver, he could use it to look at your files, he could use it to look at your virtual memory subsystem. So there's very weak isolation between all these different components. And so it can, you know, gradually become really large. This whole monolithic kernel can become really large, especially the device driver, because there's so many devices, they become, you know, they become a problem. The other issue is that performance has to be tuned. You know, you have to work with an assumption that, you know, so there has to be some file system here, right? So let's say the file system that we discussed, So how do I choose whether EXE3 is my right file system, or whether something else is the right file system? Well, my choice will depend on what kind of applications are going to run on this file system. Right, so I have to commit to a type of file system or an implementation of file system very early. I have to guess what's going to be run on this operating system at the time of OS design. And that's not a, you know, it would have been better if a user had complete flexibility to say, I'm going to run a database application. So EXE3 is not the best file system to have. And so, you know, here is my file system that you can plug into your operating system. So, you know, there would be more sort of customization to applications in terms of what's going to be run on this operating system. So, you know, there would be more sort of customization to applications in terms of performance and even functionality. But that's not possible in this monolithic model. The best you can do is support a few different file systems, like, you know, EXE3 and TFS or something. But that's not that's not flexible enough. So there's another model called the microkernel, which was made popular in the mid-90s. And the idea here is that you would implement all these services, file system, virtual memory subsystem, scheduler, drivers, and anything else that the operating system provides as just separate servers. Servers in separate address spaces. And then there will be other applications that's running. And the applications could talk to these modules like this. The interface for talking to these modules or to the servers is what's called inter-process communication. We know about this IPC, inter-process communication, right? And so what the kernel is, kernel becomes really thin. All these different servers that were present as part of the kernel in the monolithic model are no longer part of the kernel. The kernel becomes really thin. All the kernel needs to do is implement abstractions that allow inter-process communication. For example, it can just implement pipes and be done with it. And then there are all these different modules that are running all these different servers. So that's a microkernel. Of course, you know, you could have privilege levels. For example, you could say that the file system server can access your disk while a regular application cannot directly access the disk. The virtual memory server can access the physical memory, but, you know, other things cannot access. So you can implement. So the kernel is basically IPC plus protection. Now, who can do what? Basically, access control. That's all. So the nice thing about this is, you know, it gets rid of both the problems that I mentioned in the monolithic kernel. If there's a bug in your device driver, the only thing that goes wrong is your device driver. All your other parts of the kernel remain completely isolated because these are completely executing in different addresses. More importantly, if I want to run a database application, I can, you know, choose my file system by just supplying code and, you know, doing some level of authentication to say that this code is trustable in the terms of, you know, letting it access my disk. And so, you know, I can use it. It sort of also, you know, fits in with the principle of least privilege that we talked about. I only need to give privilege to this particular server to be able to access the disk and nothing else. For example, it cannot directly access physical memory, for example. So these are good advantages. What's the problem with it? Why is it not so popular? It's slow, right? If, you know, the application needs to talk to the virtual memory system, it actually needs to go from user to kernel, kernel to user, then user to kernel again, and then back, right? So there are at least four user kernel crossings that you have to make as opposed to two in the monolithic kernel case, right? So it's slower. And there were lots of techniques that were proposed to make, to implement fast IPC. So special interfaces were developed in the kernel, and there are, you know, the microkernels like L4 that implement fast IPC, but it's not. The idea hasn't caught on as much. It is caught on in specialized domains like embedded systems, where protection is very important, where reliability is very important, et cetera, where you could. Okay, so the VM process, the VM server was supposed to provide address spaces to other processes. Now it itself is a process. So does there need to be some bootstrapping? Yes, I mean, you know, you could imagine that there's some amount of bootstrapping that's going on. And this VM process has a special address space, you know, which is equal to the size of, to the, you know, maybe it's equal to the identity mapping between virtual address space and the physical address space. And then it has special privileges where it can actually make mappings and change the page tables of all the other processes. So that's just protection and access control. You know, this process is allowed to change the page table of that process. So that's basically, you know. But, you know, the nice thing is that different servers, so I can come to the operating system and say, that here's my application, and I know that the best possible, the best cache replacement algorithm for this application is not LRU. It is, let's say, MRU, right? So I can just change my VM subsystem. You know, I can just plug and play and use MRU instead of LRU. Similarly, I can use, instead of logging, I can use ordering or whatever, you know. So there's plenty of choices. What's my cache replacement policy? What's my buffer cache replacement policy, et cetera? What's my prefetching degree, et cetera? So it can all tune it and completely configurable. On the other hand, if you use something like a monolithic kernel like Linux, the developer of Linux has pre-committed you to certain algorithms, which may not be the right thing for you. Okay, that's a good question. So in monolithic kernels, there's an ability to install new drivers. And this ability to install new drivers is what's called through loadable modules. So in modern kernels, you have the notion of loadable modules. That's what I was talking about. Loadable modules are nothing but blocks of code and data that can be loaded in kernel space. And they execute with the privileges of the kernel, right? And can't you just use the loadable module idea to plug and play other parts of the kernel, like file system, virtual memory subsystem, and so on? Firstly, it doesn't take care of the problem of protection yet, right? Even the loadable module has identical privileges to everything else. So if there's a bug in your loadable module, it corrupts the entire system. And if you allow arbitrary things to be loadable, then you're increasing your surface area for that. So protection is not handled. The other thing is that the loadable module interface needs to be very carefully designed so that it's rich enough to support all these different subsystems, right? For example, at one extreme, that loadable module could just be behaving like the server in your microkernel, where it's just the only way to talk to it is IPC. At another extreme, the interface is so rich that you can actually directly make function calls to it, et cetera, instead of doing IPC. And so choosing that... So in general, while loadable modules have been used for drivers, et cetera, they're actually also used for file systems, really. So in some sense, a loadable module is somewhere halfway between microkernel and a full monolithic kernel. It is giving you some advantages of microkernels in some sense. But there's a protection problem that still sort of doesn't get solved. Okay, so that's microkernel. There was an alternate organization called an exokernel that was proposed somewhere in late 90s. And exokernel was also a microkernel, except that it tried to move most of the functionality of the operating system at the application level. So here the idea is that let's say this is my kernel and this is my application. Earlier, my VM and file system were living inside my kernel. Instead of that, let all these subsystems live inside the application. But, you know, how can they do that? You provide interfaces such that these different modules can actually function. And so you're basically moving the layer of abstraction one level below. Okay, so instead of exposing a file system to the application, you expose a raw disk to the application and let the application implement its own file system on top of it. Or instead of exposing an address space to the application, you expose physical memory and a page table to the application and let the application choose what mappings it wants and what cache translation algorithm it wants. Instead of saying that you have a scheduler, you have a virtual CPU that's uninterruptible, you export the CPU as it is and you tell the application, oh, there's a timer interrupt. You need to choose what you want to do. You know, you need to relinquish CPU and let the application decide what it wants to schedule and when it wants to relinquish CPU, right? So let me use a concrete example to show this. Okay, so let's say I'm using exocodal example. My system call interface, syscall API looks something like this. Physical page is equal to alloc page. Then there's dealloc page. And then there is create mapping from virtual address to physical address. Dealloc page is, you know, it takes an argument physical address. These are the down calls. I'm going to explain what down calls means. And then there are some up calls. Up calls are page fault on virtual address and please release the page. What I'm doing is I'm exposing APIs to the application to allocate a physical page. So application has full control on physical memory or not full but at least, you know, better control over physical memory. You can say, I need one more physical page. So it knows exactly what's its current physical memory footprint. As opposed to a monolithic kernel where the application was completely oblivious of, you know, what physical memory footprint I actually have. All the monolithic kernel application sees is an address space. It's the operating system that's playing tricks under the carpet in implementing that address space. Sometimes it's mapping the virtual page to a physical page. Sometimes it's mapping it to a memory map file and other times it's mapping it to a swap space, right? So instead of that, give full visibility to the application. And let it say, I want to allocate a physical page. You know, don't play tricks under the carpet at all. Give it the control. And so it can also say, I want to deallocate this physical page. And you can control completely tightly how much physical memory footprint it has. And then it can create a mapping between its virtual address to its physical address. For example, if it wants to create multiple mappings to the same physical address aliasing, it's free to do that. So these are the down calls. By down calls, I mean these are system calls that the application can request from the operating system. And then there are some up calls. Up calls are something like signals in Unix. Where the operating system can tell the application or ask the application to do something. So one of the up calls is a page fault, right? So a page fault up call basically means that the application tried to access a virtual address that it is not currently mapped. So it hasn't created a mapping yet for this particular virtual address. So instead of killing the process or instead of doing things under the carpet itself, it just tells the application that, look, there's a page fault that has occurred. So it converts the hardware exception into an up call signal to the application. The application sees, oh, there's a page fault on this virtual address. And that's because I haven't created a mapping for it. And that's because I'm not sure that's because I'm implementing my own virtual memory subsystem. So what it may want to do is allocate a page, create a mapping, and then re-execute that instruction. Return from that thing. Right, so it's giving full control to the application. It can choose which page it wants to replace, right? As opposed to the operating system choosing on its behalf, this question. So what about protection? Good question. What about protection? What are some bad things that can happen? Well, how do I prevent an application from just doing alloc page, alloc page, alloc page till it completely exhausts the physical memory? Well, I can implement protection at that level. I can say that, you know, this application, I can implement co-tasks for every application. And I can say that I won't give more than these many physical pages to this particular application. So, you know, it's perfectly fine. Alloc page can return minus one to say that, you know, I didn't allocate a page for you and you deal with it yourself. So that's your problem. Right? So you can implement protection even at this level. All right? What about, yeah, question. The question is, if each process implements its own cache replacement algorithm, won't there be a problem? I don't see a problem. Why is there a problem? You're just saying that what I have done is I have basically decided that I'll give this pool of physical pages to you, that pool of physical pages to you. You can choose how you want to use this physical pool of pages. It's your decision. You can optimize based on your application, which pages to bring in physical space and which pages to keep in soft space. That's your decision. All right? So multiple algorithms, cache replacement algorithms can work simultaneously tuned to your respective application. Let me give you a concrete example. Let's say I'm a database. And I'm running the database as a user level application. A database will implement some kind of a cache for all the disk blocks. Right? And so if this disk block cache is implemented in the virtual address space, which it will be, then, you know, the operating system has no visibility that this is actually cache pages. And so it will treat these pages as regular pages and it will keep swapping them in and out of the soft space. But that's a really poor thing to do because the database is using them, those virtual address space pages as cache slots. And now the operating system is moving cache slots and, you know, doing really hard work to maintain consistency of those cache or correct behavior or correct contents of those cache blocks and moving them in and out of soft memory. Instead, if I had told the database server that I need some memory back from you, the database would have known that these are my cache blocks and I can throw them at any time. These are just cache copies. I don't need to preserve their content because the contents are actually already present in my disk blocks. Right? So that way, you know, there's a, you can actually save lots of extra copying between your database disk and your soft disk. Okay? Now let's talk about the protection bit a little more. So what's the other thing that can happen? So one thing is, you know, an application is calling allocate page, allocate page, et cetera. Well, let's say an application is running happily. It has lots of pages. And then there are lots of other applications that start simultaneously. And they also need pages. So now at this point, the operating system would want to take pages away from this already running application. In the monolithic kernel, it was very uncivilized because I would just, you know, take pages from you and you won't even know. And we said that's not always a very good thing, right? Because the application knows which pages to get. In this case, you will make an up call saying, please release a page. So here is a different thing. You're being more civilized. You're saying, please, please a page, right? Now the application is going to decide which page to release. Application knows exactly how many pages it has, et cetera. And it's running at some algorithm to say, but what if the application does not honor your please, right? It just says, you know, okay, it takes you for a ride. It takes the kernel for a ride. So that's a protection problem. I want a page for other applications. I'm requesting this guy for a page, but he's not giving it to me. So the solution to that is, you know, a kill, or you could say, you first, you know, you add one more up call here. Forcefully, force release a page, right? Or, you know, just do whatever you were doing in a monolithic kernel. You first ask the application, please, you know, wait for some time and also maintain some history on its reputation or whatever. And then you can, if it's not doing what you wanted it to do, you could probably just take pages away from it. So, you know, you could implement protection orthogonally to your common interfaces. And so I get both protection and performance at the same time. Okay, I'll continue this discussion next time."}