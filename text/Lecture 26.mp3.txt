{"text":"Welcome to operating systems lecture 26, all right. So today I'm going to talk about transactions. So we have seen locks as a way of providing atomicity and transactions are another way of providing atomicity, right? What is atomicity? Atomicity is that you want certain piece of code to appear indivisible, right? It shouldn't be possible to interleave between instructions of that code and that code is called a critical section, right? And we use locks to basically, you know, we made an acquire before the critical section and made a release after the critical section and we said that that will ensure atomicity. We also studied monitors. We said, you know, locks are locks are error prone. If there is a first-class support in the language then it will it becomes slightly easier for the programmer to reason about things and monitors was basically something that was also providing atomicity but internally monitors are also using locks. So internally the monitors are also basically using an implicit monitor lock to basically provide mutual exclusion, right? How do locks work? Locks work by, so you know, the programmer has to a priori tell that I'm going to access this shared data and he tells it by calling this function called acquire and he calls it on this lock which corresponds to that shared data, right? So it's an a priori declaration by the programmer that I'm going to access some shared data and this is the shared data I'm going to access and the semantics are that the acquire will not let more than two threads access the shared data. It will not let more than two threads fall through, right? This can be called a pessimistic way of doing controlling concurrency in the sense it's like saying that let's say I wanted to enter a room where I needed some privacy I always whenever I enter the room I put a lock inside it before I start doing anything, right? Let's say you know whatever I so I always put a lock irrespective of whether somebody else is actually going to try to enter the room or not, right? If in the common case it's the case that nobody else is going to come into this room anyways, right? I'm just doing this extra work of putting the lock first then doing my work and then unlocking it and then going out, right? But you know if nobody actually came during this duration while I was using the room you may say that this this work is sort of wasteful and if this is the common case which in many cases it is, right? In many cases if you don't expect probability that somebody will actually access the same resource at the same time is often small so you think that you're you know you're doing this extra work every time even though the probability of this event happening is small, right? A more optimistic method would have been that I don't use the lock I just enter the room and I start using it and if somebody comes later he opens the room and he tries to use it he figures out that oh somebody else is already using it and then he rolls back his execution in other words he just you know in the physical analogy he just goes back and says you know let me I'll retry some other time right now somebody else is using it. If I have that kind of an organization then in the common case when there's very little chance that somebody else is going to come at the same time you know I didn't have to do any extra work to actually put the lock on the door I could just come in do my work and get out and if somebody else comes in during that time he'll roll back if needed, right? This is more optimistic this is more optimistic I call this optimistic because I'm optimistic that the probability that there will be a collision or there will be a concurrency violation or atomicity violation is small, right? So this is more it's more optimistic in that sense. Lots are more pessimistic there you say that you in sense you're some in some sense saying that the probability that somebody will actually violate atomicity is very large you know you're always basically doing this, right? Okay, so in software this act of actually coming into the room checking if somebody's just is using enough not and rolling back can be done by enclosing some the code into a transaction, right? So this whole process of actually coming in and trying to do something and then figuring out that you have failed because somebody else is using it and going back can be done by enclosing this entire code inside what's called a transaction, right? So let's look at some examples of how what a transaction looks like, right? So let's let's look at this example this running example that we've been using for transferring a unit of money from account A to account B and here is the code which uses fine-grained locking to ensure atomicity of this particular function, right? And we have seen this before. As a mutex associated with A, as a mutex associated with B and you basically take both mutexes before you do anything and then you release both. Once again this is a pessimistic, I call this pessimistic because the probability that there is another thread that will access one of these two accounts A and B is very small, right? At the same time. Yet I am pessimistically taking these locks each time, right? Let's see how I would have written this in the transactional way, right? So you see I would have done something like this. Let's avoid transfer from account A to B. I would declare two local variables AM and BM representing A's money and B's money and I will use a do-while loop to keep retrying. Let me write this code fully before we start discussing it. TX begin will say that a transaction has begun. AM is a local variable which will read, call TX read on A dot money. Let's say BM is equal to TX read B dot money and then you know so what I have done is I said that this is a transaction. The transaction will first read the value, shared value. So A dot money and B dot money are shared values and I am reading those shared values into local variables, right? And then I am going to perform my operation on the local variables. For example, I will say if AM is greater than 0 then AM minus minus and BM plus plus, right? And finally I will try to commit the transaction and in the commit let's say I can give arguments which say write AM, the value AM to A dot money and the value BM to B dot money. I do this while loop till I succeed. Right, so let's see what's happening. Firstly I declared that whatever I'm going to do next is a transaction. By saying that everything that I'm going to do next is a transaction, it basically means that whatever I'm going to do is going to have a tentative effect. It's not going to be a final effect. What I'm going to do is tentative execution. It's like I've opened the room and I'm trying to enter it but what I am doing is tentative execution and if I figure out that there is some collision, somebody else was using the same room at the same time, then you know I know where to go back. I go back to the begin point, right? So that's what the TX begin is telling me that here's a transaction and whatever I'm going to do from here on is all tentative execution and it's going to be tentative execution till you commit it, right? So this code between the begin and the commit is all tentative execution and the commit function will atomically commit the results of the tentative execution onto concrete state, right? So what am I doing? I read, so I started a transaction, then I said read the shared value into a local variable, read the shared value into local variable, perform some operation on the local variables and then try to commit the new values of the local variables to the shared variables. This commit function is atomic, right? So commit function is atomic. It will, you know, in one go try to update the values of a.money and v.money and it will succeed if between the TX begin and TX commit nobody else changed or read the value of these shared variables, right? So I'm basically saying here are the shared variables I have touched and here are the new values of these shared variables. Commit them to the real state if nobody else has read or written these variables during this time. So the question is what is the problem if somebody has only read it and I try to commit, it should commit fail if somebody has only read it. Yes, it should, right? Because it will again violate atomicity. Somebody has read the old values. Atomicity means this whole thing appears as either having done completely before it or completely after it, right? So if I read let's say A before this transaction started, before this transaction started I read A and then after this transaction committed I read B then atomicity is violated, right? I mean so you're saying that you know it's possible that you may not need to commit even if there was a conflicting read. You may not need to fail the commit even if there was a conflicting read. That's true. You know there may be some situations where that may be true but in the general case that's not true. Okay, so let's say there was another transaction that's doing the sum, right? So our running example is another transaction that's doing the sum, right? So let's say it reads A, after that the transaction commits successfully and then it reads B. Problem, right? So that's a problem because there's no atomicity between the sum and the transfer, right? So you know you're right. I mean in some cases you may be able to reason about it and say that you know you don't need to fail the commit if it was only a read because it doesn't matter. My code doesn't, you know semantics don't change. It still remains atomic but in the general case, right? Because I mean in the general case you would want to say that if there was a conflicting read or a write then fail the commit and sum is an example in this case. All right, so okay so it's going to commit it. It's going to try to commit it in an atomic way and if it tries to commit it and it figures out there was a conflicting read and write in this area while I was executing in this area then it will fail the commit, right? So how does it figure out this area? It just looks at you know it just looks at the time when the transaction was begun and between the commit. So if in this time there was a conflicting read or write it shows that there was a problem. You can optimize it further. You know you can say that you know if there was a read or write here you may be not care etc. But so but at the very high level that's what it means that you basically look at what are the conflicting who are writing and reading what locations and then the commit operation is going to check this atomically and then either succeed or fail. If it fails then you just loop back and you retry the transaction and you retry it afresh, completely afresh, right? You read new fresh values of aid or money and you retry it again. Yes, there's a question. Okay, so do both threads get not success or does one thread get success? It depends on the semantics of your transactional memory system. You know either are possible but let's say you know both of them get not success. So depending on how you have implemented your transactional memory it's possible you can implement semantics where one of them is guaranteed to succeed which means there will always be progress and you know a simpler implementation a more efficient implementation may only guarantee, may not even guarantee that. So it may say that you know it's possible that both of them fail, right? In which case both of them are going to roll back and which has a problem of what's called a live lock, right? This is different from a deadlock so it's called a live lock. Live lock basically means that here are two transactions that are continuously doing work but none of them is actually making progress. In a deadlock none of the threads are doing any work and neither are they making progress. In a live lock threads are actually doing work yet they are not making any progress. So they just fall through, try to commit, both of them roll back and so on, right? And so that's a really bad situation to have a live lock but you know transactions are useful if you expect that concurrency is going to be there and so the probability of this live lock is very low, right? So all this sounds much more complicated perhaps than the locks. Whenever concurrency occurs, won't it always result in live lock? Well I mean not necessary. It's just a matter of what schedule happened, right? So in the first case it may happen that you know none of them succeeded. In the second case it may happen that one of them definitely succeeded. In general you know the transactional memory system will have ways to tolerate a live lock. One is that you know one will always succeed. That's you know that's one way of doing it. The other is you know if there's a retry then you know maybe have some delay in the middle. So there are multiple ways of making sure that live locks become less and less probable in the common case, right? But at least in any case there's a lot of overhead to actually doing a roll back and retrying the whole thing again. So yes you can prevent live locks in many ways. One is you know make sure that the transactional system is basically always making progress. At least one of them will successfully commit if there's a conflict or you know you basically make sure that retries are serialized. You know the other way is that retries will always get serialized. So you know if you figure out that something has failed then you try to serialize execution yourself. So those are all ways to do things make sure that live lock doesn't happen. Okay yeah so what is the advantage of such an implementation? I said that you know locks have this extra overhead of actually locking and you know it would have been better if I had been more optimistic. But this looks even more complicated, right? Why is it more complicated or why is it more expensive I should say? Firstly we have to roll back but let's say you know rolling back is a very rare operation because I started with assumption that the probability of concurrent access to a shared resource is small, right? So let's say the probability of a roll back is very rare, right? And that's true in many cases. That's true for the account example, the transfer example that we have taken, right? The bank account example. So anything else? Exactly. So you have to basically track what are the locations that have been read, what are the locations that have you know and access and so on and you know how to do this bookkeeping somewhere, right? And you know how do you do this bookkeeping? You basically you know for this for every memory access for every memory location and for every memory access you have to store some data thing you know whether it was accessed or not and these kind of things and this seems very expensive. This seems much more expensive when actually doing taking just a lock, right? A lock just involves setting a bit to 0 or 1, right? So yes it's expensive because you need to do extra bookkeeping about what memory locations are accessed. Transactions are or you know what resources are accessed I should say instead of memory location because memory location is one type of resource but there could be other types of resources. Transactions are used widely in databases or you know anything that involves disk accesses, right? File systems for example. Why? Because it's easy so each read operation is a in a in in in transactions that involve disk operations involves a disk access. A disk access or a disk block access. A disk access can only be done in granularity of a block, right? You have to take a you know either read a block or write a block and you at disk access is very expensive, right? It's already milliseconds to access a disk and so this extra bookkeeping information about which blocks have been accessed and which blocks have not been accessed is very small in comparison. It can be stored in memory and it's very efficient to store this in memory. It's not a big deal, right? So transactions are very useful if if this operates the read operations or the access operations themselves are very expensive, right? Then this extra overhead of actually doing this bookkeeping becomes very small relatively. So transactions are great for something like databases and and they are used in fact to to ensure atomicity of database transactions. The other advantage of transactions is that you don't have to worry about a lot of things. You don't have to worry about fine-grained locking. You can put a large area of code into one transaction and it's the runtime system that's keeping track of what you have read and what you have written and whether you want to roll back or not, right? So for example, notice that in this code nowhere am I associating a lock per account or anything of that sort. I just put the entire transfer function within one transaction and it's a runtime system that's figuring out whether this transfer and that transfer conflict or not, right? By comparing the read-write sets, you know, what memory locations has he touched or what what resources has he touched and what resources has he touched and just comparing it. As the runtime system that's doing it, the job of the programmer becomes much simpler, right? So if he doesn't have to do fine-grained locking, he can put a transaction around the entire code. He doesn't have to worry about, you know, figuring out the right locks or placing them in the right place. He doesn't have to worry about deadlocks. That's a big win, right? Deadlocks, as we have discussed, is a big problem because firstly you have to have a global order on all the locks. That's often very difficult to get. You have to reason about, you know, what are all the locks that and different areas of your code have different types of locks, etc. And all of them have to be in a global order. That's usually a difficult thing to do and it also kills modularity, as we have seen before. It doesn't allow you to write you to write your code in a modular fashion because now you worry about what kind of locks is this he's going to take or he's going to take, etc. And what operations do I need to be atomic, etc., right? With transactions, anything that I need to be atomic, I just put a begin transaction and an end transaction around it and we're done. Okay, so it's very useful in that sense and that's one of the primary reasons that transactions have actually been very successful in the database community or database world because, you know, having to do locking in databases is actually very, very error prone. Apart from being expensive, the bigger problem with doing locking in databases is it's very, very error prone. You have to worry about what are all the, you know, let's say tables in your database and whether, you know, you're taking the locks in the right order and things like that. On the other hand, you just put it in the transaction and the database system or the runtime system will figure it out for you. All right? Yes. Right, so how commit is implemented is another thing. So it really depends on your runtime system how you will implement commit and the question is don't I require locks to implement commit? You may or may not, depending on, you know, let's say you are doing, you're implementing transactions on disk locks. Yes, you require locks, but these locks are only in-memory locks, right? And more importantly, so and more importantly, the statistical section is very small of your lock. It's just about updating the memory right there, right? So, I mean, even if you require locks in the commit, it doesn't, you know, it doesn't obviate the advantages of or it doesn't reduce the advantages of transactions. What's the problem if it requires locks? Right? The programmer still doesn't have to worry about things, right? And for databases, it's still, you know, equally expensive or less expensive than using locks, right? Now the question is, do transactions make sense in operating systems? Right? So what's the difference between operating systems? In operating systems, these are likely to be memory accesses instead of disk lock accesses. And so this business of actually doing the bookkeeping has, is actually, you know, doing the bookkeeping is likely to become more expensive than actually doing the access, right? Because you have to do a bookkeeping for every shared access that you're doing. Also, the commit is likely to become very expensive because, as I was pointing out, you will probably need locks and, you know, and then needing, having to go through all the read lists and write lists and performing, performing intersections. So for that reason, you may say that transactions are not really suitable for operating systems. And that's, that's the reason that so far, at least, you know, transactions are not usually used in operating systems. And because, you know, they're too expensive to be used. Locks seem to be cheaper to use in operating systems. However, you know, given that we are moving in the, moving towards multiprocessors and more and more processors and so on, there is a huge need to make more and more of our code more and more parallel. And that has huge software engineering challenges for programmers to be able to do fine-grained locking and all this. And it makes your program very, very, very highly non-modular, right? So later, you know, very recent processors like the Intel Haswell processor actually have hardware support for transactional memory. The idea is that they have instructions called transaction begin and transaction commit. All right? And using those instructions is the hardware that will do all the bookkeeping for you within that block of transaction begin and transaction commit. And when you do the transaction commit, it's the hardware that will compare the read and write lists of the two transactions and decide whether the commit was a success or a failure. Right? The assumption here is that doing these things in hardware is likely to be much faster than having to do these things in software. All right? And there is evidence to believe that at least for some number of processors, it is advantageous to use transactional memory over locks. Right? And so that's the reason that a lot of investment has gone from many companies and, you know, one example is Intel's Haswell processor, which is actually being available today. All right? Okay. No, the hardware support is for transactions on memory accesses. Right? So if you say a transaction begin and then you make a memory access, it's the hardware that will do the bookkeeping about what memory accesses you have read or written. And when you do a commit, it's the hardware that will compare these lists of memory accesses and decide whether the commit is successful or failure. Right? And so that's, you know, that's the domain of transactional memory. I won't go into too much detail about it, but just to tell you about transactions, how they've been so successful in databases, and how they are being very seriously looked upon as a potential option of handling concurrency in operating systems by using hardware support. All right? Okay, so the question is, you know, I am saying that locks actually hamper modularity, and I am saying that because if there's a function that takes a log and then he calls another function that takes another log, then there's a possibility of deadlock and all that. And the question is, if you take one another lock after holding one lock, can't you do something called a conditional lock where the second lock basically is released if you take the second first, the first lock is released before you take the second lock? What do you think? I mean, if you can't just release the lock arbitrarily, right? I mean, the lock is there for a reason. You can't just, I mean, if you just start releasing locks, then you are losing atomicity. It's not acquiring the same lock. So modularity is killed because it may be acquiring a different lock, right? And then you have to worry about the order of these two different locks. I mean, if it's the same lock, then you know what you're saying is similar to recursive locks that we've discussed. And we also discussed problems with recursive locks. In any case, recursive locks are an option, but really I'm not talking about modularity being hampered because of acquisition of the same lock. I'm talking about modularity being hampered because of acquisition of different locks, because locks need to be acquired in a certain order. Okay, all right. So basically transactions are an interesting idea. People are looking at it, and whether they can be used in operating systems or not remains to be seen. However, one type of transaction is actually being used for many years now in operating systems, and these are transactions that involve single memory access. So if you can write your code as a transaction such that it is going to access only one shared memory location, and then the hardware has been providing support for a long time to implement such a single memory access transaction, and operating systems have been using it for a long time. Operating systems and in general concurrent programs have been using it for a long time to do this, and one common paradigm to do this is basically what's called a compare and swap instruction. Let's see what's this compare and swap instruction. If I was to write the C semantics, so compare and swap on x86 is called the compare and exchange instruction. It takes three arguments, register one, which I'm going to call R old, let's say, register two, that I'm going to call R new, and a memory location M, right? And the semantics of this instruction are the following. I'm going to write C code to describe the semantics. It's, you know, let's say I write it as a function, then it's compare and swap M, the address, just to make it clear, in bold, and just, you know, first read, let's say, was is equal to star adder. It reads the value in adder into a local variable, or locally into the, in the hardware for the instruction, and compares the current value with R old, or with old, let's say, in the C thing, was equal to old, then R adder is equal to new. I'm going to explain it very soon. Okay, so what's going on? Basically, the semantics are, check the contents at location adder, compare them with location old, with value old, right? Check the contents at location adder, compare the contents with value old. If they are equal, then replace the contents of adder with new, otherwise don't do anything. The idea is, I have previously read some value from this location called adder, I have it in the register, and now this instruction is going to atomically check if the value of that location is still the same, and if so, then it's going to replace it with the new value, and if it's not still the same, then it's not going to replace it with the new value. So let me write it in plain English first. Compare exchange, if Rm is equal to R old, then Rm is equal to R new. That's all, in some sense, and this is done in an atomic fashion. We check if the value at that address is the same as the value that you have in your register, and if so, then you replace it with this new value, otherwise you don't do anything. And just one more thing, it basically also returns the old value of star m, or let's say, let's say temp is star m, it returns the old value of star m in R old. This operation is actually very similar to the semantics of the commit operation in a transaction, right? What is the commit operation? The commit operation says, change the value of these locations with these new values if nobody has written in the middle, right? So the compare and exchange instruction is saying similar. Change the value of this location to this new value if its value is the same as the value that I have read before. So this nobody has written in the middle is captured by the value is the same, and this is an atomic instruction. You know, you can put the lock prefix behind it to make it atomic. So what am I going to do? I'm going to use this instruction to commit one memory access to transactions. So any transaction that can be, that has done only one, that needs to do only one memory access, one shared memory access, can be modeled as a transaction. So any atomic region, any critical section, that can be modeled as one update to a shared memory access region, can be modeled as a transaction using the compare and exchange instruction, right? Okay, let's see examples. So let's say I had this operation where I wanted to do hits is equal to hits plus one. So far I was using locks to do atomicity of this code, right? So I was basically saying acquire here and release here. Now let's try to do this using a transaction, right? So I can say, if I want to do hits is equal to hits plus one, and I want to do it in an atomic way, I could do something like this. I could say, let's say I declare a local variable called local hits, and I'll declare another local variable called local new hits, let's say, just two local variables, doesn't matter how many local variables I declare, and I read the value of hits into local hits. Then I do this operation, hits is equal to hits plus one. Let's say, sorry, let's say I do L new hits equal to L hits plus one. What have I done? I have read the old value of the shared variable hits in a local variable. I have performed some computation on this local variable, and now I have both the old value of that shared variable, and what is the new value that I want to put in the shared variable? And so what I'm going to do is I'm going to say compare exchange, what? Hits as the address, L underscore hits is the old value, and L underscore new hits is the new value. I'm going to atomically try to update the shared variable hits with L new hits, but I'll only do it if the hits variable still is equal to the old value. If it's not equal to the old value, what does it mean? Somebody has concurrently tried to increment hits, and so you should not commit, you shouldn't update now, right? So you shouldn't have the lost increment problem, right? So this will, you know, this will return the old value of hits, whatever the value was read into hits, and let's say it's written in the register, I'm going to write it in the functional form, so L underscore hits is equal to compare and exchange this, and I basically say if, or let's say what, let's say I say, let's say I declare another variable called local, let's say local ABC, and I don't, I can't, so let's say I just say L underscore ABC is equal to compare exchange this, so this is going to return the value that was read by this atomic instruction, and recall that this entire operation of reading the value, comparing it, and updating it is completely atomic, right? So I basically read the old value into LABC, and I say that if LABC is not equal to L underscore hits, then what do I do? Retry, right? So goto retry, and let's say retry is going to be where? Here. You can write it in loop form, I'm just using goto because of lack of faith, but you know, basically you just want to retry the transaction. Write it as a while loop for example also. This code is ensuring the atomicity of hits plus plus, but in a transactional way. Also, this transaction guarantees that at least one transaction will always succeed. It guarantees progress. So this transactional system, so we had this discussion on if there was a conflict, or if there was a failure at commit, do both roll back, or does one, or one of them will always succeed. In this case, one of them will always succeed. Whoever executed the comparing exchange first, and notice that the comparing exchange is an atomic instruction, it's going to succeed. The second one is going to fail, and he's going to go to retry. Yeah, it will allow multiple reads. So it's basically guaranteeing atomicity against concurrent writes. In fact, it's even weaker than that. It's not guaranteeing atomicity against concurrent writes. It is guaranteeing atomicity against concurrent changes. If a concurrent write happened, but it left the same value, it doesn't matter. So it's not a complete transaction. It's not the same semantics that we discussed. It's not comparing the read-write sets. It's actually looking at the value, and if the value has changed, then it will fail. If the value hasn't changed, then it will succeed. So if there was a concurrent write that wrote it, but the value remained the same, no problem. So that's the semantics of CONCAT, and whether it fits in your program logic or not, that's really up to you to decide. So notice that I have avoided locks in doing so, and so all the problems associated with locks are also gone in some sense. Firstly, I don't need an extra shared variable, which is a lock, and secondly, you know, because there's progress guaranteed, you know, there's no problem of deadlocks here. Okay, now let's take another example. We were looking at this insert function, right, initially. So we said insert into list L some data, and we had this lock, we had this Perlis lock to do this insertion, and let's try to write this insert as a transaction. So how, let's see, I mean, using compare-and-swap, right? So compare-and-swap is also called CAS, and I'm going to use the word CAS to represent compare-and-swap instruction, or compare-and-exchange instruction, right? It's a common term called CAS to basically do this. So how can I do this? Well, I can say, let's see, new, let's say E is equal to new list, E dot data is equal to data, and then I say E dot next is equal to list dot head, right? And finally, I'm going to say list dot head is equal to E, right? Recall that was my code. List dot head is equal to E. So notice that all these three locations, or three instructions, are only operating on local data, or at least writing to local data. They're not writing to shared data. It's only this final instruction that's writing to shared data, right? And it's possible to write this using CAS without having to use locks in the following way. You can say, you can do compare-and-exchange on L dot head, with old value being E dot next, right? And the new value being E. Good, right? If you atomically try, so what's happening? Let's say this was head. You have already made E, and you have made it point to head, and now you atomically try, and you already know that, you know, the time when you read L dot head, head was here. Now you've atomically tried to make head here, but you only do that if head is still equal to this. If there was a concurrent insert, then head would have become something else, right? If there was a concurrent insert, head would have gone here. So let's say there's a concurrent insert, and somebody was trying to do this, and so let's say he won, then head would have come here, and so the second insert would have failed, right? So if this succeeds, that basically means that nobody has been able to do a concurrent insert for me, and so what I do is, if you know, if compare-and-exchange is equal to E dot next, is not equal to E dot next, then retry, right? So go to retry. Retries again. Where should retry be? Yeah, go here. Let me write it again. Void insert, L data, E is equal to new, E dot data is equal to data, retry, E dot next is equal to L dot head, L dot head, or now. This is the, this is the tricky part. If, if, if cache, instead of compare-and-exchange, I'll just use the word cache, if cache, L dot head, actually address of L dot head, comma, E dot next, comma E, is not equal to E dot next, go to retry. This is a, this is a lock-free way of ensuring atomicity of the insert operation. So these are, these are called lock-free operations, this compare-and-swap kind of things are called lock-free operations, and you know, at the heart of it, it's really a transaction with a single memory, okay, with a different kind of semantics. It's not doing read-write set intersections, it's actually looking at the value to decide whether there was a conflict or not. Finally, so, any questions on transactions before I move to my next topic, very briefly? But, so, finally, I'd like to point out that there's something called a reader-writer lock. The idea is that often there are multiple functions, some of the functions are only interested in reading the shared memory location, or shared object, and some locations, some functions are interested in also writing to the shared memory location. It should be possible for multiple threads to concurrently read, because, you know, one read does not, you know, adversely affect another read. It's only a write that affects, adversely affects a read, or it's only a write that adversely affects a write, right? So the conflicts are only between read-write, reads and writes, or writes and writes. There's never a conflict between a read and read. So if you expect that your code has lots of readers, and very few writers, then it's, it's, it's possible to use this new, this, this change abstraction called reader-writer lock, and it has three types of functions. Locks have acquire and release, this has read-acquire, and read-release, and write-acquire, and release. Struct lock. Let's say struct rw lock. The idea is that the abstraction now says that multiple threads can hold the lock in read mode simultaneously, or the lock can be held in read mode simultaneously, but if somebody's hold, but a lock can only be held once in write mode. So acquire basically made sure that only one, the lock can only be acquired once, at one time, right? A lock cannot be acquired by two threads simultaneously. In this case, a lock can be acquired simultaneously by two threads in read mode. That's possible. But if, but it's not possible to have a lock being acquired in, by two threads in write mode. It's also not possible for two, for a lock being acquired in a write mode and a read mode simultaneously, right? So in other words, read, read, and read-read is allowed, but read-write or write-write is not allowed. So that allows you to express, have more concurrency in your system, if you expect that they're going to be lots of readers and very few writers in the system. All right, good. So let's stop."}