{"text":"Last time we were talking about virtual memory subsystem and how a file can be mapped into the virtual memory address space and then accessed in a random access manner. But today we are going to look in more detail at exactly how a file is implemented using the current set of storage devices that we have and then what is the file system interface in general. Right. So let us look at the storage devices, you know predominantly there are three types of storage devices. In fact, you know the magnetic disk is by far the most popular storage device and we have already seen it. It basically has platters on a spindle and the spindle rotates at some speed, typically 5 to 15,000 rpm. There is an actuator arm which has multiple heads that are reading this data of the spindle of the platters. So these are called platters. The surface is called a platter and this, these disk arms move radially, right. File, I mean it is possible to have separate disk arms for different platters. You know in practice mostly you will have a single disk arm and multiple heads attached to the same disk arm. So all the heads move together in one direction or the other and we also discussed some things about you know how long it takes to seek and in general what is the, what is the throughput of such a system and we are going to discuss it in more detail. There is another technology which is relatively more recent, it is called flash which is available on phones, pen drives, etc. It does not have any moving parts. It is basically made of solid state which means semiconductor except that it is non-volatile. So unlike your DRAM which is your main memory, this one is non-volatile which means its state persists across power reboots, right. And then there is this older storage device that was very popular in the earlier days called tape and this tape is you know if you have seen a cassette player, it is similar. Basically the tape moves, so you wrap around the tape in these spindles and these spindles rotate in the same direction and the tape sort of moves like this and there is a head that reads off the tape, okay. So let us look at these, the characteristics of these devices in more detail before we talk about file systems. So magnetic disk, well typically 1 to 5 platters, so typical magnetic disk let us say, 1 to 5 platters, roughly let us say 1 to 8 inches in height, right. So if you have ever seen a hard disk, you will basically see that it is a package which has you know which is roughly this thick, 1 to 8 inches and it will have 1 to 5 platters in it, roughly 200,000 tracks per radial inch, right. So that is the density at which the tracks are packed radially. Recall that a track is basically a circle on the platter and so there are concentric circles and each circle is called a track, right. And so the typical density today is 200,000 tracks per radial inch. Today you can buy a disk with 100 GB to 2 TB storage capacity. You have already seen the rotational speeds, they are 5,000 to 15,000 rpm, right. So the seek time typical is 2 to 10 milliseconds, right and let us see average rotational latency. So the seek time basically means the time it takes for the head to reach the track that you want to reach and the rotational latency means the time it takes within the track to reach the sector that you need, right. So on average the rotational latency will be the time it takes to make half a revolution, right. So with let us say 7500 rpm, disk implies roughly 4 milliseconds average rotational latency, right. And what is the API of a disk or in other words you know how can you access the disk, how can the OS access the disk if I were to write it in the high level, basically you can say read and specify a start sector, a count and a buffer and similarly you can do write, same thing, start sector, count, buffer. A sector is basically a unit of storage within a track and typically a sector today is 512 bytes, right. So an operating system can ask the disk to read a sector or write a sector and the disk arm, the disk controller will move the arm to the appropriate location and then read data of it, all right. Once the arm has been positioned at the sector from where you want to read, after that it is just a matter of, so the disk, the spindle is constantly rotating, right, so it is just a matter of the head starting to read the data that is stored in magnetic form on the platter and so in doing so with the current speed you will typically get 100 to 150 megabytes per second of bandwidth transfer rate. So after you have positioned the head at the right position, you know, you can now start reading data at roughly 100 to 150 megabytes per second. Of course, you know, if you, let us say, so what that means is that let us say if I wanted to read 4 kilobytes randomly from, you know, somewhere, I have to actually do seek and latency, then I will have roughly 10 milliseconds of seek plus latency, where latency, by latency I mean rotational latency, right, plus there will be some data transfer time which will be the time it takes for 4 kilobytes to go under the head and assuming 100 to 150 megabytes per second, 4 kilobytes will probably take 4 KB upon 100 MB seconds, right. So that is 0.04 milliseconds, let us see, that is 0.00, 0.4 milliseconds, right, 0.04 milliseconds. Yeah, 0.04. All right, right, right, right. It is 0.04 milliseconds. So as you can see, it is highly dominated by the seek and latency, you know, a random 4 kilobyte read is highly dominated by the seek and rotational latency. On the other hand, if you read, let us say, 4 MB of data, then the, you know, then you are basically amortizing the seek and latency over a much larger chunk of data and this time becomes relatively more significant. Finally, disks or other devices support what is called DMA or direct memory access. What does this mean? How does, how does the CPU initiate transfer? So direct memory access says that the device can directly access memory without needing the help of CPU to, you know, do this transfer. So let us just draw the diagram again and let us say this is my bus and this is my CPU and this is my memory and this is my disk. And one way for data transfer is that CPU one by one starts reading data from the disk. So recall that the disk has some registers here, right? So the controller will have some registers through which the CPU can program the disk. For example, by writing into the register, the CPU can tell the disk, I want to read this sector, right? And then it can read values from those registers to get the answers from the disk. And recall that these registers can be accessed either using port map space, so port space like in B and out B instructions that we have seen before, or they can be accessed using memory mapped IO, right? Just load and store on some addresses that are mapped to those registers. So in either case, you are basically accessing the register. So one way of doing transfer is that the CPU basically keeps reading these registers and then writing, keeps writing them to memory. So it has a tight loop which basically reads from the register and writes to memory. That would be a relatively inefficient way of doing it. Most modern disks have controllers that are capable of writing directly to memory. The way it works is that the CPU registers a request and tells it that this is the location of the memory address. The buffer basically points as a physical address that is given to the disk controller. And the disk directly writes to memory. So the CPU just fires a command which says, write these many sectors starting at this physical location in memory. And so the disk controller is capable of doing it without involving the CPU in it. And so that's, you know, you can imagine that's a significant optimization. And typically that's how it's done. You have also seen that the CPU, when the transfer is finished, then the CPU can check whether the transfer is finished either using pooling, you know, periodically keep checking whether the transfer is finished, that's the pooling, or interrupts, right? Or you can configure the disk controller to generate an interrupt when the transfer is finished and the interrupt handler will proceed accordingly. All right, okay. Right? So that's the disk. Let's briefly look at the more modern technology which is Flash. And let's look at, you know, what it has. Firstly, it has no moving parts, right? So that's a significant difference. And which means it's, you know, shock-resistant, you don't have to worry about it, you know, moving parts breaking down or anything of that sort. Also it has faster access than disk. Let's look at, you know, in what ways it's fast. But it's also 5 to 10x more expensive today, right? So two methods of implementing Flash, without going into details, let's just say, you know, let's just understand that there are two ways, NAND or NOR, and the NAND one is more popular today. Okay. So let's see how a Flash device is organized. It's organized into blocks. And blocks contain multiple pages. They are large blocks and the blocks are basically of the size of 16 to 256 kilobytes. And pages are 512 to 4096 bytes. So the page is basically equivalent of a sector on a magnetic disk, roughly speaking. It's between 512 to 4096 bytes. But multiple pages are within a block, and a block is 16 to 256 kilobytes on today's Flash. And total capacity today that you can get for a Flash device is around 16 gigabytes per chip. So there are two significant quirks about Flash storage, which weren't there in, you know, magnetic disks. One, if there's a block, let's say this is a block, and it has a page in the middle, so let's say this is a page, this needs updation. Updating a page within a block requires erasing the entire block before rewriting the entire block again. Right? So just a matter of how it's implemented at a physical level, you basically need to erase the entire block and then rewrite the entire block to update some random location inside the block. Right? So that's a, that basically means that if you randomly want to write something on the Flash, a small write, then it's very expensive. And the second thing is we are out. You can only write to a block, let's say, around, after 100,000 times, after 100,000 times, block storage not reliable. So after you've written to a block more than 100,000 times, you know, the disk is not reliable anymore. So, you know, it has a very limited shelf life. These problems weren't there with the magnetic disk. Right? So there's a, you know, there's a huge amount of research and industry work going on to figure out if Flash can be used as a replacement to magnetic disk for all the applications that, and basically it involves solving these problems, you know, dealing with these problems in an efficient way. So for example, for wear out, what you would want to do is you want to build some support either at the operating system level or at the hardware level to ensure that all blocks wear out at roughly the same time. In other words, there shouldn't be any hotspots in your Flash storage. So you shouldn't, there shouldn't be a block that's constantly being written to, right? Because then that, the disk is only, the Flash storage is only as good as, or lasts only as long as the block that first goes wrong, right? And so that's one optimization. And the other optimization that people work on is basically that, you know, avoid random updates. So organize your file system structures or your disk drivers in such a way that random updates are avoided, okay? And so, you know, there are so many ideas there that can be discussed, but of course that's not the, that's not the topic of our discussion today. Let's also look at magnetic tapes. The magnetic tapes were used a lot for backups till maybe early 90s or even late 90s, okay? But not so much today. Today people use disks for backups instead of using tapes. And the reason people use magnetic tapes earlier was basically tapes were much cheaper at that time than disks, right? So let's see how a tape works. So let's say this is a tape. Then it has, you know, nine bits of storage in one cross-section. So it's roughly 0.5 inch, half an inch wide, right? And the length can be up to, you know, very large, feet, 2400 feet, let's say. And each cross-section of a tape will have nine bits, which means one byte plus parity. That's how typically it's used. So for error detection, you basically have a parity bit. And typical bandwidth transfer rate is roughly few megabytes per second. So as you can see that today's disks are much faster than tapes. And also disks have become – in those days disks used to be much more expensive. And so backups, which involve lots of storage, were done on tapes. And so you would find, you know, lots of tapes in the backup room or in the server room. But today the same thing is instead done using disks. Okay. Finally, let's look at how – let's look at the disk controller in a little more detail. And in particular, I'm going to look at the scheduling algorithm that goes on within the disk controller. Yes, question. Right. So in case of a magnetic tape, if you want to access some random byte, then you have to actually go through the entire tape to actually reach that byte, assuming you basically have variable sized record in your thing. Right. So that's a disadvantage. So that's why it was primarily used for backups and not for, you know, online transaction processing workloads. Right. So let's look at the disk controller. So basically, the disk controller has to decide, assuming it has multiple IOs outstanding, it has to decide which IO to serve first. Right. So by IO, I mean a read or a write request. So it has to decide which IO to service first. Okay. And it seems intuitive that the more flexibility the disk controller has, which means that the more outstanding requests there are, the better job it can do at scheduling it. Right. So for example, one option is just first come first served. Or, you know, also called FCFS or also called FIFO, first in first out. Right. Same thing. So this is okay. This is fair. Whoever comes first gets it. But what, but it completely ignores proximity. So let's say there are alternate requests, one for, one for track zero and another for track hundred. And then there's a track one, then hundred and one and so on. Then you're doing a lot of seeks. Right. You're spending a lot of time doing seeks. The better thing would have been to do zero and one together and hundred and hundred and one together. Right. So that's basically a better thing to do. FIFO is fair, but it completely ignores location. The other thing to do is shortest seek time first, or also called SSTS. So here the idea is, let's say I have N requests in my queue. I will compute which of them is my, is closest to me, is closest to the current location of the head. And that's the one that I'm going to serve. So you can imagine that if there are lots of requests outstanding in the queue, then, you know, you can get, you can get significant optimizations there. So this is good, but, but can lead to starvation, right? What can happen is that you, lots of requests are coming for zero, one, zero, one. And then there's an outstanding request for hundred, which will never get served because the shortest is basically always between zero and one. This is the part of the disk controller. So which means on the hard disk. So this scheduling algorithm is within the disk. So I'm currently discussing, you know, how the disk controller is optimizing things within that package of the hard disk, right? So there's some logic within the hard disk that's going on to do all this. And so, and the third one, which is actually what's used, it's called the scan algorithm or elevator algorithm. And it takes its name from exactly how typically an elevator works, which is that you first, you, you choose a direction. So there are two directions, right? When you're moving radially, either you will move inwards or you will move outwards. So you choose a direction in C4 order. So whoever came first, you choose that direction, just like in an elevator. But if you have chosen a direction, you will service all requests in that direction before changing the direction. It's very much like an elevator, right? So let's say somebody on the top floor presses a button. And after that, somebody in the middle floor presses a button, then, you know, it will serve all, everybody, all the way to the top floor. In fact, let's say there was somebody on top plus one who presses a button with, by the time it reaches top, then it will also serve top plus one before it actually changes direction, right? So that's the idea. And so that, that takes care of both efficiency and is also starvation-free, right? If you've chosen a direction, after that, you're going to go through and serve all those people in that direction. And of course, you're going to come, you're going to change direction sometime, and that's when you're going to, so there's no request that can get served in this case, right? So one way to think about the elevator algorithm is you choose a direction in C4 order. And within a direction, you use shortest, you use SSTF, right? So within a direction, you will choose whoever comes on the way first, right? You won't, so if you have chosen a direction, whoever comes first, you're going to give him. So that, now within a direction, it's SSTF, and choosing a direction is C4. Yes? Sir, wouldn't it be better to just... I'm talking about the radial movement here. Okay. Right? I'm talking about the radial movement. Of course, the rotation just keeps moving. Okay. I'm talking about the radial movement. Even in this case, starvation can happen. Let me write it. So 1, 2, 0. 1, 2, 0. 3, 4. So after 100, it's going to come back to 0, right? Yeah, I mean, it cannot go on forever, basically. Right? In the case of SSTF, it could go on forever. For example, I had 1, 2, 100. 1, 2, 1, 2, and so on. Right? So 100 gets starved forever, basically. But here, there's a bounded time within which it's going to turn back. By starvation, I mean that it's never, you know, there's a possibility, there's a non-zero probability that it'll never get served. There's an unbounded time. You cannot say when. Okay. All right? So good. So those are the storage devices. But by far, you know, in the last 20, 30 years, the magnetic disk has been the primary way of doing storage, secondary storage. And for that reason, most of our operating system abstractions have been designed and optimized for magnetic disk. Right? Tomorrow, if, you know, with some inventions in physics, you get some better storage. It also means that many layers above, in software and in hardware, needs to be changed to better deal with these new devices. Okay? So let's now completely think about magnetic disk from now on. And let's look at how, what kind of abstractions that the operating system provides us. Right? So recall that we have open some file name, which is, let's say, a string, x slash y, and some more, which says, you know, read, write. Right? That gives me a file descriptor. Then you have read on file descriptor, and some buffer, and some size. And you have write. Same thing. And you can have close. Right? So open, read, write, close is the interface that we are very much familiar with. This interface, you know, let's say this was, you know, Unix. So this interface treats a file as a stream of bytes, starting from zero till, you know, some maximum length. And that's it. So there's no structure on the file. Okay? So it's completely unstructured. On the other hand, you know, the other option could have been that files are some, have some structure. So for example, you could have said, oh, my operating system provides files which are a sequence of records, where each record is of size, you know, 64 bytes. And the first field of that record is a natural number. You know, just some, a four-byte number. Right? So databases do this. Right? So databases represent data as some structured data. Okay? So there's some structure. It's a collection of records, and there's a structure on the record. As opposed to that, an operating system has decided that we'll not use any structure on the file. We'll just treat the file as a sequence of bytes. That's all. If somebody wants to implement a structure, he can build, implement a structure on top of this interface. Right? So you can always say that this file, you know, you can always treat this sequence of bytes. You can always interpret a structure on the sequence of bytes. Right? The disadvantage of doing that, the only disadvantage of doing that is that you have two layers of re-direction. Right? So you're implementing a file in a certain way, and then you're implementing records on top of this virtual stream of bytes. Right? So the file system has not, so the number of disk accesses and seek times and notations have not been optimized for the record structure. On the other hand, your application may want to optimize it for the record structure, and that's the reason that a database will not use a file system to implement its logic. Right? The database tries to bypass the file system. The database wants direct access to the disk, and so, and it specifies what's the structured variant that you want. Right? So the only difference between, so that's the reason, so for performance reasons, the database will not use a file system beneath it. The database will directly access the disk and use structured representation right at the disk block level to get better efficiency. But in either case, you can always, you know, for compatibility purposes, run a database over a file system, and in which case, the file will be treated as just the disk, basically. Okay? All right? So let's look at this interface once more. So firstly, there are names. So the file system needs to implement naming. So on Unix, you basically have the special name called slash, which basically means the root directory, and then there's a concept of a directory which contains mapping from names to files. All right? Okay? And so, and you can do this in a hierarchical way. So the root directory, so there'll be a special place on the disk where the root directory is stored, and the root directory will have a mapping from names to files, and then those files themselves could be directories, and so on. Right? So this can be done in a recursive manner. All right? What do I mean by names to files? So what is a file? Well, a file on a disk typically is separate from its name, so it's also called an inode. It represents on disk file. It's a handle to name a file. So let's say this is my disk. I'm representing it as a logical namespace, sector 0, sector 1, and so on. Right? This is some value. And a file is represented by an inode. So inode will point to some location here, which will contain information about the file. This is an on disk inode. So Unix refers to a file by its inode, and not by its path name. Right? A path name translates the name into its inode, and from there on, it's an inode that's important. Right? Inode is the on disk block that contains information about the file. So for example, when you do fd is equal to open some name and some permissions, it will obtain the mapping from this name to this inode, and then store a mapping internally between fd and the corresponding inode number. Right? And then when you call read, then it's just going to look at that inode and do the operation. There's something called an inode, which basically represents the file, and the user cannot see that inode except, you know, there's a system called stat, which allows you to, so let's say stat on fd, which allows you to inspect certain fields of the inode. For example, what's the size of the inode, and what's the status of the inode, whether it's read-only, write, read-write, et cetera, and so on. You can, the structure is completely hidden from the user, except that he can inspect it using stat, which is basically just a read access. It cannot, you know, directly write to it. Writes to the inode by the user can only happen through this interface, which is read-write. So for example, when you write something, then, you know, something needs to be changed on the disk, so the inode will get overwritten. But there's no other way to directly touch the inode by the user. The file itself has, you know, the data of the file will be stored as disk blocks on the disk, right? So let's say this is data, this is data two, and so on, and the inode should say where this data is. So it should have some mapping between offset in a file and the data on the disk, right? So this data are called, you know, disk blocks, and when you say read, you basically, implicitly there's an offset, which basically says this is the offset value, and so the operating system should convert the offset into the corresponding disk block number using the information from the inode to get that data for you, right? So that's basically. So recall that in this interface, when I say read, fd, buff, and size, nowhere do I say a given offset, right? The offset is implicit. When you open a file, you know, depending on what mode you opened it, let's say if you opened it in read-write mode, the offset is initialized to zero. And then as you read or write to the file, the offset is automatically incremented by the data items that you read or wrote. This actually turns out to be an interesting design decision at the interface level. The other option could have been that you, you know, have a fourth parameter here, which says what offset you want to read. And so the user explicitly says, this is the offset I want to maintain, read, and so, you know, if he wants to read sequentially, then it's his responsibility to maintain the offset in its local variable. Let's look at the tradeoff between having an implicit offset as it is done in Unix and having an explicit offset, right? So if you have an implicit offset, then let's say you wanted to implement something like echo1 to x and echo2 to x. You basically want to write to file, you first want to write 1 and then you want to write 2. If you had explicit offsets, then they would end up overwriting each other, right, because each program will not know. Recall that when you fork, you also inherit, when you fork, child inherits the FD table, which includes the offsets. In this case, the parent is going to fork a process, which is going to execute the command echo, echo is an executable, and it's going to write something to the file. When it's going to write something to the file, because the offset was implicit and was shared between the parent and the child, so what will happen is that the parent and child will share the offset. Because the parent and the child will share the offset, if the child writes anything to the file, the offset of the parent automatically increases, right, and so you don't end up overwriting each other, you end up appending to each other's output. The append may be non-deterministic, but in any case, it's not overwriting. If you can do some synchronization of this type, that first you were going to write 1, then by semicolon you basically mean that you're going to wait for the first process to exit before you start on the second process, automatically what will happen is that the first child process incremented the offset, and so the parent's offset also got incremented, and then the second child process saw the updated offset. So you basically see 1 and 2 in the file, right? So it's possible to do this. If you had explicit offsets, then to do this you'll have a more complicated interface, you'll have to explicitly say, you'll have to pass another argument here that at what offset you want to write, and that offset needs to be incremented between two consecutive calls to echo. In general, so you may say, but it sounds very dangerous, if I spawn a child and it's writing, then my offset is getting incremented, and so I may not be expecting it. Well, I mean, if you care about different offsets, then one option is to just reopen the file, right? So when you fork, you basically copy the file descriptor table, which means you have shared offsets, but if you want separate offsets, all you need to do is reopen the file. So implicit offsets, you know, if you want explicit offsets, it's very easy to do it by just reopening, but the other way around is very hard. So implicit offsets seems to be a better interface. So it's a good example of, you know, how interface design can greatly impact your program structure and program elegance, I should say. So this file API, open, read, write, close, actually has turned out to be quite useful. It's not just used for accessing on-disk files. It's also used for accessing things like, you know, pipes, for example. You can call read and write on pipes. We have seen that before. It can be used to access devices. I can call read and write on the console device, and it basically means the same thing, right, or any other device for that matter. And also, it can be overloaded to do things like the slash proc file system that you have seen before, which allows you to look at the kernel state, right? So it's a very useful abstraction that took some time to, you know, get concretized, but has been used in Unix, later Plan 9, and then Linux, and of course, you know, other operating systems today. Okay, now let's look at durability semantics of these calls. By durability, I mean, if I say write, can I be sure that the data is on the disk? Which means if there was a power failure after I called write, or after the write returned, and then, you know, there's a reboot, can I be sure that whatever I wrote is going to be present on the disk, right? So these are, you know, this is part of the semantics that the operating system provides to you. And these are flexible. Unix doesn't specify what semantics you want. It's really up to the operating system designer to decide what semantics is what. And let's look at what are the different semantics that can, that can have, that you can have. So let's say durability semantics. So one is data. So by durability, I mean data will be on disk, or, you know, durable, then one option is when write returns. So an operating system may provide a guarantee that whenever the write returns, you can be sure that the data is on the disk. This is, this is great. It's most conservative, basically saying that I'll always, you know, on every write system call, I'm going to access the disk, which means your buffer cache is basically right through, right? And so if you're doing one byte write, then you basically, it's very slow. So it's, it's very slow. It's most conservative. So it's not, it's not, it doesn't look all that great. The other option is before close returns. So you can say, you know, I don't care if there are multiple writes, but if you close the file, that's the point I'll say that, you know, definitely all the data for this particular file has been made durable. Right? So that's another option. And some network file systems use this, right? Sample AFS. So AFS is a network file system that uses it. The other option is at some point in future. So no guarantees. So let's say, example, within 30 seconds. The write may return, the close may return, but there's no guarantee that the data is actually on disk. The only guarantee is that within some bounded time, let's say 30 seconds, your data will be on disk. But if there's a power failure within 30 seconds, there's no guarantee, right? So this has the highest performance. It's a write-back cache. The operating system's cache replacement, you know, daemon is going to run every, let's say, 30 seconds, and it's going to flush things to disk. But there's no guarantee for the user that, you know, if it writes something, then it's durable. This doesn't seem like a very nice option, right? Because after all, you know, imagine that your operating system was running inside an ATM machine, and you wanted to withdraw some money, and so it deducted some money out of your account, and it said write in your account. So it made the write system call, and it returned, and it also made the close system call. You got the money, and you went away. And what you did was you pulled out the power plug, and so, you know, your update is lost, basically. And so you plug it back again, and you have the money back in your account. So there has to be some way for the user or some way for the application to say, you know, before I give this money, I should be 100% sure that the data is durable, right, basically. And so if you're doing this, then you need some application-level interface, and this interface is called fsync on Unix. It basically says the application can call the fsync system call. So this is, again, a system call on a file descriptor, let's say, and then you say, you know, by the time fsync returns, I'll be 100% sure that the data has been made durable. And so for an ATM, you will first call fsync, wait for fsync to return before you give out the money, right? So these are all sort of options with which the operating system, with which the file system can work. And something like Linux uses this, right, at some time in future with fsync, right, because that has the maximum performance. But when you're doing something like this, you have to be very careful, because if there's a crash, you should ensure that it doesn't corrupt the file system state itself, right? So if you're doing some, so basically you're playing fast and dirty here, right? So you're basically saying, I want to be as fast as possible, but that, you know, that shouldn't mean that if there's a power failure at an inopportune moment, then, you know, all your data has been erased, or your file system has become corrupt. So it needs other ways to make sure that the file system remains intact, and that's one of the most interesting design challenges of a file system, and we're going to look at a few different ways of doing that, and how modern file systems are doing it. Okay, so let's look at how a file is, so file system organization, right? So before we talk about durability, we just got a sense of, you know, here's this problem of durability and semantics that the file system needs to solve. First, let's understand how should the file system get organized, right? So by organized, I mean, I said that a file is represented by an inode, but then how should the data blocks be laid out? So what should be the data structure that you use on the disk to basically store these files, right? So one option is what's called contiguous allocation, so where file A is equal to some base, but the base block is 10, and length, which is, you know, so here's a file which has, which starts at base 10 and has 10 blocks, right? And similarly, B is some base is equal to, let's say, 15, and length is equal to 2, or whatever, right? And so if I draw the disk, then you basically have 10 dot dot 22 is A, and this cannot be 15, right? So it has to be, let's say, 25. They cannot overlap, right? And then you have 25 dot dot 27, which is B. That's contiguous allocation, right? This sounds very similar to how we did segmentation in virtual memory, right? So basically, you know, there's a dual between files and processes and memory and disk, so you basically say that the entire disk is one big segment. This entire file is one big segment, and you find space for it. What is the problem? Fragmentation, right? So, for example, this space cannot be used for a file which needs five blocks or something, right? So one problem is fragmentation. What's the other problem? Growth. If I want to grow the file A by 10 blocks, I have to copy the entire file to some other location, and that's a global operation. That's a very, very expensive operation because I have to, you know, let's say my file is gigabytes long, and I just wanted to append a few bytes to it, and I cannot find space, and I have to actually do, you know, a gigabyte operation to append a few bytes. So that's not very good. What are some good things about this contiguous file allocation? Very fast access. If you want to, you know, so the read system call, for example, only needs to first get the inode. So this information, base and length, will be stored on the inode. So the read system call will get to the inode, get the base pointer, add the offset to it, and then that's it, right? So it's basically just one metadata access to get to any random location. More importantly, sequential reads, which is also a common case, which means, you know, an application is just reading through the file sequentially. That's very fast, right? A sequential file read directly translates to a sequential disk read, and recall that a sequential disk read is great, right? So sequential reads are really fast here, and so this is great for any workload that requires sequential reads, okay? So we are not happy with fragmentation and growth, so the other option is what's called linked list. So, you know, instead of a contiguous array, let's have a linked list, where inode, so inode will point to a file, let's say file A, which basically means the inode storage is going to have a head pointer. Head is equal to 10, and that's it, right? So that's basically head is equal to 10, and then each block will basically say what's my next pointer. So if I have a disk, then let's say this is file A, it's pointing to this location, and this will have some next pointer, which will point to B, and so on, right? And so the storage that's being used for the next pointer will not be exposed to the user, of course, right? That's something hidden from the user. The user cannot see the next pointer, it can only see the data, which is stored in the rest 512 minus 4 bytes. What do you think about this? Firstly, access for a random offset is terribly slow. If I want to say I want to access offset 100, or let's say offset 10,000 of file A, then I have to actually do pointer chasing, and at each pointer chase, I have to actually get the disk in buffer cache, and not just I have to get the disk in buffer cache, I have to wait for it to finish. I cannot overlap computation with disk access, right? I have to wait for it to come before I can actually read its pointer. So basically I have to block, I have to keep blocking. So random access is very slow. Terribly slow. Also, sequential access is also slow, right? Because now sequential access basically means that you have to actually do a lot of seeks because you have it distributed all over. Okay, that's fine. And another important thing that, so apart from efficiency, the other thing you have to worry about in file systems is errors. Disk errors. So for example, some block goes bad. One block goes bad in your file. What does it mean? So in case of linked list, if there's one block that goes bad, the entire file gets corrupted, right? Because you lose all the data. In the case of contiguous allocation, one block goes bad, only that block's worth of data has been lost. All the other data can be recovered, right? But in the linked list allocation, one block goes bad, the whole, potentially the whole file goes bad, right? So you have to worry about that. Okay. And also, when you're doing this, you know, in, okay, so basically, so that's another problem, right? So we looked at contiguous file allocation and linked list file allocation, and both of them seem rather impractical. Contiguous has segmentation problems and growth problems. Linked list has access, efficiency problems. So we're going to look at some more realistic file systems which are some combination of these next time."}