{"text":"So welcome to operating systems lecture 6. In the last lecture, we were looking at how function calls are implemented and in particular we were looking at the stack and we said look so this rectangle that I have drawn here is really the address space of a process let us say and the stack is defined by this register called ESP. So this register ESP which means the stack pointer points to the top of the stack. This address space can be thought of as going as you know going top to bottom which means let us say this is the lowest address 0 and this is let us say the highest address and the highest address in a 32 bit machine will be you know 2 to the power 32 minus 1. So this is the address space and somewhere in the address space there is this pointer called ESP which points to the stack pointer. I can mention the stack goes downwards. So this the stack the point where the stack pointer point is also called the top of the stack. It is a stack in the sense that anything which is below the stack is junk. So it can be overwritten. So safely the program can start pushing data to the stack and does not have to worry about what is below the stack. That is what a stack really means. But everything above the stack is useful and should be handled carefully. It is not your stack in the strictest sense because you can actually do random access to the middle of the stack as we saw last time. For example if I just want to access this argument all I need to do is put an offset to ESP and I will be able to reach this argument. Similarly I can put an offset to EVP and I can still reach this argument. So you know it is a choice of the compiler. Typically he will also maintain another pointer called the EVP which is also called the frame pointer and which points to which is equal to the value of the stack pointer at the entry of the function. And at that point we as a convention compilers like GCC also save the previous EVP that is saved EVP. And so EVP points to the frame pointer and the first value there will be the saved EVP. The plus 4 offset value will be return address and so on and then there will be the arguments. So you can you know access these things using offsetting EVP. And now all this can be my function's local body data for example the local variables that I allocated on the stack. Above these arguments there will be more local variables of my caller. And we also said look I mean this EVP pointing to saved EVP allows us to do backtrace because all I need to do is look at the EVP. From there I will get the saved EVP. So that will give me the frame pointer of my caller and so on. And from each of these I can also get the return address so I can get the whole call chain. Till for example I reach the top most function let's say main or something. All right. Okay. Then we said you know let's look at the workflow or the tool chain that allows us to build our program. So we let's say we write our programs in a high level language like C. There's a compiler which converts it into a .s file. What is a .s file? It's an assembly file which has all the instructions in assembly format or in assembly syntax. Assembly syntax is just a human readable representation of machine instructions. So this is a human readable representation of machine instructions. An assembler converts human readable representation into a machine readable representation. So it converts it into actual binary format which a machine can read when it executes. Then the linker links multiple .o files to make one a.out file. For example you can call printf in your function in your file in your .c file. But printf doesn't necessarily need to be defined by you. So printf could have been written by somebody else. But if you link all these things automatically things get packed so that the right printf gets called. The a.out file then gets loaded. So there's a loader which loads the a.out file. So a.out file has to be in a certain format for it to get loaded. And then it becomes a running process. So we said the loader is typically implemented in the operating system. For example when you make the exec system call, the operating system's loader comes into action. It reads and parses the a.out file, pastes it into the process's address space. So let's say this is the address space of the process. It looks at the a.out file and the loader is going to paste the contents of the a.out file into the address space. For example it will paste the code into the address space. It will post all the initialized values of the global variables into the address space. And it will initialize a stack. So it will allocate some space for the stack and it will initialize the value of ESP. And so a process gets created and now initializes the value of EIP with the first instruction that needs to get executed, which is also this information is also encoded in the executable a.out. And so now the process starts running. From a.out it got the code and it got let's say the global data, the global variables of your program. And all the local variables will now get allocated on stack, let's say. And then it also initializes the heap. So you've heard of the stack and the heap before in other courses? So the address space is let's say divided into code, data, stack, and heap. I mean the layout may not be necessarily in this order but roughly speaking these are the four different parts of the address space. The code is basically the code that gets to run and as we said even code lives in memory or code lives in the address space of a process. And code is obtained from the executable a.out. Similarly data refers to the global data of the executable and once again data is again obtained from or initialized from a.out. The stack is initialized to some empty space. And so some empty space is allocated in the address space and the stack pointer is initialized to point into the stack so that now when the program runs it can actually start pushing and popping from the stack. And then there's this thing called heap. Heap is basically all the other space that the program may need to allocate. So for example if you want to allocate a data structure like let's say a link list or a binary search tree or something then you would use neither stack nor global data are the right places to do it because stack grows only in a certain way, data is a fixed size. You want variable sized dynamic on-demand creation of memory. That's what's called heap. So heap is basically an area in the address space from where the program can request memory using calls like malloc or free memory using calls like free. So that's what operates on the heap. So the code and the data are initialized by a.out and the stack and the heap are just initialized to empty address spaces with appropriate pointers pointing to the right things. And we're going to look at how malloc etc. get implemented later on. But let's just look at the stack for now and understand that. So that's the address space of a process and I haven't yet talked about how the address space actually gets implemented. We have so far been assuming that every process has a private address space and we have talked about how the address space gets initialized by the loader and how it gets used etc. But how does the address space get created? That's what we're going to talk about in the next couple of lectures. But before we do that, let's look at the physical address space that's available to the OS. So what is available to the OS? So let's draw the address space which I'll call the physical address space which starts at 0. And it potentially goes all the way up to 2 to the power 32 minus 1. And let's look at a real world setting and let's see how does the physical address space look to the OS when the OS boots. So when the OS boots, he sees some memory and he can use addresses to access that memory and that's what we are looking at. So what addresses refer to what parts of the memory? So there's something called low memory. There's 640 KB. Then there is BIOS slash devices. Right, not necessarily in this order, but let's forget about exact numbers. But let's just say what all exists. So there's BIOS and devices. Then there is VGA memory. VGA memory is special memory. If the OS writes to that memory, it actually goes to your display. So let's say there is some area of memory which is basically called VGA memory. And whatever the OS writes to this area of memory, it actually goes to the display. So it's actually mapped into the display. It's not real memory. And then there is what's called extended memory. And the size of this is basically dictated by the size of your RAM. So when you say I bought a 1 GB machine, so the size of the extended memory is basically 1 GB. And then at the, so let's say the extended memory is to this point, then everything above it, the address space is unused, which means that if the OS says I want this address, it's going to get an error. The hardware is going to generate some kind of an error signal. It's going to say error interrupt. And that's what you commonly see as a bus error. If you have ever seen the bus error in your program, the bus error basically means that an address was generated which does not have a mapping in the physical address space. And then somewhere on the top, there are what's called memory map devices. And what these are are basically, these are addresses that are backed not by physical RAM but by devices. You can think of some device. And the idea is that when you write to this device, to this memory address, it actually goes to a device. It goes as a command to the device. And if you read from that memory address, it actually becomes a read from the device. So the device will, for example, have certain registers. So let's take, for example, the printer. So let's say the printer has certain registers which say start a command, stop a command, et cetera. And these registers are actually mapped in the physical address space. So I'm just taking an example, but the exact mappings will be determined by some kind of a manual which the printer manufacturer gives you. But basically, the way it will work is that the registers are mapped in the physical address space. And the OS can read or write to this physical address space to send or receive commands and data to and from the device. All right? So this is part of the physical address space, but it does not necessarily behave like memory. What are the semantics of memory? That if I write something to address x and then I later read something from address x with no intermediate write to the same address, then I'm going to get the same value. But this address space does not necessarily follow those semantics. So in other words, writes and reads to these addresses may have side effects. Okay? So they don't necessarily behave like memory, but they share the physical address space. Okay. So that's one way of accessing devices. Another way of accessing devices are what's called using IO space. So x86 also has special addresses, a separate space for IO, which is called the IO space, which is accessed using special instructions like in and out. So last time I said that there's special instructions for IO called in and out. So let's see how they work in action, right? So the way it works is the IO space is actually made of 1024 addresses. So it's a pretty small space, actually, compared to the 32-bit space that we had for memory, right? So it's only 10-bit space for the IO. And these addresses are also called ports, IO ports, right? And I can use commands like in from a port to, let's say, a register. So let's say the register is AX. So let's say I want to read a 16-bit value from this port to register AX. This is the instruction to do that. So once again, as we saw in the memory map case, there were certain registers of the devices that are mapped to certain addresses of the physical memory. Here, certain registers of the devices are mapped to certain addresses of the IO space, right? The difference being that in that case, I could just do read and write to that location, or in fact, I could execute any instruction on that memory address, like increment, decrement, subtract, add, move, et cetera. In this space, I can only use these special instructions called in and out, right? So in basically says read a value, read a byte, or read, in this case, read, let's say, a suffix with W. In this case, it says read two bytes from this port to this register, AX, right? Or I could say out W, percentage AX, some port, which basically says write these two bytes from AX to this port. Okay, so this is a way to access devices. And let's look at a real example of how something like this works. So we have seen two ways of accessing devices. One is using the physical address space of memory, and the second is using the IO space, which we also call ports. All right. So let's look at, let's say, the printer. So, you know, I'll give you some example code, or some pseudocode of a printer driver, which basically wants to put a character, put C, to a printer, LPT, right? Line printer, let's say. So I want to put a character to the line printer. And let's say, I want to implement a function in C, which puts this character C to the printer. All right. And let's see how something like this could be implemented. So let's say while in B, status port. All right. What am I doing here? In B is another function, which internally is just executing this instruction called in B. All right. On this port called status port. So let's say, you know, status port has been defined here. Some value, let's say. It's some number less than 1024. And the printer manual says that this is the port on which I will basically be accessible to you. Right. And so now the software can say, make, can execute the instruction in B on this particular port, and get the data on that byte in a particular register. And then, you know, it can execute its regular code to check the value. So in this case, I'm actually, you know, I'm writing C code, but you can, you can, you know, imagine converting this to assembly, where you just read, you execute the instruction on that port, you get the, you get the byte, and then you compare the byte with this particular value busy. Right. Let's say busy is also some, some number. And, and you, and you and it. And till it's busy, you keep spinning. Right. That's what it's saying. So it's saying, till the status says I'm busy, keep spinning. So it's, it keeps spinning till the status is busy. Right. As soon as the status becomes not busy, it's going to break out of the loop. All right. Okay. Then, at this point, I know that the printer is not busy. And I can, let's say, send character C to the data port. Right. I'm writing these as functions, but these functions are basically just single instruction functions that just execute the out B instruction. Right. And then I put a data value on the data port. And then I may need to tell the printer that look, now sample the data. Right. So I put some value on the data port. And now I need to, let's say, tell the printer that, you know, please pick up this data. I put the data in your register. Now pick up the data. And let's say the printer was edge triggered, which means, you know, there is some strobe signal in the printer, which picks up the data on every edge or every transition of the strobe signal. So let's say I do that by saying, by putting, so there's a control port, let's say. So let's say the value of strobe was one. So I put one to the control port. And then I put zero to the control port. What this does is it simulates an edge on the control port. And that's when the printer is supposed to pick up the data from the data port. Right. So an example of how, let's say, a driver could be implemented. First, you check the status port, waited for it to become free. Then you load some data to the data port register. And then you strobe the control port for the printer to pick up that data. Right. And now the printer may say, I'm going to be busy for the next few milliseconds or whatever. Let's say it's printing that data. In which case, if there's an extra character that needs to get printed, it's going to now, the CPU is supposed to wait till the printer is actually ready to take data, take the next byte of data. Now, clearly, let's say there's just one printer. And so the operating system should be the only one that's actually executing code like this. Right. It cannot be like, a process cannot be executing code like this. Right. Because you can imagine if multiple processes try to do the same thing, then there'll be chaos. Right. Because both of them are going to see the printer is free and both of them are going to try to write to it. And so the printer state machine is going to get confused. So there's going to be, there has to be some organization such that the printer state machine gets followed properly. And one way to do that is to use the OS as an intermediary. So the process is not going to access the printer directly. Process is going to use system calls, like read and write, and the OS is going to convert those system calls into code like this. Right. Also notice that this kind of code involves this while loop. While the CPU is executing this while loop, the CPU is essentially busy. Right. I'm basically just checking the status port to see if the status, if the printer got free. A more efficient use of my resource could have been that I use the CPU if, you know, the status port is, it's busy, I figure that out and I start executing something else. And there's some way for the printer to let me know that the status port is now free. And so that's when I start executing the rest of the logic. Right. That would have been the more efficient way of doing things. Right. In some situations. So this is called polling. Polling means I keep checking. Perhaps at periodic intervals, whether the condition I'm looking for has been, has become true or not. Right. As opposed to the other approach, which I, which I just described, which is called interrupt based approach. In this case, I tell the printer, look, interrupt me when you become free. Right. And so, and now I start doing something else. And now when the interrupt becomes, when the printer actually becomes free, it interrupts me and I check again whether it's actually free and then I execute the same logic. Busy is a constant. We're just checking if a certain bit in this value status port, in the value returned by status port has been set or not. Right. You know, for simplicity, you could even ignore this and say, you know, let's say the, the specification says that the status port is going to be either fully zero or fully one, depending on whether it's busy or not. I was actually just so busy is just a way of saying that I'm checking one bit as opposed to the entire byte. Okay. All right. So polling versus interrupt. These, these are always two options in system design. Right. So one entity wants to access another entity, whether I should keep checking whether that entity is ready to receive my message or whether I should use an interrupt based mechanism. And it's not necessarily that one is better than the other. Interrupt comes with its own cost, right? Because if I set up an interrupt on the, on the printer, the printer is going to in future invoke an interrupt handler, and there'll be some cost associated with executing that interrupt handler. For example, if I expected that the status port will become free very soon, you know, let's say in the next 20 to 100 iterations, I'm very likely to find the status port to become free and actually cheaper to do polling. All right. On the other hand, if I know that the status port is going to be free after a long time, let's say 100,000 cycles or a million cycles, then it makes sense to do interrupts. All right. So it's really a choice that an OS has to make depending on the device characteristics and workload characteristics, whether to use one or the other. And we're going to see more examples of this, of this design choice later when we talk about real examples. Okay. All right. So now let's talk about process address spaces. So, so far we have looked at physical address space and IO address space, right? Now let's look at how an OS implements process address space. So once again, what are the requirements for a process address space? Firstly, it should be private, which means nobody else should be ever able to access it. Right. Second, it should be protected, which means a process should never be able to step out of its address space. So it should not be able to access anything outside that. So let's just work with those two sort of requirements. And, and one way to implement address spaces is what's called segmentation. Right. So how does, what is segmentation? So we saw when, when we were discussing the x86 architecture 16-bit, we said there are these special registers called segment registers, CS, DS, SS, ES, right? So they were basically being used to add, so, so there's an MMU. Here's an address that the, the program gives you. And here's the address that actually goes out on the wires to the memory, right? So let's get, say that the address that the program gives you is a virtual address. And the, and the value that actually goes out on the wire physically is called the physical address. In the 8086 machine that we saw, this MMU was doing a simple addition. So, you know, let's say this is VA and this is PA. Then on the 8086 PA was just equal to VA plus segment into 16, right? And that's really not enough for you to be able to provide any kind of protection or anything of that sort. It was just a, it was just a matter of convenience that, you know, you can access more memory with less number of virtual addresses. But now we want the MMU to be smarter in the sense that the virtual address of a process should get translated to a physical address such that it only accesses a memory that it's allowed to access. It never steps over other person's memory and so on, right? So let's look at how such an MMU is implemented. And now, and what I'm going to look at is segmentation on 32-bit x86. So this was 16-bit x86, 8086, which we saw. And now let's look at how processes are implemented, how segmentation works on 32-bit x86. When 16-bit processors like 8086 were designed, at that time, multiprocessing or the ability to have multiple processes which do not trust each other was not that important because, you know, workloads hadn't evolved to that stage at that time. So at that time, they just used the simple segmentation hardware to be able to do MMU. But by the time 32-bit, very soon, you know, multiprocessing was needed. And so they needed more hardware in the MMU to be able to do that. All right. So let's look at 32-bit x86 segmentation. So once again, a virtual address is a pair of segment selector and an offset. For example, ds colon 1234, right, or ds 4567 and so on, right. So that's how a virtual address is specified. It's specified using what's called a segment selector or the name of a segment register like csds, et cetera, and an offset, which is a 32-bit offset now in the 32-bit world. And once again, in the 16-bit world, it was very simple. I just used to multiply this by 14 and add to it. But now we're going to do something more complicated because we want to provide protection. All right. So the selector, let's say cs, is a 16-bit register of which the top 13 bits are used to index into a table, which we call the descriptor table. Let's say it's a global descriptor table into an entry which contains things like base, limit, permissions. What's happening? If the application specifies cs colon 0x1234, I am going to look at the cs register. I am going to take the top 13 bits of the cs register, use it to index this table, which we call the global descriptor table, to get an entry. This entry will have values which say base, limit, permissions, among others. And now what the hardware is going to do is it's going to add the offset to the base before comparing it with limit. So first it will compare the offset with limit to see whether this offset is allowed within this segment or not, and then add it with the base. And that's what the physical address is going to become. So the physical address will be offset, coming from here, plus base, assuming that it was less than limit. All right. So let's see. Here is the global descriptor table. Let's say I call it gdt. Here are segment registers, cs, ds, ss, es. And actually 32-bit has six segment registers, fs and gs. These are all 16-bit wide. The gdt itself has 2 to the power 13 entries. The top 13 entries, the top 13 bits of each of these, points into the gdt and tells you which entry to use to do the translation, right? So let's say there's an offset that's coming from the user, and there's a segment selector. So this is a selector. The selector, the top 13 bits, tells you which entry to look here, look at here. And from here, I'm going to get numbers like base, limit, and permission. But let's just say base and limit for now. What I'm going to do is, I'm going to say PA is equal to VA plus base, where VA is this offset, but also assert that VA is less than limit. It's giving you more than what the 16-bit does, and the thing that's giving more is that it's also asserting, it's also checking that it's not overflowing its permitted limit. It asserts on VA, just a matter of convention. Just a matter of convention. All right. Okay. What am I going to do? You know, if I wanted to implement address spaces, one way I could implement address spaces is, I would set up the segment registers in such a way that they point to memory regions, which are allocated for that process. And I will set up limits in such a way, such that the process is never able to access anything beyond what I've allocated for him. So for example, what I could do is, let's say this is the physical address space, and let's say this is 0, this is BIOS, and other stuff. But here's where the actual memory starts, which is usable memory. Right? So from here, I said, okay, this region, I want to give to process P1. Right? And this region, I want to give to process P2. And this region, I want to keep for myself. What I'm going to do is I'm going to look at, let's say this was base B0, this was address B1, this was address B2, and this was address B3. When I run, when I schedule process P2, I'm going to set up the segment registers in such a way that, let's say I want to run process P1, so segment registers will be set up in such a way that base will be equal to B2, and limit will be equal to B3 minus B2. And so when I run this process P1, when it uses a segment selector, it can only access this area of memory. Right? So the process itself will use addresses like 0, 1, 2, 3, or, you know, any other address. The legal addresses for a process in this case would be 0 to B3 minus B2. Right? It can use any of these addresses. 0 gets translated to B2, B3 minus B2 gets translated to B3. Anything in the middle just gets translated to B2 plus BA. Right? So the process will just use 0 till some limit, and the OS is going, the hardware is going to convert it to the appropriate physical address. Assuming certain things, a process will never be able to access anything outside this region. Right? So that's how I'm going to implement address space. Notice that in doing so, a process does not have to worry about where it was placed in physical memory. Right? The executable can assume that it's starting from address 0, or the address space is starting from address 0, and it has some limit to it. Right? So in the compiler, linker, loader, or the compiler, you know, assembler and linker don't have to be worried about exactly where the process will be placed at runtime. Right? The canonical, there's some convention that the operating system is following. In this case, the operating system is following that the address starts at 0 and goes up to some maximum limit, and so the program safely can be put at address 0, let's say, irrespective of where it gets put in the physical memory later on. Okay. So how do we know, at load time, how much memory to allocate for that process? In some sense, we are limiting the space that the process may have. Right? That's a great question. Firstly, we know that when the OS loads an executable, it knows a sense of how many bytes the code takes, and how many bytes the data takes. It also has some default values of how much space to allocate for the stack. Right? And then it will have, you know, let's say in this scenario, I could also say, here's my default value of, here's the maximum value that you can allocate for the heap. Right? And so the operating system is forced to make this pre-allocation upfront, at load time. The process actually may be a very short process and may not allocate any memory at all, in which case I just over-allocate it. Or the process may actually need a lot more memory than what I allocated, in which case I under-allocate it. And in both cases, I'll have to tell the process using some kind of signal, like segmentation for something that you are trying. So in the first case, there's no problem. In the second case, you'll basically tell the process that, look, you're doing something illegal, not allowed. Right? But of course, you know, I'm giving you a first flavor of how address spaces can be implemented. There are more advanced ways in which address spaces are implemented and that allow better, more dynamic allocation of space to processes. All right. So let's just look at this for the moment. All right. Okay. All right. So, okay. Plus, actually, here I've drawn different processes as different address spaces. By the way, if I wanted some space to be allocated only for the OS, the OS could say, you know, this space will never be marked as base and limit for any process, but will be used as base and limit for its own functioning, et cetera. All right. Here I'm saying that each process must, so one limitation of doing this base and limit way of doing MMU is that a process needs to be contiguous. Right? One could relax that requirement by saying that, look, there are six segments. So I should ideally allow up to six different contiguous segments. Right? So an OS could be more complex and say, oh, I'm going to have, you know, P1 CS here and P1 DS here or something. Right? So that way a process is allowed up to six different contiguous regions. And so in that way, he can have more dynamic allocation depending on what it does. For example, something like the code segment needs to be allocated statically in most cases. So it can just get allocated statically and other things like stack can be allocated on demand or heap can be allocated on demand. Okay. But more interestingly, let's understand how an OS can actually set up these segment selectors, but not allow a process to be able to manipulate these segment selectors. So the hardware has to make some, give some hardware support to the process so that the process is not able to overwrite the segment selector. Right? Or the process is not able to overwrite the global descriptor table and manipulate the base and limit. Right? Okay. So firstly, how is the GDT computed? So there's another register on the hardware, on the chip, which is called the GDTR, global descriptor table register, which points to the base of the GDT in memory. So the GDT itself is saved in memory. Right? And so GDTR points to the GDT in memory. And what the hardware is going to do is it's going to take the selector, take the 13 bits, index the pointer at GDTR, add the 13 bits to GDTR, and multiply them by whatever the size of an entry is, and then use that to get base and limit. Right? Okay. What if there are more than 2 to the power 13 processes? It's, it's okay. Right? So I'm going to discuss how exactly it's implemented. What about the remaining three bits? Yeah, let's just hold on and we're going to discuss how they're used. All right. So firstly, the GDTR itself should not be manipulatable by the process. So process shouldn't be able to set a value of GDTR, because if the process is able to set the value of GDTR, then he can pretty much do anything he wants. Right? So to be able to do that, firstly, the processor must support two modes, privileged and unprivileged. Privileged mode is the god mode, can do anything. Right? If you're running in the privileged mode, you can do pretty much anything. Unprivileged mode is the peasant mode, where, you know, the god tells you exactly what you're allowed to do. Right? Now the process runs in the peasant mode, and the operating system itself runs in the god mode. Right? The hardware has certain restrictions on what instructions can be executed in god mode and cannot be executed in. There are certain instructions that can be executed in god mode, but cannot be executed in peasant mode. And the instruction that sets the GDTR, load GDT, is an example of one such instruction. Right? So the LGDT instruction, which actually loads the GDTR, can only be run in god mode, not in the peasant mode. So you're going to, as you can imagine, you're going to run the process in the peasant mode, and because if the process is running in the peasant mode, the process, even if it executes the LGDT instruction, what's going to happen? It's going to create an exception. Right? Just like a segmentation fault is going to create an exception, the operating system is going to get notified, and it can, for example, kill the process, saying that you're trying to do something illegal. On the other hand, the operating system is free to execute this instruction. That's number one. Right? So the GDT can only be set by the OS, and not by the process. Okay. Second, the segment registers, could they be allowed to be overwritten by the process? Well, one answer is no, but actually, on x86, a process is allowed to overwrite its own segment registers. Right? For example, the compiler may be using things like, I want to shift my code segment somewhere. So the architecture designers didn't want that a user shouldn't be able to set up its own segment registers. The segment registers are used for more purposes apart from this protection. So a user is allowed to overwrite the segment registers, but then what protects it? What disallows it from doing something wrong? It can only set up these registers to one of the allowed values in the GDT. All right? So what an OS can do is that it can engineer the GDT in such a way that only certain values, certain base and limit values are present in the GDT. And so even if the program changes its selectors, it can only change it to one of the allowed values. Right? So if there is no base and limit for the OS address space in the GDT, then it'll never be able to change to that. It'll never be able to access the OS's address space. Right? Okay. We can still change the segment to the other process's base and limit. No, even that is easy, right? Before I context switch to this process, I'm going to switch the GDT such that it only contains entries of that particular process. Right? So at the context switch time, before I start this process running, I will remove the GDT of the previous process, or I will modify the GDT such that the previous process's entries are not there, and the new process's entries are there, and only the new process's entries are there. So the new process is allowed to freely change his segment registers between these new entries, but he's not able to access any other processor's memory or its own memory. Or the OS's own memory. Right? Convincing that this is an okay way of ensuring that applications have private address spaces and are not able to write on another application's address space and are not able to write on the OS's address space. Some more facts. How does the processor know what is the current privilege level? The way an x86 processor decides what the current privilege level is by the last two bits of the CS register. So the last two bits of the CS register tell him, tell the processor, whether I'm working as God or whether I'm working as present. So the convention is that zero, so there are two bits, so you have four possible values, four possible values, zero, one, two, three, right? And three is the least privileged, and zero is the most privileged. Most operating systems just use zero and three, don't use one and two at all. Right? So the operating system, when the operating system is running, the last two bits of CS are set to zero, and then when the application is running, the last two bits of CS are set to three. Right? So zero and three basically decide whether I'm running in this mode or that mode. All right? Okay. Interesting. So great question. But a process can modify its CS, so if it modifies its CS, then it can, you know, change its privilege level at will. So the hardware also makes sure that only certain types of modifications are allowed. Okay? So when you modify the CS, you cannot go from a less privileged mode to a more privileged mode. You can go in the other direction, for example. And in some cases, which we need for implementing system calls, which we're going to discuss later, but let's just assume for now that a process cannot in general just upgrade its privilege level by modifying CS. Right? If it tries to do that, so you can change the CS, and the way to change the CS, by the way, how can you change DS? So there are instructions. How can you change segment registers? There are instructions like move a register like AX to the segment register. So this is a valid instruction. A process is free to execute this instruction to change a segment register. But CS is an exception. You cannot just execute this instruction on CS, just like you cannot change EIP directly. You cannot change CS directly. So the way to change the CS is basically what's called an LJUMP instruction. Right? And LJUMP basically takes arguments selector and offset. So we've seen the JUMP instruction before, where we just say jump to an address. It just changes EIP to that address. LJUMP takes two arguments, selector and offset, and the semantics are that it will set the selector into CS and set the offset into EIP. Right? That's the semantics of LJUMP. And when it executes the LJUMP instruction, only certain values of offset are permissible. So for example, if I am executing at privilege level 3 and I execute LJUMP to a selector which has privilege level 0 in it, that's not allowed. It will cause in the hardware to generate an exception and the OS will come into action. Right? So only certain values are allowed. So basically you are not allowed to go from a less privileged mode to a more privileged. Okay? Finally, what prevents an application from just modifying the GDT? Why? So GDT lives on memory, right? Why can't an application just say, just figure out the address of GDT and just write something there? One answer is you require a privilege instruction to do that. Is that right? You only need a memory move instruction, right? Just some move instruction is going to suffice to be able to write to a memory address. So the GDT is stored in the region of memory which is only accessible to the OS and not to anybody else. So when I have this figure and I say this is a physical memory, so GDT is probably going to live somewhere here. Right? And because this area is not mapped into the address space of any other process, no process can actually modify GDT. Right? So we looked at protection. We saw that a process has different privilege levels. Let's say there are two privilege levels at least, unprivileged and privileged. Certain instructions can only be executed in privilege mode. The loading of GDT is one such privilege instruction. Then we said that the segment registers are free to be changed by the application, but they can only be changed to certain values. And those values are dictated by the values in the GDT. All right? And you cannot just lower your privilege level or upgrade your privilege level or lower your, you know, so lower your ring level. So these 0, 1, 2, 3 are also called ring levels. Right? Lowering a ring level means upgrading your privilege level. So an application can't just lower its ring level. And so that's another thing. And finally, because GDT is a protected structure, a protected structure must live in a section of physical memory that's not mapped in the address space of any other process's memory and only in the address space of the OS memory. Okay. Let's stop here. And we're going to continue this discussion next time."}