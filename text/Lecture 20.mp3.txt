{"text":"Alright, so welcome to operating systems lecture 20. So we have been looking at how a process is implemented. A process has an address space. In xv6, it also has a kstack. So there is a stack per process, which is a kernel stack per process. Every process has a separate stack on the kernel and every process behaves as a thread inside the kernel, right, because there is a shared address space inside the kernel for every process. While the process is executing in the kernel side, then there is something called a trap frame that gets pushed right at the entry time, right, and that's the trap frame. The trap frame basically contains all the values of the user registers at the time of the trap and can be used to do different things. For example, it can be used to implement system call arguments and system call return values, right. And then we said there is this structure called context, which is also stored at the stack. It's really at the bottom of the stack, right. For a switched out thread, the context structure is at the bottom of the stack and basically stores the values of the kernel registers at the time the switch was called, right, so including the EIP, right. So when you switch to this thread, you are going to reload those registers and reload the EIP by calling the return instruction and you continue from where you left, right. So basically there is some function which we will call the switch function and, you know, whoever calls the switch function is not going to, the switch function is not going to return back to that function. Switch function is going to return to some other function, right. So the switch function was something very special and typically the way it works is that let's say this is switch, right. So this is one thread that calls it and, but return doesn't happen on that thread. Return happens somewhere else, right. So let me draw it like this, right. And at some point later, this thread is going to call switch again and return is going to happen here, right. So this is a special function where, you know, the caller doesn't get to see the return, right. Okay. And that's how you implement the context switch, right. And we said the context switch is basically the core of the kernel because it does all the switching logic. Everything in the kernel is either a process or a thread, right. So if I were to say whatever, you know, if I were to look at the kernel, let's say you have these multiple threads. Some of these threads are backed by address spaces. These are called processes, right. There could be other threads that are not backed by processes. They're just doing kernel functionality, right. So let's say there's some thread that's just checking if everything is okay and has nothing to do with any process or anything of that sort. So it can just be a kernel thread without any address space of its own, right. The only thing it has is the kernel address space, right. So recall that every process has a kernel address space and a user address space. So these rectangles there are showing the user address spaces and this one doesn't have a user address space. It's only a kernel thread. So one such example of a kernel thread is the scheduler thread, right. So this is a thread, for example, that was started right in the beginning. So when you booted the program, booted the kernel, you started a stack and you initialized the stack and you started executing some piece of code on that and then eventually you call the scheduler on that stack, right. And so when the scheduler gets to run, he's running on this stack and then what the scheduler will do is it will look at this array of processes and pick up one that is runnable and just switch using the switch function to that thread, right. That thread is going to run and let's say that thread was a process, so then the process is going to run because it's going to do either trap it and it's going to get back to user mode. That process is going to get to run for some time. It's possible that the process says, okay, I don't want to run anymore, in which case it makes a system call called yield, let's say. It makes a system call called yield. Yield internally also called switch, right. So yield will call switch to switch to the scheduler, right. And then the scheduler will again get to run. Before it calls switch to switch to the scheduler, it will change its state from running to runnable, right. Now scheduler gets to run and it picks up one of the runnable processes. It's possible that the same process gets picked again, all right, or it's possible, whatever the scheduling policy is. If it's round robin, let's say, then you know some other process will get to run if there were other processes in the system, right. So then, so basically the chain of control is scheduler, P2, scheduler, P1, scheduler, P2, and so on, right, whatever. So you switch the stack from P2's case stack to the scheduler's case stack and then you switch from scheduler's case stack to P1's case stack and so on. Sir? Yes. Sir, if the switch is happening on the yield... If the switch is happening on the yield system call, are the interrupts disabled? Well, you know, it's up to the operating system whether it wants to disable interrupts on a system call or not, right. In general an operating system will not disable interrupts on a system call, right. So yield is just like any other system call, so it's not going to disable interrupts at that point. But if there is an external interrupt while the yield was executing, then the handler of that external interrupt will execute with interrupts disabled, right. So we said that all external interrupt handlers must execute with interrupts disabled to have a bound on the case stack, okay. Sir, so there can be... Okay, all right. So the other way for a context switch to happen is that you know the previous example was P2 called a system call called yield. So it's possible that P2 never called a system call called yield which is you know usually when you write your program, you never call yield really, right. You just expect the program to run and still be friendly with all the other programs in the system. So the way it happens is let's say you are executing in P2, a timer interrupt occurs. The timer interrupt handler gets called. The timer interrupt handler internally will probably call the same function that the yield system call would have called, right and which will internally call the switch function and it will do the exact same thing as the yield system call, right. The difference, the only difference between an external interrupt and a system call is that the external interrupts execute, the handler of the external interrupts executes with interrupts disabled and the system call may or may not execute with interrupts disabled, okay. It's possible that while you are executing a system call handler and the system call could be any system call including the yield system call, a timer interrupt occurs or any other interrupt occurs and so what will happen is you will continue pushing the trap frame on the kernel stack. So the kernel stack could have two trap frames, right and that's perfectly possible. Could the kernel stack have two contexts? No, it cannot have two contexts because by the time you push the context, you have already switched, all right and you also ensure which we are going to see later on that while you are pushing the context, the interrupts are disabled. So you know it's not possible that you push two registers of the context and then an interrupt occurs and so you know half the context is there and you know another context is not possible because the kernel just carefully disables interrupts while it's context switching and as soon as there is a context switch, it can just you know re-enable interrupts or revert to the previous value of the interrupt flag. Whether it was enabled or disabled depends on you know how you came there, all right, okay. Good, now today I am going to talk about how you handle, so I said arguments are passed to syscall, right, so syscall arguments and one of the big reasons we had this organization of an OS where there is a kernel and the users sharing the same address space is because user and kernel can communicate very fast, they can just exchange a pointer and the user can dereference that pointer because we are in the same address space, right, so that was a big deal and what we said was let's say I have a system call called exec and it takes a string, right, now a string can be of arbitrary length, so clearly a string cannot be passed into registers or even you know on passing the string on the stack may not be the right thing because you know stack is you know, right, so you could pass it potentially on the stack, but let's say you know you cannot pass it on the registers, so one way, so the way it is done is you pass the pointer, so you, what the user does is it initializes the string somewhere, the string could be either on the user's stack or it could be in the user's heap or it can be in the user's data section, it doesn't matter, it is in the user's address space, that's all I care about, right, so the string is in the user's address space and what I give to the kernel is a pointer to this string, an address, right and the kernel gets this address and the kernel is supposed to dereference this pointer to get the value of the string, okay. Now what can happen? Well, what can happen is if the user is not trusted which it is not in general, right, what can happen is the user can give a pointer which is let's say a pointer in the kernel address space, right and so it can ask, the user can ask the kernel to dereference that pointer for itself, right, so let me give you a concrete example, let's say I wanted to call the write system call, right, so write takes a file descriptor and a buffer and a size, let's say, okay, so once again the buffer is a string, I basically communicate it using a pointer, so what do I tell the kernel? I say here is the address and just take, you know, size bytes from this address and put it in the file, right, now if I was a malicious user, what I can do is this pointer can be a pointer in the kernel address space, right and the kernel is just going to just dereference it, right, so if the kernel just dereferences it, even if the pointer was in the kernel address space, because the kernel was executing in privileged mode, it will be able to dereference it and it will be able to read all those values and it will now write all those values into the file, right, so what can the user do? It can actually read the entire kernel, right, yes, question, so I am just saying that if the kernel was to just dereference buff without doing any checks, then there is a problem, do you agree? Yes, so if the kernel executes starbuff and puts it into file descriptor fd, then the user can potentially read the entire kernel, right, he just needs to craft his pointer in such a way that, you know, it looks at different addresses of the kernel and I can look at the entire code and entire data of the kernel, I can look at the data of other processes, right, because after all, all the other processes are mapped in the kernel space also, including myself and so I can look at the data of other processes, right. Similarly, in fact, so this is just, you know, this is a way of reading the kernel, I could actually crash the kernel, so in fact, the read system call can cause, can allow a process to write any arbitrary data to any arbitrary pointer, so here what is going to happen? Kernel is going to execute, starbuff gets whatever the file contents are at the current offset, right and so I can, what the kernel, what the user can ask the kernel to do, it can just say, oh, change this memory location to something else, right and it can ask him to do whatever he likes for that matter, okay, so this is a security flaw, right, so these are, this is, so what this buff or any other pointer that is passed as an argument to a system call by the semantics is called a user pointer, okay. A user pointer is supposed to be untrusted in the kernel, a kernel should never trust a user pointer, it should always check the validity of a user pointer before ever dereferencing it, okay, so what can, what kind of checks can the kernel make, well one thing is buff should never be above current base, buff should always be a value below current base, is that enough, right, so it should be an address that is actually mapped in my address space, right because if it is not mapped then what I can do, what the user can do is ask the kernel to dereference a location which will cause a page fault, right and the kernel may not be capable of handling page faults while it is executing, right, a kernel may be capable of handling page faults while the user is executing but the kernel may not be able to handle its own page faults, right or kernel does not expect itself to have page fault, right and so that can cause problems, right, so basically the two things you have to check, you have to check that the address is mapped and that the address is mapped with user privileges, not with kernel privileges, in other words the user himself should be able to read or write to that address, in other words user is asking the kernel to read or write to that address but the kernel should only do that if the user himself is allowed to read or write to that address, okay and so that may involve for example checking that you know in our x86 organization it may involve checking that buff is less than current base 2 GB and buff when you walk the page table directory, the page table of the process then buff is mapped and buff is mapped in user privileges and you know if it is a read system call then it should be writable, right, so if a process is doing different privileges for different areas for one page is writable and another page is readable then you should only be able to call the read system call on a writable page, not on a readable page, so it can do all these checks, okay, all right. So in fact if you look at x86, let's say this is the address space, okay, this is current base and this is current base, you know there is some kernel data here, I am just going to call it K and what x86 organizes its address space is that all P1 space should be from zero to some size. So that's you know x86 takes a simple approach, it says that the user's address space, the process's address space should be limited between zero and size, okay. Of course in the physical memory it can be all scattered, it doesn't matter but in the virtual address space, so this is of course virtual address space, it should be from zero to size. On Linux for example that is not true, you could you know you could just have you know a segment here and a segment here, anything is possible, paging gives you full flexibility, you can have completely discontiguous mappings but x86 decides to use a contiguous mapping for its process, perfectly okay, all right and so what does the x86 kernel need to check that any user pointer should lie between zero and size, right, before it re-references it. So for example if I do a write fd buff size, all I need to do is if buff is greater than size, let's say proc.size, proc is you know the PCB structure of that process, then you know just say sorry, just return minus 1 for that particular system call, otherwise re-reference it for example. Actually it is not so simple because you don't just have to check buff, you should be checking really buff, buff plus 1, buff plus 2, all the way to buff plus size, right and so question is what is the minimum number of checks I need to make, is it okay to just check buff, you know because I could have given buff here and buff plus size could be here and so you know when I re-reference buff plus 10 or something, then it can cause a page void, so that's not right, is it okay to just check buff plus size, so instead of this, if I just say if buff plus size is greater than proc.size, return minus 1, otherwise just continue and start re-referencing, is this correct? Right, so assuming that size is an unsigned integer, so firstly you know just assume that the size is an unsigned integer, it is not a signed integer, is this correct, so there is one answer which says it may cycle back actually, so what the programmer can do is give a buff here, right and so buff plus size will be somewhere here, it is a 32-bit number, so it will just wrap around and it will come here, so you are going to check buff plus size is less than size and you are going to start re-referencing it and that's an error, so what do you need to check? You need to check both actually, so it turns out that if you check both, you are done, right, so you check buff, you check buff plus size and you are done, right, so that means that both buff is in this area and buff plus size is in this area, so the entire string must be in that area, okay, alright, okay, so I will encourage you to look at these functions in xv6 code called fetchInt and fetchString, right, these are the functions that are doing these checks before de-referencing any user pointer, these are functions that are designed to handle user pointers inside xv6, right, so the convention that the xv6 programmer has followed is that anywhere I have to de-reference a user pointer, I will never do it directly, I will do it using these special functions called fetchInt and fetchString, right, so that and these functions should do the appropriate checks, okay. Similarly, on Linux, there are these functions called copyFromUser and copyToUser, right, once again these are special functions designed to handle user pointers, right, these, if you happen to miss, so for example, you know kernel's code is relatively large and it is possible and multiple people are contributing to a kernel for like you know Linux or any other operating system and so it is quite possible that a programmer makes a mistake and one of the very common mistakes that were happening in the first you know 10 to 15 years of let's say Linux were that programmers were forgetting to actually use these functions to de-reference user pointer and they were just you know somewhere by mistake de-referencing the pointer directly because you know that sort of you know it didn't occur to them that this could be a user pointer, right, so because some of the functions could be called both on user pointers and on kernel pointers and those functions were just de-referencing happily and that you know they should have prevented that, so these kind of bugs were actually pretty common in the beginning and they were very easy targets for security exploits, right, lot of work has happened since then and today it will be very difficult to find such bugs in modern operating systems, right, your first, your second assignment on Pintos asks you to implement these checks, right, so when you are going to do system call handling and you are going to implement the system calls, one of the things you will have to do to make sure that all your tests pass and to make sure that even if the user gives you bad pointers, you are handling them gracefully, right, you are not, it doesn't cause your kernel to crash or behave in unexpected ways, all right, okay, good. Okay, so now I am going to talk about another very interesting topic and perhaps the most practical and useful topic in this course called concurrency, right, okay, so what is concurrency? So we said that you know every process is a thread, so we already know that there is a notion of a thread and the idea is that threads execute in the same address space, right, typically when we write code, we assume that we are the only one running, right, so let's take an example to illustrate this. So for example, I am going to write some code that is taken from the IDE device driver of xv6 which is just a link list manipulation code, so you know the device driver for the disk on xv6, the disk device has the structure called list where each element has a data and then there is a pointer, so it is a link list, okay, I am sure you are all familiar with link list and you know initially, so I am just going to call it list instead of check list, let's assume there is a type def here, I am going to list star list is equal to null, that is the initial configuration and then there is a function called insert that takes a data element integer, right and that is what, it just says, okay, so let's say this is void insert, I am going to write the code for this, the code is very simple, it just says list star l, this is local variable is equal to new list, you know I am using new instead of malloc but basically, you allocate some space of the heap to create a node of the list, let's say l.data is equal to data, right and you say l.next is equal to list, right, you just append it in the beginning of the list, right and just say list is equal to l, okay, so that is your insert function, right. So whoever writes this will probably think that this is correct, right. In fact it is correct under serial execution but if two threads try to insert different data into the same list then what happens, right. So let's see, so we all understand this. So let's say this was list originally, one thread calls insert l1, another thread calls insert l2 and what happens, well we would expect that if there are two threads and they can execute concurrently, so you can think of those threads as just you know backed by processes, so both processes have made some system calls that want to write to the disk for example, right. So both of them are going to make the insert system call and what will happen is that either you will have something like l1, l2 and list, right and let's say this will be the new list head, right. So let me just call it list, either this can happen or this can happen and both of these are acceptable but what is the third thing that can happen that's not acceptable, right. So what will happen if let's say this statement was statement A and let's say this statement was statement B, right. So what happens if thread one executes A then thread two executes A, right and then thread one executes B and then thread two executes B. So the schedule is A1, A2, B1, B2. So let's see what's going to happen. Well I am going to say let's say so there are going to be two elements l1 and l2 and you have a list originally and so l1 is going to say I want to point to list, so he is going to point to list then A2 is going to execute. So l2 is going to say, thread two is going to say I want to point to list, so he is going to point to list like this and now B1 is going to execute so he is going to say list is this, right and now B2 is going to execute and he is going to say list is this and the ultimate structure I have is something like this, right. So if the execution gets interleaved instead of l1, l2 this was a valid output because it's still a well formed list, this is also a well formed list but this is not a well formed list, right. So everybody agrees that this can happen? Okay. So what is the problem? The problem is that the programmer when he is writing this code, he assumed that this code is going to execute in isolation but this code is not going to execute in isolation if your program supports multiple threads in the same address space, right. Kernel is one example of a program that supports multiple threads in the same address space. You could write your own program that has multiple threads either user level or kernel level, it doesn't matter in either case there are multiple threads in the same address space and so this program becomes incorrect, right. This kind of an error is called a race or a race condition, right. So the idea is that one of them is doing something, I am in the middle of doing something and I am not finished doing it and somebody else comes and he starts spoiling my intermediate state and so the end result is something completely wrong, okay. One way to achieve this would have been that I somehow ensure that while one thread is in this code, no other thread can enter this code, right. That is one way to ensure that, okay. With that if you could ensure that then you know the programmer can write his code just like he would write any other serial code and his code will still be correct. So this is you know similar to anything else where you have computation on shared state. So for example you are you know you are typing your program in an editor like VI and typically as a matter of habit you will first do you know you will first save the file and then you will call the compiler like let us say GCC on that file, right. That is a matter of habit. This is just synchronization in your own mind going on between one program VI and another program GCC, right. If you don't wait for the save message to appear and you try GCC's a priori then bad things are going to happen, right and you cannot even predict what kind of bad things can happen, right. Which blocks were written, which blocks were not written, completely out of your control, race condition, okay. Other examples, well I mean even in physical world you know one road, multiple cars, they better follow some discipline otherwise they are going to collide, right. So that is a synchronization basically or a crossing, right. So the one way to do, one way you deal with a road crossing is basically you have traffic lights. You say okay now you can go and now you can go and so on. So you basically do round robin scheduling and that is how you basically ensure that when you know it is not like you get you know you collide with each other. So you just do some kind of synchronization. Another example, let us say classroom scheduling, right. So I am here teaching you a class. There was somebody else who was teaching a class before me and there will be somebody else who will teach the class after me. How are collisions prevented? How are bad things prevented from happening? There is a time table that is put up on the board and you know we are all following the time table, right. So these are all examples of shared state, in this case a classroom and multiple threads. You know we are all threads and you know why do not we collide? Because we are doing some synchronization. Similarly, threads need to do some synchronization to make sure that they do not do these bad things, okay. Let me take another example. Let us say I am a web server and I have multiple threads in the web server, okay. So you are running a website and you have a web server and what you have done is you have said that each thread is going to handle one request. So one thread per request. So whenever I type let us say www.iitd.ac.in, you know my request will be a thread in the system and that thread will take my data, process it, get me the data, you know get the content from its local file system and serve it on the network and I see it on my browser. So if multiple threads are doing it simultaneously, multiple you know multiple that is how concurrency is supported. The same web server, multiple people can access simultaneously, all right. So let us say I have had this global variable called hits that was basically you know trying to understand how many hits do I get, how many people actually visit my web page in a day. So that is this variable hits and this you know all you do is you say hits is equal to hits plus one for every request that you get, right. So let us say what happens is two threads try to execute hits is equal to hits plus one simultaneously, all right. So let us see hits is equal to hits plus one gets translated into let us say load hits into a register R, right and then it says add one to register R, right and then it says store R to hits. I am using load and store but you know we know that on x86 it is a move instruction which does both load and store, okay. So let us say this is the code that gets generated in the assembly level for this C statement called hits is equal to hits plus one and once again what can happen is let us say this is statement A, B and C and there are two threads T1 and T2. If the schedule is A1, A2, B1, B2, C1, C2 then what is the final value of hits? Hits plus one, right. So what will happen is both the threads will load hits into their private registers, both of them will so let us say initially hits was zero. So they will load the value zero into their private register, each thread has a private register, right. So each thread loads the value zero then each of them increments it, so you get one in both the registers and now both of the registers write one to the final output, right. So what you get is hits is equal to hits plus one. On the other hand if you had A1, B1, C1, A2, B2, C2 then the value would have been hits plus two, hits is equal to hits plus two, right. So the final value is schedule dependent firstly, so it is a race condition again and some of these values are incorrect. Now the correct value probably should have been hits is equal to hits plus two, those are the two requests that have happened. This is another example of a race condition and let me just give you one more example. So let us say there are two threads and one of them is saying while i is less than ten, i plus plus, right and says you know when it completes it says print A1 and let us say there is another thread which says while i is greater than minus ten, i minus minus and print B1. So if this is my program and these are two threads, one of them is trying to increment i, another thread is trying to decrement i, what will be the final output and you know whoever finishes, whoever reaches ten or minus ten first wins, right. So he prints happily that I have won and so what are the possible outputs? A1, B1, another output is B1, B2, C1, A1, whoever prints first has won, right. You cannot really you know there is no way to approach. So the semantics of thread don't tell you who is going to win. They just say anybody can win. Is it even guaranteed that this program will ever terminate? No actually the program may not terminate really, right in theory. There is a non-zero probability that this program just keeps shuttling between values but neither reaches minus ten nor reaches plus ten, right. So these are you know bad situations. Programmer does not want this kind of uncertainty in general. So what's the problem, what's the solution, what's the possible solution? Well one solution is don't do anything assuming that this kind of situation is acceptable, right. So for example I gave you an example of web server hits. You may say okay what's the probability that I will miss an update to hit. So I said that in general if you know these were three instructions to update hits. If they execute like this everything is okay. If they execute like this everything is okay. If they execute like this then there is a problem. What's the probability that they execute like this? It's relatively small because what you need is that that thread should be executing in exactly in the middle of these three instructions. The probability is very small and you may say oh I don't really care about you know the exact number of hits my website gets. You know I just want to get a ballpark figure so you know I don't care if it was supposed to be one million even if I get a number which is one million minus one thousand I don't care, right. So in this case you just ignore it. You say let it be, okay. So that's a valid response in some situations but not in every situation. For example the link list example, no that's completely an invalid response. You have corrupted your data structure and so everything in future will actually be wrong. The other response is try to avoid sharing. If there are two threads why can't you just duplicate state, right. So you want to have hits, you want to count the number of hits at the end of the day. Just have two variables, one for this thread and another for this thread and let them update their private variables and at the end of the day just sum up those variables and get your answer, right. That's another valid response. Works in some cases, right. And you should try to do that as much as possible. Avoid sharing. If you avoid sharing you avoid these bugs, these problems. But what these are, both of these are not general solutions. The general solution is to avoid bad interleavings, right. So I want to avoid bad interleavings, okay. So what are bad interleavings in this case? Well this is bad, right, and this is good. So I have some notion of what's a bad interleaving and what's a good interleaving and I will avoid the bad interleavings and allow the good interleavings. In general when we write code, we assume serial executions and so bad interleavings are usually interleavings that involve overlaps, right. And good interleavings are interleavings that involve mutual exclusion, right. So in general, good interleavings are usually executions that involve mutual exclusion. So while I'm running here, you're not going to run here. So you're going to run after I have run. It doesn't matter the order in which I run. So even this is a mutually exclusive execution and this is also a mutually exclusive execution. This is not a mutually exclusive execution, all right. So mutual exclusion is usually a good way of ensuring that your code is correct, okay. So this problem of managing concurrency is a general problem. There are several solutions to this problem, all right. And the problem is in general a complex one because the different kinds of situations and different kinds of situations require different types of solutions and responses from the programmer. But one common response that most programmers use is what's called locks, right. So what are locks? Well, locks allow you to implement this mutual exclusion, right. So you want to say that two calls to the insert function should be mutually exclusive, right. So for example, in this, okay. So let's say I wanted to say yes, all right. So let's say the calls to insert should be mutually exclusive. But actually is it okay to just say that the calls to this function insert should be mutually exclusive. Do I really need mutual exclusion on the whole function? Well, I just need mutual exclusion on these two statements, right A and B. If I could ensure that these two statements execute in a mutually exclusive way, I would be done. It doesn't matter if these are mutually exclusive or not, okay. If these two statements had occurred in a mutually exclusive way, I would have been done. So I need some extraction that allows me to say that these two statements are mutually exclusive, other things I don't worry about. If you had made the entire thing mutually exclusive, would that have worked? Yes. Yeah, it would have worked because it's still mutually exclusive, except that you are constraining the system more than it needs to be, right. So which can cause performance loss, right. So these two statements could have executed parallelly on two CPUs, but now you have made them mutually exclusive, so you know, they cannot execute parallelly. So you have serialized more things, right. So in other words, to do, you know, call this serialization. So when you make something mutually exclusive, you're basically serializing the calls or execution of the functions. So one thread is executing this, another thread is also executing this. Instead of allowing this execution, you have serialized it, right. So that's mutual exclusion, right. So you could have put the lock entirely, but then you are serializing more than required. So you want to put a lock around these two statements, that's one thing. Also, do I need to serialize all calls to insert, or can I do something smarter? Can I say, okay, you know, let's have multiple lists, and so when I'm inserting to the same list, only then I need to serialize, right. If I'm inserting, if one thread is inserting to this list, and another thread is inserting to that list, I don't need to serialize, right. So I need to be able to represent that, right. So serialization is not only at the code level, it also depends on the data. So let's say the insert function was taking an argument as a list in which you want to insert, then you want to say, oh, you know, only if the L1 and L2s are equal, do you need to serialize. If they are not equal, then you don't need to serialize. So you know, whatever this abstraction is, needs to be able to do this, right. Also, I've said that this is insert, but let's say there's another function called delete, right. So once again, you know, delete will have some similar code that will involve two or three pointer manipulations before it gets complete. And so you may want to serialize not just inserts with respect to each other, but also insert and delete. So it's not just insert versus insert, it's also insert versus delete, right. So all these things need to get serialized with each other, right. So it's not just saying that, you know, this function should be serial with respect to itself. This function should be serial with respect to some other functions, and I should be able to specify which other functions. And it's not just functions, it's basically saying these statements should be serial with respect to those statements, right. So I need some abstraction to do that, right. And locks is one such abstraction, and I want to talk about it in the next lecture."}