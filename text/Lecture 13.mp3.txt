{"text":"So far, we have been looking at paging, and let's review it once again. We said that segmentation allows you mapping from virtual address to physical address, but it only allows contiguous mappings, because it's a simple base plus VA gives PA mapping, which is not very flexible. It doesn't, it has problems of fragmentation, and it has problems when, if a process wants to grow, et cetera. So if you had more general mapping, and that's what paging implements, then it would be much more flexible. And what paging does is divides the physical address space and the virtual address space into page-aligned, fixed-size, page-sized units, which are called pages. So aligned, fixed-size units, which are called pages. And now there is a mapping hardware in the middle, which will map a page in virtual address space to a page in physical address space. This mapping hardware basically requires a table, which can have at most, you know, 2 to the power 20 entries. So such a large table cannot be stored on chip, so such a table needs to be stored on physical memory. Right? Or, you know, the technology for physical memory is also called DRAM. So you store the table in physical memory. And also we said that rather than storing the table contiguously, which will make it very large, 4 megabytes, let's divide it into a 2-level hierarchy. And so that way, small processes will only have very small page tables. And large processes will also, you know, will also benefit in space. Even if there are all the pages mapped, even then, you know, the 2-level hierarchy is not too much space hungry than, hungrier than 1-level page hierarchy. All right? Okay. So on the chip, there is a register called CR3, which holds a physical address, which points to the base of the page directory. The virtual address is used, the top 10 bits of the virtual address are used to index into the page directory, from which you get the 20-bit physical address of the page table and 12 bits of flags. These flags are, you know, 3 relevant flags for us are present, user, and writable, whether the page is present, whether the page table is present in this case, whether a user mode execution can access it, and whether it's writable. So for example, if this is non-writable, then all the pages which are referenced from here, they're all non-writable. On the other hand, if this is writable, then depending on what the flags are in the corresponding page table entry, the page will have writable or read-only functions. All right. So the top 10 bits are used to index the page directory, the next 10 bits are used to index the page table, and the last 12 bits are used as an offset into the page. Right? Okay. All right? And we said that an operating system can implement multiple processes, or multiple address spaces, by switching the page table on every context switch. Right? So each time the process wants, the operating system wants to change from P1 to P2, you change the CR3 value from P1's page directory to P2's page directory. You just load a new value into CR3, and you have a new page table. And we also said that the kernel maps itself into the address space at the same point in every page table. So a process does not see the entire 4 gigabytes of virtual address space, it sees something less than that. In xv6 it only sees the bottom 2 gigabytes, and the top 2 gigabytes of the virtual address space are reserved for the operating system, or the kernel. And so the kernel is really mapped at the same place, pointing to the same physical addresses in every process. And so we also said that hence every user process is also a kernel set. Because a process now has two halves, one is the user half, which the user can access, and the other is the kernel half, which only the kernel can access. A user cannot just switch into the kernel half, the only way a user can switch into the kernel half is through the interrupt descriptor table, through a trap. And once it switches into the kernel half, then it executes in kernel mode. It's possible that multiple processes simultaneously are executing in the kernel half, and that's why we are calling them kernel sets. On the other hand, if multiple processes are executing concurrently in the user half, no problem, because these are independent addresses. So how many accesses, physical memory accesses, does the processor need to make on a memory access by the program? Three. So first you will dereference, first you will add these 10 bits to CR3, and then dereference this value, so that's memory access number one. Then you will get these bits, and dereference this value, that's memory access number two. Then you will get this, and then dereference this value, that's three. So for every memory access, you're actually making three physical memory accesses. So for every virtual memory access, you're making three physical memory accesses. And that's not acceptable, that's too costly. In segmentation, we saw that such a problem was solved by basically ensuring that when a segment descriptor gets loaded, the segment descriptor gets cached into the chip. So you don't need to access memory to get the base value each time. Base value is just cached on chip. Similarly in paging, also there is caching involved, and there's a special cache called the translation look-aside buffer. Translation look-aside buffer, or TLB for short. This caches VPN, virtual page number, to VPN mappings. So each time you basically access a page, a virtual address, you walk the page table, the two-level page table, and you get a physical address. But there was a mapping from the virtual page number to the physical page number that this page table structure was providing. So if you could cache this end-to-end mapping from VPN to VPN in your TLB, and you could first check your TLB, and the TLB is on chip. This is on chip, on CPU. So if the virtual address, the VPN corresponding to the virtual address being accessed, is already in the TLB, you don't need to walk the page table. You can just get the VPN from there, add the offset, and directly access the memory. So that way you just have to have, if you get a TLB hit, implies one access per VA access. One physical memory access per V access. A miss implies three. So if there's a TLB miss, you have to walk the page table, just like before. If the TLB hit, you don't need to go to the physical memory. So it's important in the real world that most of your memory accesses are TLB hits. Most of your virtual address accesses are TLB hits to have any acceptable performance. The whole idea of paging falls flat if the TLB hit ratio was not high enough. Because segmentation was giving me a very fast translation. If paging is giving me a 2x overhead on every memory access, that's not acceptable. So fortunately, most programs have a very high locality of access. Usually because we are dividing the address space into page-level granularity, one entry, one VPN to VPN entry is capturing locality of access within an entire page. And so it's possible to have a small TLB and yet have hit rates of 99.9% or above. And that's the kind of hit rate you will want in such a system, to have any acceptability of this paging idea. So typical TLB sizes on modern processors will be, let's say, 4 kilobytes also. So a 4 kilobyte TLB can cache, let's say, each TLB entry requires 8 bytes. Then a 4 kilobyte TLB can cache 512 such page entries. And assuming that your memory footprint, you can basically cache 512 VPN to PPN mappings in your TLB. And assuming that the program's memory footprint is less than 512 pages, then pretty much you should be able to access, you know, this translation should be pretty much free. You're only accessing the on-chip TLB. Also, let's understand, whenever we talk about caching, so assume you have seen caching and you know what is hit ratio and all that. So I've been talking about hit ratio, and let's say this is, you know, greater than 99, you know, just a ballpark figure, 99 to 100% somewhere in the middle. That's the kind of hit ratios you're looking at. So when we talk about caching, we should also say, you know, what is its cache replacement policy, what is its associativity, what is, whether it's a write-through or a write-back cache, right? So you have seen all these terms in your computer architecture class, right? Okay. So firstly, what is its associativity, which means, you know, you know about cache associativity. So typically, the TLB is a fully associative cache, right? You want your TLB to have such a high hit rate, so you better make it fully associative. If on the other hand, you had a direct map cache, you will just have conflict misses in your cache, right? Okay. The other thing is, what's the cache replacement policy? Well, the hardware is free to implement any cache replacement policy it likes. Given that most of the things are going to be hits, you know, cache replacement policy does not, cache replacement policy usually matters if the size of your caches are small. If the size of the caches are bigger than the working set size, then cache replacement policy is actually not that crucial. It doesn't really matter what your cache replacement policy is. So, you know, you can use something like LRU, which will require some bookkeeping, or simpler policy may be FIFO, and the hardware designer can make a choice, you know, FIFO is simpler to implement, but it may have a slightly lower hit ratio than LRU. LRU is more complex to implement. It may or may not have better hit ratios than LRU. Now let's look at whether it's a write back or a write through cache. Let's see what happens. Each time a page table is walked by the hardware, the entry gets cached into the TLB. So this is a read operation, right? All these lookups are just reads of the page table. The only write to the page table is when the kernel actually overwrites some entry by looking at that particular address, right? Recall that the page table itself is mapped in the kernel address space, right? So the kernel can change the page table by just writing to a memory location there. If the kernel changes a page table entry, TLB does not even come to know about it, right? So the kernel needs to tell the hardware explicitly to invalidate a page directory entry, a TLB entry, right? So there's an instruction called, let's say, invalidate page, and it takes a virtual address, right? Basically means invalidate any entry in the TLB corresponding to this virtual address, right? So there's an explicit invalidation of the TLB entry, the cached entry. If the kernel forgets to do that, it's a bug, right? Very bad things can happen. You can imagine what can happen. What can happen is that the kernel thinks that it has mapped a certain page somewhere else and removed the entry here, but the TLB still caches that entry, and now a user can probably access somebody else's page. So those kind of bad things can happen. So it's important that the kernel executes. Each time it changes a page table entry, it invalidates all TLB entries corresponding to a particular virtual address, or the single TLB entry corresponding to a virtual address. Also, each time you do a context switch, you change CR3, right? So what should happen to the TLB? All the entire address space has changed, right? So you should completely flush the TLB, right? So reload of CR3 implies TLB flush, right? So in every context switch, you flush the TLB completely, because you know the entire address space has changed. All the VPN to VPN mappings have—all the cached VPN to VPN mappings need to be invalidated. Okay, all right. So it's important for an operating system to ensure that the number—so the number of entries in the TLB typically remains small, you know, and one way that—one method that the hardware provides to do that is supporting what are called large pages on x86, right? So there's something called large pages. So we said, you know, normal pages are 4 kilobytes, but large pages are 4 megabytes. You could have large pages, and how would you—how does the x86 architecture implement large pages? In the flags of the page directory entry, there's yet another bit. So for PDE flags, there's another bit that says page size, right? If the page size bit is 0, it means it's a normal page. If the page size bit is 1, it means it's a large page. And if it's a large page, it reads the 20-bit pointer not as a page table, but as a pointer to that 4 MB page, okay? So if it's a large page, then this pointer is pointing to a 4 MB page directly, right? Because you can imagine that, you know, a large page, if you are basically talking about a 4 MB page, you have divided your physical address space into pages of size 4 MB, right? And so pages of size 4 MB means 22 bits are basically used for an offset, right? And the top 10 bits—so 22 bits are used for the offset, and the top 10 bits are used to identify the page number, right? And so the top 10 bits of the virtual address are used to index the page directory entry to get the large page number, right? Even large pages need to be aligned at page granularity, right? So a large page cannot—a large page should always start at 4 MB boundaries, right? So there will be a large page at address 0, and then the next large page will be at address 4 MB, and 8 MB, and so on. Just like small pages were aligned at 4 KB, large pages are aligned at 4 MB, right? And so 10 bits are enough to name a large page. And so the top 10 bits of the virtual address are enough to name a large page, and so the page directory entry directly points to a 4 MB region, right? If you're using small pages, then you allow this 4 MB—in any case, the page—one entry in the page directory is quite capable of mapping 4 MB of virtual address space. If you're using small pages, this 4 MB can be fragmented in 4 KB chunks across the physical memory. If you're using large pages, then this 4 MB chunk has to be contiguous in physical memory, right? That's the only difference. So what's the advantage of doing this? The disadvantage is the time it takes to actually walk the page table has reduced. You only need to dereference one, or make two accesses for one, even if there's a TLB miss. More importantly, the number of entries that need to be cached in the TLB has reduced, right? As opposed to 210 entries for a 4 MB space, you need only one entry, if you can ensure that the entire space is contiguous. But it also means that the operating system should take care of things like fragmentation, right? For example, small processes should not be given large pages. Large processes can be given large pages, but make sure that they're actually using those large pages, because in the entire address space, if they're not, then I'm basically worrying about fragmentation and all that kind of issue. One big place where large pages are really useful is to map the kernel itself, right? So we said that every process, let's say this is P1, maps the kernel starting at, you know, starting at 2 GB, let's say, right? And we said this mapping of the kernel is basically a one-to-one mapping to the physical address space. This is the physical address space, and this is going from 0 to M, then this is going to from 2 GB to 2 GB plus M. It's a completely contiguous mapping in the kernel. That's what we saw, that the kernel just maps the entire physical memory into its address space, at least in XB6, and we said other operating systems can recycle virtual address space for, if, you know, the amount of physical memory is greater than what can be supported in the address space. So for this kind of a mapping, which is completely contiguous, it doesn't make sense to use small pages. You can just use large pages, right? So you can just have, you know, 4 MB pages to store this mapping from, for the kernel. That way, the kernel, the size of the, there are a few advantages of this. Number one, the size of the page table has reduced, right, because you only need a few entries to map the kernel. You have made the granularity bigger. So the overhead that we had, so we said that every process needs to map the kernel. So every process has to have this extra overhead of having these extra entries for the kernel address space. If you're using large pages, then this overhead has decreased significantly, number one. Number two, more importantly, the TLB pressure has decreased. The number of entries that need to be in the TLB to cache this mapping has decreased, and so you can have better height ratios in your TLB, right? So large pages can be used other places, but one place where they have a direct use, immediate use in the operating system design that we are talking about, is that you just use these large pages to map the kernel itself, okay? While caching a page, no, you mean in the TLB? So in the TLB, you only store one mapping, right? VPN to PPN. If you're using 4 kilobyte pages, you could potentially have 2 to the power 10 entries to map an entire 4 MB space, right? On the other hand, if you're having large pages, then you'll have only one entry for that entire space. But large pages have problems of fragmentation and wastage of space, potentially, if you're not careful, and yeah, so there's a trade-off, but here's one case where it's a no-brainer to always use large pages. Yes, question? Okay, so the question is, why use any sort of paging mechanism to map this space? You know, it would be great if the hardware could provide me a mechanism saying, oh, map the entire space here. But the hardware doesn't do that, right? I mean, once you have enabled paging, you have to go through the page table, right? And you know, one way the hardware designer could have said was, you know, have another bit saying that this is not going to go through paging, this particular address is not going to go through paging. You know, it makes the hardware even more complex. So you know, one way to do that is just use the existing page table hardware. And large pages can be used both for the kernel and for other things also, okay? The hardware designer does not necessarily want to complicate his hardware just to support the kernel space mapping, when he can achieve the same effect by using a more general mechanism of using large pages, right? Right? Okay. Okay, so with that, let's continue our discussion from yesterday, where we are looking at how does the OS boots. Let's look at the OS boot-up. And we said, let's say here's a disk. And here are its blocks. Block 0, 1, 2, and so on. And we said this particular block or sector is special, right? So each of these sectors is 512 bytes. And sector number 0 is special, it's called the boot sector. And the contract with the hardware is that it will load this boot sector at a particular memory location. It will copy this boot sector in the particular memory location. So let's say this is the physical address, going from 0 to some value, let's say M. So it's going to copy these 512 bytes at some location, the boot sector BS. And the address it loads it as, it as 7C00, right? I mean, these addresses are just historical in nature, you know. This must have been the address at which it had loaded, you know, back in the 1980s, and it still continues, because for backward compatibility, we want that the operating system that was written in 1981 should still run, right? So for that reason, it has to do the same thing. Okay. So, and we said that what, and so what the, so the operating system developer needs to write a boot sector code that should know where the kernel is in the disk, number one. And it should load the kernel into memory, and then jump to the kernel, right? So all that operation needs to be coded up in the boot sector. And all this, the code to do all this should fit inside 512 bytes, right? That's the constraint. And that's relatively easy to meet. It's not a big deal. And we are going to look at the boot sector code of xv6, all right? Okay. So please take out your, this thing, code listings. Okay. So firstly, we are looking at code of xv6, right? This code will get compiled using, let's say, a compiler like GCC, and we have discussed this before. And then the code will get linked, right? And then you will get an executable. That executable is what we call the kernel. The xv6 links into what's called an executable file, which is called the kernel, right? And that kernel will also, and there will be another sort of file which will be the boot sector code, right? And the linker will set up the addresses and set up things in such a way that the boot sector code will live in the first sector of the disk, number one. And number two, the boot sector code knows that it will start at a certain address called 7300, right? That's the hardware specification. So let's assume that the boot sector, so the boot sector code actually starts at this point, right? 8412. So this is the assembly code of the boot sector, all right? And the line 8409 is saying code 16, which basically means treat this code as a 16-bit code, right? So this is a 16-bit code. The next line says global start, which basically means consider this symbol start as a global symbol, okay? What that means, we can talk about it later. So this is the first instruction that gets executed on when the computer gets booted up in xv6. The first instruction in this case is CLI. CLI is basically just disabling interrupts, right? So what does the CLI instruction do? Recall that the x86 architecture has this register called eflags. And one of those, one of the bits in the eflags is whether interrupts are enabled or disabled. If interrupts are enabled, then an external device is allowed to assert the interrupt pin and my execution will switch to the handler immediately. If interrupts are disabled, even if the external device asserts the interrupt pin, I will not switch to the handler, right? Okay? So first, recall that the x86 has a register called eflags. One of the bits in the eflags is what's called the interrupt flag, or interrupt enable flag, or if, okay? The semantics are if if is equal to 0, implies will not receive any external interrupts. In other words, just ignore the external interrupt, okay? If it's 1, then it's the usual thing, you know, take a trap on an external interrupt, okay? So if if is equal to 0, you just ignore the external interrupt. If i is equal to 1, then you will take a trap on the external interrupt and recall that the trap goes to the interrupt descriptor table to call the handler. Why the first instruction is clearing the interrupt? Because I have not set up any handlers yet, right? The BIOS may have had set up its own handlers. There's some, you know, BIOS that has run before the first instruction has run. It may have set up its own handlers. And now I want to completely forget what the BIOS has done to the system. I want to now, you know, re-initialize the system according to myself. And while I'm doing that, I just I don't want any disturbance from outside worlds, right? So if an outside, if there's a network packet coming or anything of that sort, or, you know, disk wants me, wants some attention, just ignore all that. I'm not in a state to actually be, serve all these people. So let's clear the interrupt, right? So that's what, so CLI basically sets IF to 0, okay? Okay, then the next thing you do is you say XOR AX, AX. The effect of XORing a register with itself is that you just zero out the register, okay? And then you move AX to all the segment registers, DS, ES, SS. Why am I doing this? To have a flat address space, right? I don't want to have a segmented address space. Let's set up my segment register. Recall that in 16-bit mode, the segment register, the segmentation works by simply multiplying the segment register value by 16 and adding it to the virtual address to get a physical address. So I don't want any segmentation, so I just set it to 0, so my virtual address is equal to physical address from now on, okay? So from now on, my virtual address is equal to physical address. Yes? Yes, could you have directly moved $0 into AX? Yes, you could have. Why is he doing it in this way? It turns out it's more efficient to do it. And this is, you know, this is a standard thing that assembly programmers use, that, you know, instead of moving 0 to AX, just XOR it with itself. That turns out to be more efficient, counterintuitively, right? Anyways, I mean, you could have done the other thing also. Okay, so no, but see, DS, ES, SS are special registers. They cannot only, you know, a certain instruction move can be used on them. You cannot use arithmetic instructions on segment registers. You can only use arithmetic instructions on general-purpose registers, which are ABCD, ESP, EBP, ESI, EDI, right? So you can only use arithmetic. And you can't even move an immediate value to the segment register directly. You can only move it to a general-purpose register. So these are the constraints of the architecture, all right? So that's why he's doing it like this, all right? Then there is some code to allow addresses above 20 bits. So original 8086 machines didn't allow addresses which are greater than 20 bits, right? At that time, the machine was 16 bits. But now we want a 32-bit architecture. So there is some code to allow addresses above 20 bits. In any case, we can ignore this, all right? So let's ignore this. It's needed for your program to run. But it's not needed for us to understand what's going on, all right? So let's just ignore this. And let's instead come to this instruction. LGDT, GD desk. What am I doing? I want to load a global descriptor table. And notice that this LGDT instruction is actually a 32-bit instruction, all right? It's not a 16-bit instruction. Because in 16-bit, there was no GDT. In 16-bit, segmentation was just multiply the segment selector by something, and that's it. The GDT is only in the protected mode. And so what I'm going to do in the next four lines is basically switch to 32-bit mode, all right? But before I switch to 32-bit mode, I need to set up my GDT and all these things so that when I switch to it, it knows exactly where am I standing and all that, right? Okay. So what are the semantics of LGDT, GD desk? GDT desk itself, the descriptor, is defined here at line 8487, okay? And it contains the location of the GDT. And it contains the size of the GDT, right? Recall that we said that the GDT is specified by a size and a base also, right? So this is the base of the GDT, so 0, 1, 2, 3. And this is the maximum size of the GDT. So that's what you specify in the GDT desk. And let's look at where GDT is. That's at this line 8482, right, which contains your segment descriptors, right? So segment descriptors are nothing but numbers, right? They're just numbers which basically specify this flag should be set up, this is the base, this is the limit, and all that kind of stuff, right? So in this case, here is a number called seg null ASM, which basically says zeroth descriptor is null. No, never use it. Nobody should ever use it, all right? The second one says it's using a macro. So these are all macros. Seg null ASM is a macro which will get macro expanded. So you can actually browse the x86 code to see exactly what number this seg null ASM represents. But, you know, whatever the number is, it basically sets up the GDT zeroth entry to say that this should never be accessed, basically, dereference. The second says seg underscore ASM, which is again a macro. STAX, which says give execute privileges. X is for execute. STAR is saying give read privileges. So these are basically specifying what flags I want in the GDT descriptor. Zero says what's the base of the GDT descriptor. And this FFFF says what's the limit of the GDT descriptor, right? So you use a macro to say construct the GDT descriptor with flag execute and readable, and base zero and limit 2 to the power 32, minus 1. Okay? So the intent is that the descriptor number 1 is going to be dereferenced for all your code, right? So you're going to load the value 1 into CS. Descriptor number 1 into CS. And so code will always go through this. And so that's why it will become executable, right? And all the other ones will have writable, which means writable means it's both readable and writable. And base is zero. And limit is again 2 to the power 32, minus 1, right? So you have a GDT of size 3. The zeroth entry is a null entry. The first entry is pointing to an executable segment, which is the entire address space. And the second entry is pointing to a writable segment, which is again the entire address space. The developer has separated code and data in this way. So he'll load code in CS, and data in all the other segments, right? You may say, could I have had a single segment which is readable, writable, and executable, and load that same thing in everything? That's also perhaps possible, depending on whether x86 allows that kind of thing, that a segment is both writable and executable, right? But in any case, they're sharing the entire address space. It's not like CS is living somewhere else, and all the other segments are living somewhere else. They're actually living in the same space. The different segment descriptors are only being used to check the type of access, execute versus write. Yeah? All right. Okay, so you load the GDT. And then you do some things to make sure that you're moving into 32-bit mode. So CR0PE basically says, move into protected mode. So the 32-bit mode that we have discussed is also called the protected mode, because it offers protection, right? And then you move this value into CR0. There's a control register 0, which indicates which mode I'm running in. I'm running in 16-bit mode, 32-bit mode, or protected mode, et cetera. And so that's how you move to 32-bit mode. Why have we taken... Okay, so the question is, why both of them are pointing to the same region, 0 to 2 to the power of 32 minus 1? Why not code referring from 0 to some M, and data from M to 2 to the power of 32 minus 1? Well, I mean, he could have done that, but that complicates the programming model. Sometimes you want to access code just like data. But we will not be able to differentiate over whether we want it. Right, so basically this kind of organization allows you to see a flat address space, right? It doesn't matter which segment you're going through. You'll always... VA will be always equal to PA, right? On the other hand, if you do the other thing, then I'll have to worry about, you know, whether I'm going to go through this segment or that segment. And if you have a flat address space, it allows you to change code using directly its address, and then execute it. And because all of them are pointing to the same region, you don't have to worry about, you know, oh, CS is pointing here, and DS is pointing here. So all these segments, CS, DS, et cetera, they basically see identical things, right? So the program needs to only worry about the offset. It doesn't need to worry about the segment at all in this organization. So it's a flat segmentation model, right? We have discussed this before. Okay, so I have moved into segmentation mode, into 32-bit mode. But there's one more thing that the x86 architecture requires you to do, which is to say that, oh, you know, switch. So right now, what happened was, I changed the control register zero to say that I want to execute in 32-bit mode. And then I executed this instruction called LJUMP, right, long jump. We have seen this before. Segment ID and offset, right? Recall what the LJUMP does? It just loads CS with this value. The value of segK code is one, all right? So this is equal to one. So it just loads the first descriptor into CS. Segment descriptor number one into CS. And what is start32? Start32 is just here, right? So start32 is the value or the address of this particular instruction, whatever comes after start32. So it's basically saying, jump. By using LJUMP, it's basically causing CS to get overwritten. Right now, CS is zero. Now you're overwriting CS with one, one shifted by three bits, right? So that way, you basically are looking at the first segment descriptor, and now you're actually executing through the GTT, okay? And start32 is the address of this. Once again, because, you know, everything was zero, so I didn't have to worry about, you know, start32 is an offset, et cetera. Start32 is just an address, and offset is equal to address. So no problem, okay? All right, so at this point, I have loaded this code segment, and I have loaded the EIP, and I have moved into 32-bit mode. So the moment I have loaded the new code segment, I basically have declared that I am executing in 32-bit mode, and that's why this particular directive here, code32, is telling the hardware, telling the assembler to interpret all the next instructions as 32-bit instructions, okay? And now the first few things that you do is that you load all the segment registers with K data. K data is, you know, two, let's say. So two. So you load segment number, descriptor number two, into DS, ES, SS, all the other segments. Right. Recall that the segment selector, so the segment register had the last three bits reserved for something else. The top, you know, the segment selector itself was living only in the last, in the top sort of 13 bits. So that's why we're shifting it by three. Right? Also recall that the last two bits of the CS register were meant to indicate the privilege level. In this case, when I'm shifting it by three, the last two bits are zero, and so I'm still executing in privilege level. Okay? All right. So I load up all the segment registers, just like, just like before, but this time with a different value, say K data, which is two shifted left by three. Okay? Right. And then what I do is I move $start to ESP. What am I doing here? Let's see. What is $start? $start was the address of this place, which was 0x7c00, right? We said that, you know, that's where the code is going to start, and so the linker has organized it in such a way that the address of this place is 0x7c00. That's why we started at that place. Right? So, so the value of $start is basically 0x7c00, and I'm putting it in ESP. What I'm really doing is I'm initializing my stack. All right? So, so what happened was, let's say this is my PA space, and let's say this is 0x7c00. All this code that we are looking at is actually living in this area, right? So this is, this is $start, and let's say this is, somewhere here is $start32, right? And up to 512 bytes. Okay? So all this is living in this area, and what I did right now was I set my ESP register to point to this location. And recall that the ESP goes downwards, so all this area here can be used as a stack. Now, right? So when I'm going to make a function call, the stack frame is going to get pushed somewhere here, in the lower area. So that's what I'm doing here. When I say move $start% ESP, I'm putting the value 7c00 into ESP, and then the next thing I do is I call, make a function call. Right? So make a function call to bootmain, and so the return address gets stored at 0x7c00. Actually, 0x7c00 minus 4. That's where the return address will get stored, and ESP will get decremented by 4. Right? Okay. Where is bootmain living? Where did this bootmain come from? Bootmain is, so we have, you know, we have executed the first few instructions in the assembly. Programming in assembly is difficult, so let's write the rest of our code in C. Right? So we are going to, so the bootsector itself, the rest of the bootsector is living, has been written in C. So bootmain is also living in the bootsector somewhere. So somewhere here, in this space itself, there is bootmain. And the linker has appropriately arranged for it to be at that address. And the code for bootmain is in the next sheet, sheet 85. Okay, and now we see some C code, and that's, you know, more familiar and easier to sort of understand. Okay? So what this function is going to do is it's going to load the kernel from the disk into memory. So, you know, 512 bytes is too small, so I want to load the other bytes of the kernel into memory and jump to the first instruction of the kernel. That's what this function is going to do, right? And the compiled code of this function is living in the bootsector. All right. And so let's understand first how the kernel is organized. So what happened was you had some .c files and .f files in your xv6, right? You converted them to star .o files, and then you, let's say, through GCC, and then you linked all these .o files and got some image which is called the kernel. This kernel is an executable file in a format called ELF. Even your programs, your a.out files, et cetera, they are also in this format called ELF. And there's a standard format with a specification that this is the format of an executable file. And what the executable file will have is basically it will have things like, if I look inside kernel, it will say, oh, by the way, this is my file, and this is byte number 0, byte number 1, and so on. And it will say, oh, byte number 0 has to have a certain header which will indicate where my code is. So there will be some pointer here which says, here is all your code, right? Here's where it starts, and here's where it ends, so all the size and all that. And then it will say, okay, you know, let's say, here is all your data in the file, and so on. It will also say that this code should be loaded at what address. So it may say load code at address, you know, 1, 2, 3, 4, 5. And load data at address, you know, whatever, 4, 5, 6, 7, 8. So it has all this information. It says, here's your code, here's the data, load code at this address, load data at this address, right? These are virtual addresses, right? I mean, this is basically whatever address space I'm currently executing in. So if I'm executing a program, I look at this ELF file, and the process has some address space, and so ELF file will tell me in this address, so let's say the process has an address space from 0 to 2 GB. The ELF file will say, load the code at address 5 MB, and load the data at address 7 MB, all right? And that's it, okay? And here is the first instruction. So the other thing it has is start. Where to start? So you've loaded the code, and you've loaded the data into the address space, pasted them there, and now you want to know, where should I start? So what's the first instruction I should execute? In other words, what should I initialize EIP to, right? And so that's also stored in this file. It basically says, after you've loaded it, set EIP to this, and you're good to go. After that, you just execute whatever you like, and you'll probably execute some instructions, you'll make some system calls, whatever you do, right? Similarly, in this case, in the kernel case, it's not really an, it's not, the kernel is not going to be loaded inside the process, but it's going to be loaded by the bootloader into physical address space, initially, right? Also, when you say that the code should be loaded at a certain address, the code internally could have pointers to itself, right? For example, inside it, I could say, you know, jump to some address, 1, 2, 4, 5, 6, right? So 1, 2, 4, 5, 6 should be meaningful, right? Because I know that I'm going to be loaded at 1, 2, 3, 4, 5, so 4, 5, I know that what instruction is going to live at 4, 5, 6. And so internally, I could say, you know, jump to 4, 5, 6, so I know what instruction is going to get executed at that time. For example, you know, functions are named by addresses. Variables are named by addresses, right? So a variable will live in the data section, a global variable will live in the data section, right? A function will live in the code section. And these things will have internally pointers to each other, right? Code, some instructions will have pointers to some functions, right? Or some instructions will have pointers to some data. But these are all at fixed addresses because the ELF has dictated that this particular data should be loaded at this address and you have already pre-computed the address of the variable inside the data section. Okay. Right, so similarly, what's going to happen is that this kernel will say that this is my code and this is my data, and it'll say, load this code at this address and load this data at this address, and so on. And what my boot loader is going to do is respect whatever the ELF file is telling him and put it at the right address. And now, and also the ELF file will contain the start address and based on that, it's going to jump to the kernel. Right? And so the kernel has, so the boot sector job is over. Now it's the kernel which takes over after that. So the boot sector loads the kernel and jumps to its first instruction. And how it does all this is basically dictated by the ELF file. All right, let's stop here and we are going to discuss more code next time. So highly recommend that you familiarize yourself with x86 and all that, and we're going to do more of this next time, next class."}