{"text":"Welcome to operating systems lecture 23, right. So last time we were talking about locking and we took this example, this hypothetical example of a bank with many accounts and with functionality like transfer and some and we said that look coarse-grained locking can solve all the concurrency problems but coarse-grained locking is not good because it serializes everything, right. So because it ensures that everything is mutually exclusive, it basically causes everything to get serialized. So even if there are multiple processors, only one transfer function will be able to execute any time if you are using coarse-grained locking. So then we said okay, you know we should use fine-grained locking and the question was how should you decide how to use fine-grained locking. So this choice of how to choose where to use fine-grained locking is a bit of an art. So there is no, I mean it is basically something that the programmer has to decide based on what he feels is the right way of doing things, right. So there is no one rule to say that this is how you should fine-grain lock in this program or that program. Depending on the program, you would want to choose your fine-grained locks differently. So for example, you know yesterday we said that every account should have a lock, so there should be a per account lock and any operation that requires access to multiple accounts, you should take all the locks before doing that operation. All the locks for all the accounts that are touched in that operation. So transfer operation touched two accounts, you are going to take two locks. The sum operation touched all accounts, you are going to take all locks, right. And we also said that one way to take the locks is to take it in an on-demand way. When I say I take it in an on-demand way, it does not mean that I release the previous locks, right. Because this operation is basically an operation that needs to be atomic, we need to take all the locks at some point in time anyways. It is just that you can say that I will take the lock for the first account, then do some computation, then I will take the lock for the second account without releasing the lock for the first account and so on, right. So you could do that, but we also saw that the locks have to be in a certain order to avoid deadlocks, right. And so the ordering has to be global. Once again, you know, the programmer has to figure out what the order has to be and the order may be tied to your data structure. It may be tied to the semantics of your program. For example, the last time we decided that we are going to order it on the account ID of the account. And based on that, we will take a priori all the locks needed. For transfer, we will take two locks. For sum, we will take all the locks, do our atomic operation, then release all the locks, right. So that was a hypothetical example, of course. Let us look at another example. Let us say I have a file system. So as an operating system, one of the services that an operating system provides you is a file system. What is a file system? A file system is an on-disk data structure, right. So a disk is nothing but a raw magnetic device which has lots of locks. And a file system is a data structure built on top of this sort of storage and the semantics of the file system are usually a file system is hierarchical. So you have a root directory and then you have some names and each name may be a file or another directory and so on. And so you basically build a directory tree and that is basically what a file system is. Now you can imagine that there are multiple processes running in the system. Multiple processes are making multiple system calls concurrently. So one is calling read, another is calling write. On different files, on same files, all these are possibilities. The question is the operating system needs to synchronize or make sure that accesses to the file system are correctly done. And basically, it basically means that operations should be atomic. So if there is an operation going on here and an operation going on there, they should not appear interleaved at any point because interleaving of those operations can cause bad things in your file system. So one option is, once again, coarse-grained locking. Put a lock on the entire file system, you're safe. Definitely safe. But of course, that's not a very good solution. You can imagine that your system will run at very, very slow speed. Nobody will be able to access the file system concurrently. Only one person will be able to access the file system. What do you do? Once again, choosing what locks to take is a bit of an art. You may say, let's have a lock per directory. Or you may say, let's have a lock per file. Or you may say, let's have a lock per, just very hypothetically, let's have a lock per pair of files. If you figure out that most of the operations are actually occurring on pair of files, so why not have a lock in sensation for a pair of files? And if you're going to do an operation between those two files or something. But in that case, if you're going to touch one file, then you're going to take all the locks for that file where that file belongs to a pair. So for all the pairs for that file, you need to take a lock. So that doesn't make a lot of sense. So yes, I mean, intuitively, it seems like the best thing to do is basically take a lock per file. So what I want to show you is basically, if you do this kind of fine-grained locking, it hurts your program structure. So the program structure does not, the modularity in your program actually reduces because of this locking behavior. So let's say, because of fine-grained locking, basically. So let's say I have a function which looks like this. It says move. So it's just moving a file from one directory to another directory. So it says move this file name called old name from old directory and put it as new name in new directory. So that's the semantics of this function. And what it does is it basically looks up the disk block. So let's say I number is a disk block or some identifier which is identifying the number at which this file is stored. This looks up the old name in old directory, deletes old name from old directory, and adds new name to new directory at that I number that you looked up. And so this code is correct, let's say, when you're running serially, when there's only one thread that's accessing it. This code is also correct if you're having one big global lock that's protecting this entire function. But let's say I have per-file locks, or per-directory locks. So let's say I have per-directory locks. And so what do I need to do? I'm accessing the old directory, reading and writing the old directory here. So I need to lock this region where the old directory is locked. And I'm adding something to new directory, so I need to lock this region with the new directory's lock. But can I do these in isolation? Well, no, because I want, perhaps I want my move operation to be atomic. If I just say that, oh, let directory delete do the locking inside it, and I don't care about what locking it does inside, and then let directory add do the locking inside it, and I don't care, then what happens is at this point here, no locks are held, and anybody is free to observe these directories or the state of these directories. And at this point, what you're going to find is that this file doesn't exist anywhere. And so the file system is in an inconsistent state at this point. So there are some disk blocks that are not pointed to by anybody, neither by the old directory, nor by the new directory. And that's an inconsistent state. In other words, if you do it in that way, the move operation is not atomic. So what would you want? You would, again, want to do basically something like this. You would say acquire old data.lock and acquire new data.lock. And then you will do this operation, and then you will just release these locks. So what has happened is basically because of fine-grained locking, any function that is building upon these... So earlier, it was very modular. A move function could have been written in three lines and without having to worry about what these functions are doing inside, whether these functions have to take a lock, not take a lock. That's not my business. I just call these functions. But now, because I'm doing fine-grained locking, now it is my business to know what locks are they going to take. And in fact, instead of asking them to take them, I'll need to take them on their behalf, and I'll need to take them in a certain order. So in other words, basically what I'm saying is locks and modularity are sort of... So locks basically hamper modularity. So locks are not very friendly to modularity. They make your code more complex, less modular. Earlier, you could just say that this function is going to do delete. This function is going to do add. I don't care what it does internally. But now you have to worry about, oh, this function is actually going to need to take a lock, and this function is going to need to take a lock. And because I need to do this atomically, instead of them taking a lock, let me take a lock on behalf of them. And now, because I'm taking a lock, they shouldn't be taking a lock, and so on. So the entire semantics of your function has become complicated. The semantics are not just that this function is going to delete a name from the directory. The semantics now need to be this function is going to delete a name from the directory, and it should assume that a lock has already been taken, and it should not be taking a lock itself. So I mean, locking and fine-grained locking especially sort of complicates things. So let's look at locks and locks implementations in a little more depth. So we said that how are locks implemented? One of the ways we implemented locks in a couple of lectures back was a spin lock, and where we said that there's a function called acquire. And let's just take struct lock star l, let's say. And it just says while. And let's say this is a function which is internally calling the exchange instruction. So I'm calling the exchange instruction. And so one way to do this is let's say there's a register which I put a value 1 into, and then I say while exchange register address of the locked field in l is equal to 1. I keep spinning, otherwise I return. So just read this code once more. Basically what I'm doing is I'm basically trying to put the value 1 into the locked field of this l. So I want to put a value 1 into the locked field of the l variable, except that I want to make sure that earlier it was 0. If it was earlier 1, then I should be just waiting for it to become 0. So that's basically acquire. That's the semantics of acquire. And here's how I'm implementing it. And we've seen it before. So I put the 1 value in r. And this function is going to atomically swap r and this memory location l.locked. And so if l.locked was 0, r is going to become 0. And so you're going to come out of the loop. But if l.locked was 1, then r is going to remain 1, and you're going to retry making it retry it until you see a 0 value in locked. And we also talked about last time why this implementation is anatomic, or it's worse because if two threads try to call exchange simultaneously, one of them will occur before the other. They cannot get interleaved. So the swapping operation is atomic, basically. So everybody remembers this, right? All right. So let's see what happens at the hardware level when you execute something like this. And just for completeness, let me just also write release. Release is just l.locked is equal to 0. All right. So let's see what's happening at the hardware level. So let's say here's my bus, right? So we've seen this diagram before. I basically always draw a bus here. And I say that here's my CPU. And let's say this is CPU 0, and this is CPU 1, right? And let's say this is memory. And inside the memory, there is this variable called l.locked. And in the CPU 0, there are private registers r, right? What each, let's say both the threads are executing simultaneously on CPU 0 and CPU 1. This thread is going to set it to 1. This thread is going to set it to 1. Both are going to say exchange. One of them is going to win. Whoever wins gets the lock. The other one just spins. That was the idea. Typically, you must have studied in your operating system class or in your computer architecture class that every CPU also has a cache, right? So let me just hash. My first question is, when I call the exchange instruction, is it OK to just exchange from within the cache? So l.locked is just another memory location, right? And so when you access it, it just comes into the cache. And can the exchange instruction just do the local operation without having to go on the disk, on the bus? No, because the exchange operation is an atomic operation. And there needs to be serialization between who is doing this. So there has to be some communication on the bus. Either the communication has to be directly with the main memory, or they have to talk with each other to basically make sure that there is serialization. Either he wins or he wins, right? So one of them is going to get 0, and the other one is going to get the answer 1. Both of them cannot get the answer 0, basically. And so there has to be some bus protocol here that has to happen here. And so each exchange instruction will require some bus transaction, right? In general, memory accesses don't necessarily require bus transaction, right? Whenever I read or write a value, if the value is found in the cache, I can just locally satisfy it from the cache. It's only when there's a cache miss do I need to go to the memory, right? And typically, you know, these processors have what's called a cache coherence protocol. So the idea is that, let's say, I access the memory location A, and it gets cached here. And then this CPU accesses the memory location A, then, you know, there's some protocol that's going on here, which will invalidate this location, and then bring it here, right? So, you know, if both these CPUs are accessing the same location, then there'll be some bus transactions that are shuttling this variable between these two, right? In any case, you know, when we are doing this exchange business, then the problem is that there's a lot of bus traffic, basically, going on. You know, if there are two CPUs, there's a certain amount of bus traffic. If there are four CPUs, there's more. If there are eight CPUs, there's even more. If there's 64, then, you know, basically, bus is definitely the bottleneck. There's a question. So cache coherence protocol is for every memory access, all right? So for every memory access, clearly, I mean, you cannot have… so the hardware ensures that, you know, there is some sort of… so that's what coherence means. So there's coherence in accesses. It cannot be that the same location has two values, basically, at the same time. So for every memory access, the cache coherence protocol works. It need not work… so if the same CPU accesses the same location ten times, it's only the first time that there will be a bus transaction. The next nine times, it will get satisfied from the local cache, without any bus transaction, without any cache coherence protocol getting… having to kick in, because assuming that this other CPU is not accessing that location, that location is locally satisfied from the cache, all right? But if you are executing the exchange instruction each time, then you have to make a bus transaction, because it has to be atomic with respect to everything else, right? So in the code here, let's say if there are, you know, if there are two processors, one of the processors gets the lock, the other processor just keeps calling exchange, and the exchange… each exchange, execution exchange, is calling a bus transaction, and so there's a lot of bus traffic, okay? So this is not the best possible implementation of a spin lock. And how can you make it better? Well, one way to make it better is, for example, put another loop here, which is not using an atomic exchange… exchange operation, which is just checking. So exchange instruction is a more costly operation, because, you know, it needs atomicity. On the other hand, this operation is just a read operation. A read operation is a less costly operation. It doesn't need atomicity, right? And so what I'm doing is, I'm basically, you know, instead of every time calling the expensive operation, what I'm basically doing is, I want to… I want to wait for logs to become zero, right? And so instead of doing it in… but I'm doing… I also need the exchange instruction, because I want to sort of swap it atomically. So the checking code can be… can be done through just reading, and then once you… once the read says, yes, it has become zero, then you can retry the exchange operation. It's not necessary the exchange operation will succeed, but it's a high likelihood that it will succeed this time, right? If it doesn't succeed, no problem. You again come back here, and you again wait for it to become zero, right? So what will happen in this case? Let's say, you know, both CPUs try to do exchange. One of them wins. The other one just calls the loop, and this time that the inner loop is going to get satisfied from the cache, right? So the inner loop is going to get satisfied from the cache. You have reduced the bus traffic. All right. All right. So this is all good, but let's see what happens if you write code like this, you know, without having to, you know, let's say you write this code in C. You just say while exchange, and then the inner loop is while locked. You know, a compiler is basically looks at these variables and decides which of these variables to register allocate, and which of these variables to keep in memory, right? So what happens if this variable becomes register allocated? So I hope people understand what is register allocation of a variable studied in the programming languages class or, right? So basically the idea is, let's say, let's say there's a variable called A, and say A is equal to one, you know, B is equal to A plus two, and C is equal to two, three into A or whatever. And so the question is one way to deal with A is basically say that keep it in memory, and each of these operations are memory accesses. And this other way is basically read A into a register. So let's say there's a load instruction, and you read A into a register, and then you perform all these operations on R. So you say, you know, R plus two is equal to B, and C is equal to three into R. And let's say you also say A plus plus. So you say R plus plus. And then later on, you can say store R to A, right? So this is, this is a common optimization, a very most basic optimization of a compiler, that if there is a memory, there's a variable, instead of variables are basically, you know, have a one-to-one relation with the memory location. But if there are multiple access to a variable, and the compiler can see that there are multiple accesses to a variable, the optimization is that you just bring the variable from memory into a register, do those accesses to the register instead of the memory, so you save some memory accesses. And then after you have computed the thing, you just save it back into the, into the memory, right? Common optimization of a compiler. Similarly, in this code, this variable L dot log, a compiler is free to register allocate. So what can happen if the variable gets registered allocated? It's an infinite loop, right? It'll never finish. So, you know, with the best of the intentions, compilers are not really, you know, playing well with what the operating system designer really wants. And so, you know, either, either the operating system designer writes this loop in assembly, or actually the compilers give you special keywords to basically say, oh, don't optimize this variable, right? So there's a, for example, in C, there's a variable called, there's a keyword called volatile. So if you declare a variable, you know, or a field with a volatile struct, or with a volatile type, then basically the compiler says, oh, this is something that, you know, the programmer has really written very carefully, I shouldn't be optimizing it at all, right? So, you know, just an interesting example of how, you know, how a compiler writer, so a compiler writer does not worry about concurrency and does not need, does not understand which one is, what is a log and what's not a log, etc. He's just looking at code, and he's just, you know, optimizing it. But, you know, if you're writing the special code like this, you should basically, basically declare things as volatile. And this is one of the reasons why, you know, it's difficult to, difficult to get concurrent programs correct. Notice that, you know, it's easy to basically say that acquire, write this acquire function very carefully, and then use this acquire function to mark critical sections. But on the other hand, if I didn't want to use logs, and I just wanted to very carefully write this sort of code, then I had to worry about, oh, the compiler shouldn't optimize it, and, you know, other things like that. And so that's a very hard thing to reason about in general. All right. The other thing a compiler and even the hardware does is reordering, right? So if, if I basically have an instruction that says A is equal to 1, and then I say B is equal to 2, a compiler is free to reorder these instructions, right? For a compiler, these are completely different memory accesses, completely different variables. It doesn't matter which occurs first, right? On the other hand, if you look at our locking code, you know, reordering is fatal for our, for our logic, you know, because if we are writing to the locked field, and then we are accessing some shared variable, if the compiler reorders these things, then, you know, the critical section is outside the lock, or before the lock, and bad things can happen, right? So similarly, it's possible that the release, the, the, the, the, an access in the critical section is reordered after the release, right? So for example, in this, in this case, L dot log is equal to 0. Before this, I had a shared variable access, you know, these two are completely independent memory accesses, and, you know, a compiler may say, oh, let's just reorder these things. It's not just the compiler who can do this, it's actually even the hardware that can do this, right? So most, so modern hardware basically do out-of-order memory accesses, right, even the Intel architecture, and most of the performance they get are basically because of out-of-order memory accesses. And the reason you need to do out-of-order memory access is because some memory accesses are going to take a long time, and others are going to take a short time, because some memory accesses may be cache hits, and others may be cache misses. So if whatever is a cache hit, you know, let's just do that first, and whatever is a cache miss, you know, let's let it come whenever it, when it's ready, right? So even the hard, even if the compiler played well with you, the hardware can actually reorder accesses. So it's possible that the locked access, locked variable was in cache, and just sort of got, you know, got set first, and later on, the other shared variable is getting set. So once again, you have to be very careful in doing this. And so, you know, modern processors provide what are called fences, right? So you basically put a fence, and the fence is basically saying that all memory accesses before the fence should have finished before any memory access of the, after the fence starts, okay? So the idea, you know, from a hardware designer standpoint is that in general, let's allow reordering of memory accesses, reordering of unrelated memory accesses, of course, which seem unrelated at least. But a programmer has a way of saying that, okay, here's a memory access, and here's a memory access. So in this case, if I want to disallow this, so let's say I want to say that this is, this should not be possible, then I'll put a fence in the middle. So that's, you know, so there are multiple ways of putting a fence. It's very architecture specific, you know, you have special instructions, which you can say that, you know, there's a fence, there's a fence instruction, so you can put a fence instruction, so that way this will, will get disallowed. Or there's special instructions, like the exchange instruction itself acts as a fence, right? So some instructions will never allow reordering of, across themselves. All right. Okay, good. Now, all right, so, so let's look at this, this implementation again. So what I'm saying is that the exchange instruction itself is acting as a fence in the case of acquire. And in the case of release, the programmer should put a fence in some way or the other, right? Either a fence instruction, or instead of using a simple write, use some exchange instruction to do the write, for example. All right. Okay. Now let's say, so this is a spin lock, right? And let's say I'm an operating system developer, and I basically also get interrupts. So these spin locks will protect against multiple, concurrent accesses by multiple CPUs. But if I am within, let's say I'm, I'm within, within a critical section, and an interrupt comes, the interrupt handler will get to run. And if the interrupt handler also needs the same lock, then there are problems. Right? You can either end up with a deadlock, or, yeah, so if, if you're doing it like this, then if, if I am within a critical section, and, and I'm holding a lock, and it's possible that the interrupt handler also wants to get the same lock, then I'll have a deadlock. Right? So, and, you know, the most, the core of the operating system typically has such code. For example, there's a lock to protect the process table, P table, in x86, for example. So that lock is, you know, is being accessed by multiple functions. And even the interrupt handler, the timer interrupt handler is going to need to access this P table lock. Right? And it's going to need to access the P table, and it's going to need to acquire the P table lock. So such locks are, you know, are even more special. And so what you do is, in that case, you basically make sure that not only do you just do this, you also disable interrupts in your acquire, and you're re-enabling interrupts in release. Right? So, so you, you know, when you acquire a lock, any lock, if you know that these locks can be acquired by, or can be requested by interrupt handlers, you also disable interrupts. So within the critical section, an interrupt is not possible anymore. Right? It's only when you release, you quit the critical section, will an interrupt get, get in the way. And yeah, so, you know, on x86, you'll find a function called, instead of just CLI and STI, you will find PUSHCLI. And instead of STI, you'll find POPCLI. And the idea here is that it's possible that, you know, you are trying to acquire multiple locks. So let's say you acquire P table lock first, and then you acquire file system lock second. And both of them wanted to, you know, both of them need to do CLI. But then let's say you release one of the locks, then you don't want to immediately do STI. So basically, you have some kind of recursion. So each CPU, so there's a CPU pointer dot, so there's a, there's a CPU dot NCLI variable. And so PUSHCLI just does CPU dot NCLI plus plus. And if CPU dot NCLI is equal to one, then you actually call CLI, right? Which means you just transition from zero to one, so you actually need to disable interrupts. And similarly, POPCLI, so that's PUSHCLI, roughly speaking, and that's POPCLI. So POPCLI is basically CPU pointer dot NCLI minus minus. And if CPU dot NCLI is equal to zero, then STI. Right? Yeah, question? Sir, CLI can only be applied if the user wants to acquire a lock. Right, right. So clearly, I'm talking about within the operating system, where the interrupt handler can require, would need to acquire the same lock that you are holding, right? Only in that case, you need to disable interrupts. And I'm really talking about the real inner core of the kernel. Okay? Clearly, so, so, so, you know, so, so for example, when you see implementation of the spin lock in the XP6 kernel, and the spin lock is basically used for your P table lock, and among other things, you will basically find that the acquire function not just does the exchange to protect against other CPUs, it also does a CLI to protect against interrupt handlers. Okay, so you need to do both these things. All right. Okay. So, and also, this NCLI variable is a CPU private variable, right? So you, there are ways to say that this variable is only going to be accessed by this CPU. And so no other CPU can, will ever be able to access that variable. Or you can just have an array with, you know, first, where each element is accessed by only the corresponding CPU and nobody else. So that's a per CPU variable. Right? So, so let's say I'm in the user mode. Okay, so I've talked about kernel mode. But let's say I'm in the user mode. And I want to do, I want to implement my let's say banking application. And, and I want to implement locks. So what kind of locks should I use? Well, firstly, the question could be, you know, whether I'm running on a multiprocessor or a uniprocessor. Or in fact, even before that, the question should be whether you want to implement a spin lock or a blocking lock, right? So if you want to do a spin lock, do you need any kernel involvement? Unless you want to disable interrupts, would a user level lock need to disable interrupts? I said you need to disable interrupts only if that lock could be requested by an interrupt handler. I mean, assuming that the user level locks are just private to the user, and the kernel has nothing to do with it, then the interrupt handler has nothing to do with that lock, right? So you will not need to disable any interrupts for a user level lock. So can you do implement a spin lock without having any kernel involvement? The answer is yes. Okay, all you need to do is declare a variable and use the exchange instruction. Exchange instruction is an unprivileged instruction. It just has the semantics that things will be atomic. That's all, right? So the same code that I showed you, this one, without the CLI, implements a spin lock in user mode. So a spin lock in user mode is as fast as a spin lock in kernel mode. You just basically, you know, try to atomically set it to one. And if not, you just spin just in exactly in the same way. And hopefully your critical section was small, and you will immediately get the lock. All right. Does it matter whether you're using kernel level threads or user level threads? Because user level threads will only run on a single CPU, you don't even need to do this exchange business, right? User level threads will only run on a single CPU. And so instead of using a spin lock, you would probably want to use a blocking lock instead, right? And of course, so blocking locks will be used either if you're using user level threads, or if you're sure that, you know, your threads are not going to run on a single CPU, for whatever other reason there could be. And if your critical sections are known to be very large, right? For example, if you're making a system call while holding that lock, you might as well just, you know, use a blocking lock rather than using a spin lock, right? So in all these cases, you will not use a spin lock, you'll use a blocking lock. Do you need kernel involvement to do blocking locks? To implement blocking locks? Yes, because a blocking lock basically needs to tell the kernel to change my state from ready or running to blocked, right? And so I need, and the user has no way of changing it from ready to blocked. And so it has to tell the kernel to do it, right? So there has to be a kernel interaction. Unless, of course, you were using user level threads, in which case the kernel has no idea. And so you're, in that case, your user level scheduler is just going to, is just changing the state of your, you know, currently running thread to, from ready to block. So in that case, your P table is maintained at the user level. So in either case, the P table, the state in the P table needs to be changed from ready to blocked. If you're running kernel level threads, you need kernel interaction to do that. If you're running user level threads, you can just do that locally in the user. All right. So let's see how blocking locks are implemented. Right. So you can imagine that there is a P table, right? Or, you know, I'm using an array, but you could even have a list of PCBs or any such data structure that's maintaining all your process PCBs, process control blocks, right? And what you're going to do is let's say somebody says lock and he's not able to get the lock, then you will basically want to change its state. So let's say this is a process and this is currently running, then, and it calls acquire. You would want to change its state to blocked. And you will want to record that it's blocked on whatever was the argument of L, acquire. So let's say blocked on L, right? And then if somebody calls some other process, so this becomes locked. So this never gets to run in future till somebody calls release. So let's say here's a process that was running and then it's called release L. And what release is going to do is it's going to go over the P table, right? And pick up one process that is blocked on L, right? So here the Ls are matched and change it from blocked to running or ready, not running, but ready. It will change it from blocked to ready, right? So that's how blocking locks will be implemented. Okay. All right. So but this P table structure itself needs to be protected. Now access to the P table structure itself by multiple threads needs to be protected. So you will use a spin lock to protect the P table. And then, and so blocking lock internally will use a spin lock to protect this structure and to switch from running between running and blocked, these different entries, right? So there'll be a P table dot lock, let's say, which will be a spin lock. Well, I mean, will the P table dot lock only be needed for a multiprocessor? Well, you know, so on a uniprocessor, a P table dot lock equates to a CLI, you know, clear interrupt. Basically you want that while you are in the middle of accessing the P table, nobody else should basically interrupt you, right? So on a multiprocessor, you will use a spin lock. On a uniprocessor, you could do that just by disabling interrupts, right? Basically what you want is mutual exclusion while you're accessing the P table, right? And mutual exclusion on multiprocessor only way is spin locks. Mutual exclusion on a uniprocessor, the way is disabling interrupts, right? All right. Now let me talk about some locking variations. So there's something called a recursive lock. So you may have seen that sometimes we run into this situation where you acquire a lock and then you call some other function and that wants to acquire the same lock and at that point we deadlock, right? Because the same thread cannot acquire the same lock multiple times. So, you know, the recursive lock, basically what it does is it allows the same thread to acquire a lock multiple times, right? And the semantics of a recursive lock are fairly simple. Let's say this is recursive lock. Let's say this is a recursive acquire, L. You will say if L dot owner, you'll keep something called an owner, is equal to current thread in L dot count plus plus else, as you call the regular acquire, right? And you set L dot count to zero and L dot owner to current thread. So basically the idea is, you know, a lock is supposed to provide mutual exclusion between multiple threads. If for some reason the programmer feels that, you know, or for modularity or whatever reason, if he feels that the same thread wants to acquire the same lock multiple times, let's allow that, right? So that's the spirit behind the recursive lock. And of course, recursive release will just basically decrement count and only if count becomes zero does it release the lock, right? So that's what release means, done. So should I write release? So let me also write release. I'll just say L dot count minus minus if L dot count is equal to zero, then release L dot lock, right? Something like this, right? So this is a recursive lock. Sounds like a good idea or a bad idea? Idea. Okay. All right. So, okay, so so it's actually a bad, it's generally considered a bad idea to do recursive locks, all right? And why? Basically, usually the semantics of a lock is that when you acquire a lock, you know, at the point when you acquire the lock and you just enter the critical section, you can pretty much assume that the state is in a, there's a consistent state of the system, right? So the idea is that if you have been able to acquire the lock, anybody else who has released the lock has left the state, the shared state in a consistent state, right? Or has left the memory in a consistent state, right? That's basically, that's basically, that's basically been our invariant, right? That if I am able to acquire the lock, I can assume that at the first instruction of my critical section, the system is in a consistent state. And that other invariant I usually maintain is that just before I release the lock, I have ensured that the system is again in consistent state, right? And so then I release the lock so that the other person who acquires it will also see the system in a consistent state. So generally, you know, you, the assumption is that as you have acquired, if you have acquired the lock, the system is in a consistent state and you will maintain it in a consistent state before you release the lock or you'll keep it in the system. But if you do recursive acquire, then, you know, then it's possible that you have a function four that, you know, says, let's say I'm going to say recursive requires, I require L does something, makes it, makes it inconsistent, right? Makes a state inconsistent, hasn't released the log right yet. So he's going to say I'll release here, somewhere here, but he calls bar here, right? And bar internally is going to say I require, and he's going to start doing something, but he's going to assume, you know, assuming that this code has been written, you know, in modular fashion, in a different file or different program or whatever, he's going to probably assume that it's in consistent state or assume consistency. But because, you know, you're using recursive locks, you will, you know, you'll violate that assumption and this bug will be much harder to find. So in fact, you know, using recursive locks, you have made it easy for your program to have bugs that have not been, that cannot be found. On the other hand, if you didn't use recursive lock, you know, the first call to bar would have told you, oh, there's a bug in your program, right? Because there would have been a deadlock right there, right? So in general, you know, a programmer wants to keep his thinking simple and consistent with this idea that when you get a lock, things are consistent. When you release a lock, things are consistent. And if the programmer is really doing that, then recursive acquire is a bad idea. All right. Okay. Then there's another, another variation of locks called try locks. Okay. So what are try locks? Instead of, so the same thing, let's say, instead the, so, so, so the idea is that acquire L has a return value now, int, right? Which basically says, and you know, the release is just void. And the acquire basically says success or failure. So in our, in our regular lock, an acquire basically always succeeds or it waits. In the case of a try lock, you try to get the lock. If you didn't get it, you just return a minus one or a, you know, a failure value, right? And when you, and so it's, it's up to the caller to do whatever you like. Of course, you know, you can implement a regular lock using a try lock, you can just sort of put the try lock in a loop and you get a, get a regular lock. It may not be the most efficient way to do a regular lock, right? But, but the advantage of a try lock is it gives some flexibility to the caller. You may want to say, oh, let's try to acquire this lock. If I don't get it, oh, then I have something else to do. Let's do that first, right? And then retry it. So, so it gives him that flexibility. On the other hand, and the previous lock and acquire basically is committing that I'm definitely going to, I'm going to either do that or wait basically, right? So try lock gives you some flexibility into, you know, whether you want to, whether you want to wait or whether you want to do something else. Okay. So let's, let me now discuss a real example. So I hope you all know that the banking example that I took earlier was a very, very hypothetical example for many reasons. Firstly, you know, bank accounts are not maintained in memory. Secondly, you usually don't write code in such a way where you're going to do a global sum operation on all the accounts. You would want to do some kind of more distributed and segmented way of calculating some. And so that there's more scalability in your system or, you know, whether you want to calculate some at all, you can just, you know, update the sum as the transfer is going on or something like that. In any case, it was just an idea, a way of telling you, you know, what the problems of fine-grained locking are. Let's take a more real, real example of a web server. So what is a web server? A web server is, let's say, you know, this running on this machine, which has a disk and it has a network. And a client sends an HTTP request and receives a reply, HTTP response. And I mean, let's take a simple case where the HTTP request is a URL and the reply are the contents of that URL, which is an HTML page, let's say. How is a web server like this implemented? Well, well, let's say, you know, at a very high level, the web server is probably running a loop like this, while one, so while true, no, it's an infinite loop, read message from incoming network queue. Let's say URL is equal to parse message, right? Read URL file. So whatever the URL, you can parse it to get a file. So let's say, read the URL file from disk, and then write. So you get the URL file from disk, you get the contents of the file, and then you write those contents. So write, as a reply, you write the reply to outgoing network queue. So what am I assuming here? I'm basically assuming that there is a network queue right? There's somebody who is filling up this network queue, so there are packets being received on the wire, and those packets are getting stuffed into this network queue, incoming network queue. There's this server that's running, that's picking up packets from this incoming network queue, processing them in this way, and then there is an outgoing network queue, which you just, and the server is putting things into the outgoing network queue, and there's somebody who is picking things up from the outgoing network queue and putting them on wire, right? So let's see what is the performance of this web server, all right? So basically what will happen is, let's say there are multiple clients in this, let's say there are, you know, there are multiple clients that are accessing this web server, their request will get queued in the incoming queue, and you know, depending on how many clients there are, what is the concurrency level of clients, the queue will keep filling up, and the server will pick up one request and start serving it. So you know, the maximum number of clients that it can serve in a second depends on how much time it takes to execute this code, right? And how much time does it take to execute this code? By far, the most expensive operation in this is this, right? Reading the URL from disk is by far the most expensive operation. Okay. These operations are likely to finish in, you know, hundreds of nanoseconds to maybe microseconds or something, but this operation, URL from disk, is an operation that takes milliseconds to complete, right? Why does disk take so much time while the other things are so much faster? Have we discussed this before? No, okay. So let's say there's, so typically, today's modern CPU runs at one to, you know, let's say three gigahertz, right? Or roughly one to two nanoseconds per instruction. One to two nanoseconds per instruction. If the instruction was a memory access and the memory access was a cache hit, then also, you know, typical execution times are one to three nanoseconds. So cache hit, including cache hit. If it's a cache miss, then, you know, let me put approximately, and roughly 100 nanoseconds for a cache miss, or let's say main memory access. These are all electronic operations. These are just, you know, semiconductors exchanging electrons to basically access either cache or memory or things like that. The only reason memory is sort of more costly is because you have to travel a longer distance, you have to go over the bus, there's some bus contention that you have to worry about, and then you get to the memory, and then, you know, but it's all electrons traveling, and so it's very fast, right? On the other hand, you know, a disk access or a magnetic disk which has persistence is a mechanical device, right? So a disk, actually, if you look at a disk, then it's a mechanical device. So it's a mechanical device with moving parts. Exactly what is the structure of a disk, and why, and so, you know, hence it's much more costly, and it's on the order of, you know, five, or let's say, you know, one to 10 milliseconds. So that's 10 to the power, a million times slower than an instruction access, which means that accessing a disk operation, in that time, you could actually have executed a million instructions in CPU. So we're going to discuss how a disk is organized exactly, and what determines what the access time is exactly, and then what does it mean for a web server and its scalability, which means how many concurrent clients can it support, and how you can optimize it, and what role does multithreading have to play in optimizing it, right? So we're going to look at that. Okay, thanks."}