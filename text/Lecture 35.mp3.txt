{"text":"Welcome to Operating Systems Lecture 35. So we were discussing about logging as a way to implement fast cache recovery. We first looked at a very simple way of doing logging where every disk operation that needs to be atomic was considered a single transaction and there was a commit after every disk operation. And we said there are lots of problems with that and then we said but in practice you would extend this idea and we were taking the example of the exe3 file system on Linux where a transaction is lots of atomic operations bunched together into a single transaction. So in other words, with respect to power failures, either all these disk operations happen at once or none of them happen at once. So it is making one big atomic operation out of lots of smaller atomic operations. And if you do that, then firstly in the previous case we said that only one transaction is possible at a time. So even now there is only one transaction possible at a time but because one transaction can have lots of different disk operations, many simultaneous disk operations are possible at the same time. So transaction is no longer causing serialization across multiple disk operations where disk operations are completely different parts of the file system. Of course, if the disk operations are to the same part of the file system, then there is in-memory locking to ensure that there is no concurrency problem that we have discussed already, right? So there is buffer level locking to ensure that there is no atomicity violation by concurrent accesses to the same block. But here we are talking about atomicity with respect to power failures, right? And for atomicity with respect to power failures, we wanted to have a transaction and we wanted And we could only have one transaction at a time and so that was causing serialization. But because you have multiple operations in a single transaction, that serialization problem goes away. Also, writing to log is just one large sequential write and we know that sequential writes are fast. Assuming that you are doing, you would commit, so the way it works is you open a transaction and you close it after every periodic interval, let us say every 5 seconds or every 30 seconds depending on what kind of guarantees you want. And after that interval, you are going to commit the entire transaction which includes all the disk operations that have, disk writes operations that have happened during that time. And so one large sequential write is very fast. Whole blocks are logged still, so we said one of the problems with this kind of logging is that the whole block is logged. If you even modify one byte in the block, the entire block needs to be logged into the log and that is more space overhead in the log. But it is still present in EXE3. The only saving grace is that if there are multiple writes to the block, which is a very common case, you know, likely if you are, for example, creating lots of files in a single directory, then you are probably making lots of writes to the same directory inode and you know, hundreds of writes to the same directory inode. You do not need to create, log all those 100 versions of that block. You only need to log the last version of the block, right. So multiple writes to the same block get absorbed by the cache. Write to log is still eager. So when you commit, you have to commit, you have to write the entire log at that time. You cannot lazily say, I am going to commit it later because otherwise you completely lose your guarantees with respect to power failures. So it is still eager. But the nice thing is that these eager writes to the log are happening at very coarse in time intervals, 5 seconds or 30 seconds depending on what you choose. And so it is not that big of a problem anymore, all right. Similarly, log blocks are applied to the file system tree lazily. So the commit of the log, the commit of the transaction is eager. But the application of the log or of the transaction blocks to the file system tree is lazy, right. So you can just do it whenever you, you know, whenever you find time or whenever you find the disk to be idle. Or more importantly, you know, you can take a lot of blocks and try to write them simultaneously And so that gives your, gives the disk a lot of scheduling potential. So recall that if you do lots of in-flight IOs simultaneously, the disk can schedule it much better than if you have one IO at a time. So you can, and all these can be done simultaneously, so you have a lot of scheduling potential. So even though still you are having 2x writes, you are actually writing each block to the disk twice, the overhead is not 2x, right, because the first write is part of a large sequential write. So it is almost free. The second write is also not as expensive because you have a lot of scheduling potential because you are doing lots of in-flight IOs for application of, to the FS tree. So the number of seeks or latencies that you pay is still roughly 1x, right. Even though you are writing the blocks twice to the disk, the amount of overhead or the number of, assuming that you are counting overhead as number of seeks you are making to the disk is still 1x, right. So that is, so you are getting, so basically with logging, you have used the characteristics of the magnetic disk to ensure fast crash recovery, yet be able to maintain the same level of performance. The advantage of logging over the previous method that we saw, which was ordering, if I just compare logging versus ordering, as we saw the performance difference is not much. Logging is almost the same performance as ordering. In fact, it is a little simpler, you do not have to worry about, so you know, recall that ordering had this problem that some invariants will get, you know, some inconsistencies can arise. For example, free space, there can be disk block leaks or there can be, in the case of moving a file from one directory to another, it is possible that the same file is pointed to by two directories. So ordering had possibilities of inconsistencies. Logging does not have them. Logging ensures the commensity of the entire operation across crashes. Also the other biggest advantage is basically recovery, at recovery time, you do not need to do a full global file system scan, right. Recall in the ordering case, to check my invariants, I had to actually traverse the full file system in a global way and that was becoming very expensive and we said it can take an hour for the sizes of disks that we have today, which is hundreds of gigabytes to, you know, one or two terabytes, it can take up to hours to do this and that's becoming very impractical as we go along. But logging does not have this problem, right. You don't have to do a global file system scan, right. What do you need to do to recover from a crash? You need to scan the last few transactions in the log, right, and see which of them have not yet been applied to the disk, right. So we're going to look at how exactly the crash recovery works, but you can already see that you don't need a global operation, all you need to look at is the last few blocks of the log. Yes, question? Does the log exist in the file system? Yes, I mean log is part, log is a structure on disk and disk, yeah, you can consider the log as a separate file in the file system, but I mean it has special semantics, it's not your, you know, it doesn't have the same semantics. So for example, the user cannot see the log, right, so user cannot read or write directly to the log. The log is basically being written by the system, by the kernel. Sir, I am actually asking this because... So it depends on, so the question is if something gets corrupted while you are writing the file system structure, can we recover it by using the log? So I mean, in general, yes, but I mean it depends on what kind of corruption you're talking about. I mean the whole idea of a log is that you, so basically your file system now is equal to the tree, which is, you know, tree of inodes and logs and directories and so on, plus the log, right, so your tree can become inconsistent, but the total of tree plus log will never be inconsistent, okay, so that's the guarantee that, you know, if you look at this whole structure as one whole thing, tree and log, then, you know, the invariants across these things are always maintained. The tree itself can get inconsistent and that's why the, you know, if there's a crash, then the log can basically correct the tree, if it likes. Okay, so let's look at the EXE-3 structure, so there is an in-memory writeback buffer cache or block cache, okay, we already know this, we have seen this, this was present even without logging, you basically have a buffer cache, which you need and needs to be write back for performance. You have now an in-memory list of blocks to be logged per transaction, so as the disk operations are happening, you make, you're keeping an in-memory record of these are the list of blocks that belong to this, that have been dirtied, that have been modified and they belong to this particular transaction, so when this transaction commits, these are the blocks that need to be flushed to the disk, right, so you need to maintain this data structure in memory, right. And then, of course, you have the on-disk FS-tree and then you have the on-disk log. And this log is maintained as a circular buffer because, you know, the log cannot keep growing, you know, infinitely, so you basically just wrap around the log and you also keep freeing the log as you apply the log changes to the file system tree. We're going to see how, okay. So let's look at the EST3 log. So EST3 log, let's say, if I draw the EST3 log, this is my, this is not my disk, this is my log on the disk. So this is a part of the disk which stores the log, let's say, all right. And the log has, let's say, the zeroth block in the log is the log superblock. This superblock is different from the file system superblock. This is the log superblock. By superblock, I basically mean that it contains meta-level information about the log, right. For example, where does it start, right. So this will contain, you know, start offset and a sequence number. The sequence number indicates what's the current, what's the sequence number of the current, of the transaction that, that's the first transaction at the start offset, right. So the idea is that you have, you maintain a global sequence number and each time you close a transaction, you increment the sequence number for the next transaction. So one after another, the sequence, each transaction will have a new sequence number, right. So this is basically pointing somewhere here, the start offset and, and you can find the log starting here, right. And any time you reach the end of this area, you wrap around like this, okay, all right. What does the log have? The log has two types of blocks, descriptor blocks and data blocks. The descriptor blocks basically say that the next block, so it just describes the next block, for example, or the next few blocks, basically says the next few blocks are for, the next few data blocks are for block numbers X, Y, Z. And so these are the data blocks. So for example, you can say here's a descriptor block which says number 10, number 100, and so on. And then after that you have the blocks, the respective blocks, the contents of the respective blocks, right. So there are two types of blocks. There's a descriptor block and there's a data block. The data block contains the full data and the descriptor blocks describes what data it contains, right. Are the descriptor blocks similar to inode instruction? No, not at all. Descriptor blocks are just something specific to the log, which just says that, you know, so basically the log contains all the dirty blocks, right, that need to be applied to the file system. So the descriptor block just says that these are the, you know, the next few blocks are data blocks and these are, these data blocks are the contents of block numbers X, Y, Z. So you know exactly where to apply these blocks, that's all. Okay, so the descriptor blocks contain information like block number, right, so which block it is. We already discussed this. It also contains information about sequence number. We're going to see why it needs it. So every descriptor block contains a number which says which transaction do I belong to, right. So the sequence number is the same as the sequence number in the block and we're going to see why it's needed, why do we need a copy of the sequence number in the descriptor block. And finally we have something called a magic number. By magic number it basically means that there's some number there which says that yes it is a descriptor block. It's some identifier. So it's not some random block. It is, if the magic number is present you can be sure that it's your log block. It doesn't have any garbage in it. So it's just an identifier which says it's a block of EXT3 log. And so in general the magic number is used in many different places and the purpose of a magic number is to identify or to disambiguate a valid block or a valid piece of data from completely garbage data. And then so there's a descriptor block, there's a data block and then there's a commit block. It basically says that at this point this is where the transaction committed or not. And the commit block contains, once again it contains the magic number and it contains the sequence number of the transaction that it commits. Okay, so just to review, how does the syscall work? It basically says, let's say I wanted to make a disk write, so I'll say H is equal to start. It's like start transaction as we had discussed last time or start operation as we had discussed last time. And then you're going to say, you know, get H block number to indicate that you are dirting these blocks. You're going to write to these blocks and then you're going to say stop. So this is how you basically say that this operation should be made part of the current transaction. And this operation should be atomic with respect to crashes. So it's basically, so all you have to do is basically look at areas where you're doing multiple disk writes and identify points where you want to say that these multiple disk writes should be atomic with respect to crash failures. And then you put a start and a stop around it, right? You know, one simple thing to do could be that you put a start and a stop around at the start of the system call and at the end of the system call, always. That's one safe option to do. So this has a lot of similarity with transactions as we have discussed earlier. You say begin transaction, end transaction, and it's the runtime system that takes care of what are the things that you modified and whether there were any conflicts or not, etc. Similar thing here, right? Because they're transactions, they're not locks, you don't have to worry about fine grain versus coarse grain. You don't have to worry about deadlocks and things like that. So you will get all the advantages of transactions, right? Okay. So let's see. So the question is, you know, in these transactions, it's possible that multiple transactions are happening at the same time and if there's a discrepancy, then you roll back. Notice that these things here are not what I'm calling transactions. These are, the terminology I'm using for these are atomic operations. And there are multiple atomic operations that can go on concurrently and they're all part of a single transaction. Atomic operations do not have any conflicts with each other. So we are not talking about, you know, so one atomic operation and another atomic operation, as long as they're touching different buffers, have absolutely nothing to do with each other. But we want to make sure that, you know, they're atomic with respect to crash failure. So here, atomicity is not with respect to each other. Here, atomicity is with respect to power failures, right? And so you can, you know, so the way you deal with it is different. So now you put lots of, you allow these atomic operations to execute concurrently, but you put them all in a single transaction and that transaction is made atomic with respect to power failures. That's what you're doing. So let's see how, what happens on a crash. So it may interrupt the last transaction while it was writing, let's say, the transaction has decided to commit. So it's writing the transaction to the log and before it could write the commit log, the power went off. So that's possible, all right? So it's possible that you have many full transactions. When you recover, at recovery time, you see many full transactions and one partial transaction in the log. Also, you may see that the FF tree has been updated partially, right? So when you recover from a crash, these are the possible things that can happen. You can, you know, you can see lots of full transactions and you can see one partial transaction and you can see that the FF tree has been partially updated, which makes it inconsistent, but you know, together with the log, you can, you know, the entire structure is consistent. So, but what's the other thing? Can it be updated partially? Can it be updated with the transactions that have not yet completed? The FF will only be updated partially, but only for blocks belonging to full transactions. So the invariant we are following is that after, only after we commit a transaction to the disk, do we start applying that transaction's blocks to the tree? So the one that's partial, none of it is applied to the file system, you can be sure of that. It's only the ones that are completed whose blocks have been applied to the file system. And so if partial blocks have been applied to the file system, no problem. You can, you know, because application is completely idempotent, so you can reapply them, these are the same contents that you're writing again, so all you need to do is look at all the full transactions and reapply all of them to the file system tree. And completely ignore the partial transactions. It's as though the partial transactions didn't happen at all. Right? So what kind of semantics is this giving the programmer? It's basically saying that if you, for example, create a file, and there's a crash after, you know, even though the create system call returned, and after that you have displayed something to the user saying, you know, I have created the file, et cetera, and there's a crash, and then you come back up again, there's no guarantee that the file is actually on the disk. Because the transaction which contains this particular create operation may not yet have committed. Let's say it's committing every 30 seconds, then that's not committed. But given that you have some reasonable interval for the commit transaction, you can be sure that something that you wrote, you know, a few minutes back or one hour back is definitely on disk. All right? So let's see, how does recovery work? So first, check superblock, okay, to get offset and sequence number. Then can starting at offset until bad record. So basically, at recovery time, I look at the superblock to get the start of the transaction, and then I just keep scanning forward until I find something that's bad. What do I mean by bad? No. So by bad, I mean I see a block that does not have the same sequence number that I'm expecting. Okay? So how do I know that something has not committed? Basically I'm going through all the blocks, and I'm looking at the sequence number of these blocks. Right? So how can you see a sequence number that's older? Right? So that basically means that this transaction couldn't write all the blocks. Okay? So the sequence number is disintegrating between blocks of the current transaction, and blocks that may have been written earlier. Recall that it's a circular log, right? So, you know, there could be some writes that are living from the previous log. You just scan the log until you see something wrong. And something wrong could mean a bad sequence number, or it could mean a bad magic number. So it's possible that, you know, the log had completely garbage content in the beginning, and then you basically, you know, started writing. And now the sequence number is somehow correct, but the magic number, you know, the assumption is the magic number cannot be correct, if, you know, this is some garbage. The probability that the magic number is also correct is very, very small, basically. Okay? So you basically start here, and you keep scanning until you find the first record that's bad. And then you scan backward from there to find the first commit record. So you just go like this to find something which is bad, and then you find the first commit record after this, before this. And these are the transactions that you want to apply to the file system tree. And when you apply these transactions to the file system tree, you can free these blocks. What does freeing the blocks mean? Update the start offset. Right? So basically, as you apply the transaction, you also update the start offset to point to the next transaction, and so on. That's basically how recovery works. So notice that, once again, if there's a crash, you identify a bad block by either a bad sequence number or a bad magic number, and that indicates an incomplete transaction. And from there you go back to figure out what are the last full transactions that you saw, and that's the one you apply to the file system. And once you apply it to the file system tree, you also free up the space by advancing the start offset. And it's a circular log, so it just wraps around, and so you basically keep on using the log. It's possible that, you know, while you were, while the transaction was executing, the log is completely full. Right? So let's say you were executing the transaction on the log, you were writing disk logs on the log for this particular transaction, let's say it was a system call that involved deleting many files. So lots of different writes are happening, and so you basically have a large transaction, and this transaction is not being, is not fitting in the log. What do you do? Well, firstly, if the size of the transaction itself is larger than the size of the entire log space that you reserve for this purpose, then there's nothing you can do. So you should, and the programmer should ensure that the size of a transaction can never become larger than the size of the log. Either the log should be allocated large enough, or, you know, the size of the atomic operations that you're taking should be limited, should be bounded to some size, number one. Number two, but still it's possible that while you're doing something, so, you know, you hit the end of the log. And clearly, you know, you hit the end of the log if you ever, you know, reach the start offset again. You'll tap around and you'll reach the start offset again. It's very easy. In memory you can figure out that you've reached the end of the log. So what you can do is you can just commit the previous, free the previous transaction. Not commit, but they're already committed, but you free the previously committed transaction. By freeing the previously committed transaction means you apply their contents to the file system tree and you advance the start offset so that the previous, there's more space that's freed up in the log. Why do we have the start offset? To indicate the start of the log. Why isn't the log starting at log number one? Because it's a circular log, right? So I want that, you know, so there's a producer to the log and there's a consumer from the log. And so it's a circular buffer, so you need to store both head and tail. So let's say you just had a log which always started from one, right? So now if I wanted to free a transaction, it would mean copying the rest of the log all the way. So it would have been a global copy to block number one, to free a log. So let's say I wanted to free the first transaction in the log, right? And I have to copy all the transactions starting from transaction number two. That's a very expensive operation, so the better way to do it is maintain it as a circular buffer and have a head pointer, which is a start offset, and a tail pointer, which is a, there's no tail pointer here, recall, right? You're using, you're finding, you're inferring the tail pointer using the sequence number and the magic number. Okay. Right? Okay, so a scan starting at offset until bad record, scan backward to identify last committed transaction. Or, you know, this is a full transaction, this must be a full transaction. And then you can apply those transactions and completely discard the last transaction. So even without recovery, there should be an asynchronous thread that's freeing up space on the transaction log, right? So the asynchronous thread works similarly. It starts at the start offset and looks at the first transaction, you know, so how does the asynchronous thread work? It just looks at the start offset and looks for the first transaction and frees it up. By frees it up, it basically applies that transaction to the file system. If it finds that that transaction is really small, it can club multiple transactions and free all of them together, right, to have better scheduling. So this is the kind of freedom it has to get better performance, right? So let's look at, so as I've discussed so far, we are logging the entire file system, including the inodes, the free block lists, the bitmaps, and the data blocks themselves, all right? Let's look at, you know, what kind of semantics does it give us. So let's say there were two operations, echo i greater than x, and then there is ls greater than y, right? So you execute two commands, one after another, and let's say they execute concurrently. So, you know, let's say I put an ampersand operator here so they can execute concurrently. There is no sequentiality here. And so basically, if there is a crash, the consistent, so I should either see that y is, let's say initially the directory, the current working directory is not there, so either I should see y is empty and, so what I shouldn't see, let's see, is that y has x written into it, but they could, an inconsistency could be that y has x, but x does not exist. So this would be an inconsistency. I created a file in the directory and then I did an ls to read the content of the directory and wrote it to another file y. And, you know, if there was no crash, then definitely you will always see consistent behavior because, you know, all your answers, all your answers that you're getting, what exists in the file system and not, are being served from the buffer cache. But if there's a crash, then, you know, you shouldn't see this kind of behavior that y seems to have seen the file x, but the file x does not actually exist. So there should be some serialization that basically says either the file x does not exist and y does not contain x, or the file x exists and y contains x. But it shouldn't be that the y contains x, but the file x doesn't exist, right, so that would have been an inconsistency. Now let's see whether this can happen in the exe3 transactional system that we have discussed so far. Okay. So let's say these two operations are a and b and they are happening concurrently, so and we basically want to make sure that this kind of an inconsistency does not happen, so we don't like this. So let's say, okay, so let me just, let me just write it here, a echo x, b ls, so let's take cases if a and b belong to the same transaction. So my question is if a and b belong to the same transaction, can this inconsistency happen on a crash? I see some people nodding their heads, it cannot happen, right, why cannot it happen? Because if they belong to the same transaction, either the transaction would have committed, in which case you would see hi in x and x in y, or the transaction wouldn't have been committed in which case you wouldn't see either, so you'll never see this inconsistency, right, either the transaction would commit, in which case you will see both the updates, or the transaction will not commit, in which case you will not see any of the updates. It won't happen that one of the updates is seen and the other is not seen, so if they belong to the same transaction, there's no problem. Let's say, case 2, a in t1 and b in t2, where t1 commits before t2, right, so a is in t1 and b is in t2, so in this case, you write, so if the crash happens, it's possible that t1 has committed, but t2 hasn't committed, in which case what you'll see is that the file x has been created, but the file y hasn't been created at all, right, so that's also fine. So there's no inconsistency of this type, that y has been created and it seems to have x, but x does not exist, so that's also not a problem at all, right. So basically if a is in a transaction t1 and b is in a transaction t2, there are a few things that can happen, either at the crash time, none of them have committed, in which case none of these updates have happened, or it's possible that both of them have committed, in which case both of the updates have happened, or it's possible that t1 has committed and t2 has not committed, in which case a's update has happened, but b's update hasn't happened, in which case, you know, the y hasn't been even written to the file system, right, so there's no y, but x has been created, but y hasn't been created, so that's not an inconsistency. The inconsistency was y has been created with the contents which are inconsistent, so even this is okay, right, so let me just say okay and okay. Now let's say, is it possible that a is in t2 and b is in t1? So a is in t2 and b is in t1. Can this inconsistency happen in this case? Okay, so there's an answer, yes, right. So it's possible that y is in, so because now if there's a crash, and so once again, if there's a crash, then either none of them have committed, in which case we are okay, or both of them have committed, in which case we are okay, but if t1 has committed but t2 hasn't committed, then y's contents have been written to disk, but x's contents haven't been written to disk, right. But you may say, you know, if y's contents have been written to disk, but y must not have seen x's update, right. So how can this happen? It can happen if, let's say this was b and this was a, so b started first, right, and then a started later, so this, and here was the close point, so the transaction got closed after this, so this, b's transaction gets into t2, t1, and a's transaction gets into t2, right, because there was a transaction closure between these two starts. Recall that when you close a transaction, then all the existing transactions, all the already opened operations are completed, and all the future operations are made part of the next transaction, right. So if b's start happened before a's start, then b could be in t1, and a could be in t2. Now a could have, you know, made its update, and then stopped, and then b, okay, let me write it on a different page, so let's say this is b, a, and this is start, and this is start, there is a dotted line which says close, transaction close, and so this is part of t1, and this is part of t2, and then this updates, creates, creates x, and then stops, and then this says, creates y, writes x to y, and stops, so even though b could see a's update, b happens to be in a transaction earlier than a, so in this case b could see a's update, but because b started before a, b is in a previous transaction, so now it's possible that at this point I commit t1, and you will see b, you will see the file y containing, the contents x, but the x, but because t2 hasn't committed, you don't see x in the file system, so you see an inconsistency, and so it's possible that this happens. So the question is in this case we are running two transactions simultaneously, so recall, so let's look at what does the transaction mean in the exe3 sense, a transaction means that you know only one transaction is open at any time, all operations that have started during at this point get assigned to the current transaction, right, then you close the transaction, at some point you decide I have closed the transaction, so all operations that start after this will start in the new transaction, so by close I don't mean commit, so there's a difference between transaction closure and transaction commit, so I have to decide some point where I can commit the transaction, right, close the transaction, right, right, so let's understand this, so let's say I close the transaction, and so there's some, there's possible that some ongoing operations, so what you are suggesting is that when you close the transaction you don't take any new operations, and you wait for all the existing transaction operations to finish, okay, but let's say then A starts, let's say I wait for the transaction to close till here, and then you know this transaction starts, so this transaction becomes part of this existing transaction, but this can continue forever, right, so let's say there are lots of these operations happening, so I will never get a chance to close the transaction, so the idea was that I just decide that I have closed the transaction, and then all the future operations become part of the next transaction, if I wait for all ongoing transactions to finish before I close the transaction, then it's possible that I never get a chance to close the transaction, because there are lots of ongoing transactions that are going on, right, so the idea is that I just decide to close the transaction, which means that all the transactions that are already started, they will get through in this transaction, but everybody who hasn't been able to catch the bus will get the next bus basically, right, so you know that way you basically are sure that you can close the transaction at periodic intervals, otherwise you will never be able to do this, so to avoid this consistency you basically add one more rule to this that you don't start ops of next transaction till ops of previous transaction, till all ops of previous transaction finish, the idea is that you don't till this one finishes, so it's similar to what you are saying, except that you still have guarantees on you know, you can choose when you want to close, you don't have to wait for things to be able to close the transaction, so you say that if you have decided to close this transaction, before you start executing any operation of the next transaction, you are going to wait for all the operations that are ongoing in the previous transaction to finish, to prevent these inconsistencies, so in this case you will make sure that this start happens after this stop, in which case you have complete, you don't have any inconsistencies, you have serializability in the behavior with respect to power crash, the idea is you don't start the operations of the next transaction, till the operations of the previous transaction have finished, notice that I am just waiting for the operations of the previous transaction to finish, I am not waiting for any disrites, right, these operations may happen in memory, right, I am just waiting for them to finish, so that what I read from the file system including the buffer cache is serial with respect to the previous transaction, okay, so it's not a very long wait, you are not waiting for the transaction to commit, you are not waiting for any disrites, you are only waiting for the operations of the previous transaction to stop, so in this case what happens is that X didn't get written to Y, and if there is a failure, so basically this inconsistency is in avoided, but you are saying that in this case X hasn't been written to Y, so isn't that inconsistent, well that's not inconsistent, what is consistency mean? There is some serial, so there is some serial order in which things appear to have happened, right, if you delay this, the serial order seems to happen, that seems to happen is that D happened before A, right, it's completely acceptable if D happens before A, it's also completely acceptable if A happens before B, but what's not acceptable is that after a crash it seems like A has happened partially, but B, and B has happened partially, right, so that was the inconsistency, so A seems to have happened partially, you know, the contents of the directory seems to have changed when I, as I had read them, but actually the directory doesn't seem to have that content, so that's an inconsistency, but B happening before A and A happening before B both are legal, so that is consistent, and so what this is doing is it's basically making it look like B happened before A, and that's totally fine, okay, right, okay, so basically we said that T2 waits for T1 ops to complete, but it doesn't wait for T1 to actually commit, T1 can, you know, commit later, that's not a problem, so what happens if T2 starts, what if T2 starts while T1 is committing, so T1 is committing to the disk and T2 has started this operation, so what are some bad thing that can happen? Well, in this case what can happen is, you know, you have decided that you are going to write this buffer to the file system tree, and that buffer basically, to the log actually, so let's say there is some buffer B, that's modified by T1, and about to be written to log, but before it is written to the log, T2 modifies B, so if you are allowing T2 to start running before T1 has actually committed, then it's possible that a buffer that belonged to, that was dirty by T1 is again modified by T2, and so what can happen is that if T2 modifies B here, then what gets written to log is the contents of T2, which include T2's modification. Let me just rephrase it, so let's say T1 modified B, writing T1 starts commit, starts commit means starts writing its modified buffers to the log, so before T1 writes B, T2 modifies B, so what can happen? The value, the contents that get recorded on the log contain T2's modification, and that's not something that you want, so what is a simple solution? You just make a copy of this buffer B, so you basically say that this buffer B belongs to the committed buffers of a previous transaction, and this buffer has not yet been written to the log, and if there's a, so you do copy on write basically, by T2 on T1's buffers, so if T2 tries to write to a buffer that belongs to T1 and has not yet been written to disk, you make a copy of it, and it's the most recent copy that's the valid copy, the previous copy is only for T1 to commit to disk, yes question? Why can't we use the logs? Okay that's an interesting question, you want to keep a log on all these buffers while you're committing this transaction T1? Okay so here's a suggestion, I will leave the logs only when the transaction gets committed, is that an option? Well that seems like an option to me, you know you're basically making sure that till the transaction commits, you basically log all the buffers that belong to the transaction, but isn't that too long to hold on to the log of a buffer? You basically want that things should happen concurrently as much as possible, imagine that you are constantly writing, or constantly modifying the same file, or you're constantly adding or removing files from the same directory, then if that directory belongs to the previous transaction and you're holding logs to it, then the next transaction just gets completely serialized with respect to the previous transaction, that's not a good idea. The better idea is to basically do this versioning of the blocks, you're basically doing versioning, you're saying this block has, T1's version of it is this, and this is the current version of this block, and so that's a much more efficient way of doing things. Once again, I think it's a classic example of a trade-off between transactions and logs. Here transactions provide more concurrency, and yet come at a very small cost, because here transactions involve disk accesses, so doing this, figuring out of what needs to be copied, et cetera, is possible. Let's quickly look at the performance of EXE3. Let's say I create 100 small files in a directory. In the simple logging, or I'm going to call it the xv6 way of logging, xv6 logging, it would have involved, each create would have involved five or six or eight disk writes, and a commit, so each write would have been one commit, and that would have been, let's say, 100 into five or six writes, so let's say 20 milliseconds is roughly, 100 into 20 milliseconds is two seconds, is that right? Yeah. Okay, but with EXE3, all these 100 small files creation happens inside memory, and then there is one sequential write that's 10 milliseconds, so two seconds versus 10 milliseconds, and then there's an asynchronous copy from the log, or no, asynchronous application to the file system tree, which is not in the critical path, and so that can be done there. So it's much faster in that sense. Also, I say one sequential write, but you may want to do actually two sequential writes, one sequential write for writing all the blocks in one go, and then after they have been written, then you write the commit record after that. You don't want the disk to also schedule the commit record, right? You don't want the disk to reorder the commit record with respect to the other data blocks. If it does that, then you have a very bad situation. The commit block has been written, but the data blocks haven't been written. You first give the disk all the data blocks to be written, and the descriptor blocks. After they have been written, the disk says, I have written them, then you give them the commit block, and so there may be two revolutions there, right? Okay, let's continue this discussion, or finish this discussion next time, and I'm also going to discuss security and access control next time, okay."}