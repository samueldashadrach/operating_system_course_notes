{"text":"Welcome to operating systems lecture 40, right. So, we were discussing operating system organizations and let's review it. So, there are, you know, there is a monolithic OS organization that we have been used to, which is, which has, you know, let's look at the Unix abstractions. The file system, virtual memory, scheduler, drivers, all live in one common address space that's called the kernel. And then these different modules, exports, system call APIs like read write, open close, mmap, page fault, etc., right. So, these are all sort of abstractions that these things provide. Then there was an alternate organization which is called the microkernel, where these different components are moved in separate production domains. For example, they are run as separate servers. These servers could be running as separate processes in unprivileged mode or they could be running as separate processes in privileged mode. In either case, they are isolated from each other. And what the kernel is basically is just an inter-process communication, IPC, and protection. So, nobody can run away with resources and the job of the kernel is really to provide fast inter-process communication, right. So, this has the advantage that it's very plug and playable, pluggable and playable. You can choose your file system. You can choose your virtual memory subsystem depending on what you want to run. You can actually say, here is my virtual memory subsystem, why don't you run this one instead of that one. Of course, there are some issues there that, you know, that virtual memory subsystem should be trusted, maybe signed by some certificate authority, etc., but it's possible to do this, right. The other thing is, of course, if there's a bug or there's a security flaw in one of these things, let's say there's a security flaw in the driver, it doesn't affect the security of the full system. It just affects one container and so faults are not propagated, okay. And then we were looking at another type of microkernel that's called the exokernel. Here, the idea is that you expose as much information, low-level information as possible from the kernel to the application and all these different subsystems like the file system, virtual memory scheduler, even the drivers are part of the application logic, okay. So, for example, the application can decide what the application knows that there is a physical address space and there's a virtual address space and it can decide what should be the footprint of my application or my code on the physical address space and what should be the footprint in the virtual address space and what should the mapping be. In other words, it can decide what the replacement algorithm should be, when should I take a page fault, etc. and so on, right. Similarly, in the file system case, it can choose what file system it likes. So, instead of exposing open, read, write, close calls to the application, how about exposing the device interface, sort of like a disk interface to the application, right. So, tell the application that look, you have access to a disk, a raw disk and so you can write to sector number 10 and so on and you choose, you know, what layout you want on this part of the disk. Of course, that has problems, you know, what about sharing, etc. But those can be, you know, built on top of that. So, how do different processes share files, etc. That can be built on top of that. But at the end of the day, the kernel is not involved in implementing the file system. The kernel is only involved in providing an abstraction that allows the application to build a file system on top of it, right. And so that makes things very configurable. An application can choose what file system it wants and I was using the example of a database. You know, a database does not like, for example, a database may want very strong persistence guarantees. So, if it has written something to a disk block, it needs to be sure that the disk block is actually present on the magnetic platter before it actually, you know, prints a message or releases the money or something like that, right. So, these kind of guarantees, if you provide on top of a regular operating system like Unix, you will need to do system calls like fsync. And those, because there's multiple layers, they know those layers don't interact very well with each other, that overall performance of the system is not optimal. But if you have this kind of a system, then an application can actually tune, you know, you have gotten rid of the layers, the multiple layers. You were saying that the application can choose. So, the application can choose what file system abstraction it wants. So, there's only one file system abstraction, okay. So, okay, let's look at, so you may say, oh, but you know, that's too much headache for the application developer. Does that mean if I want to implement a small application, I need to implement a full file system and a virtual memory subsystem before I can get Hello World to run? Well, the answer to that is very simple. You just use libraries, right? So, you have standard libraries. For example, you have, you use, you know, you have a library that implements the exe3 file system. It's a user-level library, right, that implements the exe3 file system using the abstractions provided by the kernel. Most applications, they want to just use the exe3 user-level library. But let's say your application is a database application. It says, I don't want to use the exe3 library. I want to use my own sort of library. So, this flexibility is possible because you're giving lower-level interfaces to the application as opposed to higher-level interfaces to the application. Yes? Okay, won't there be too much overhead? Interesting. Won't there be too much overhead? Why do you think there will be too much overhead? You say, why? Oh, well, every app will have its private copy of the file system code. Every process will have a private copy of the file system data, or similarly, VM data, driver data. So, isn't that too much overhead? Well, you know, you could argue there is some overhead, yes, because, but at the same time, you could share these things across processes, right? So, processes can actually share address spaces. So, libraries that are common to multiple processes can actually live in shared space, can actually have one physical address copy, and multiple virtual address spaces can point to the same library copy. And these kind of optimizations are well-known and we already know about them, right? So, multiple processes, sharing libraries, sharing data are actually pointing to the same physical memory area, and so the space overhead is not much, right, negligible in fact. Right, so what the kernel has to do is firstly, it has to design a low-level API that allows all these applications to actually control these low-level interfaces, like low-level devices, for example, or low-level disk, or network, or whatever, or CPU, or memory. But the other thing it needs to do is basically implement protection. So, these applications are not trusted, they are not trustworthy, and so an application should not be able to, for example, take to run away with the resources, or it should not be able to create mappings that it's not allowed to create, or it should not be able to write to blocks, disk blocks that it's not supposed to write to. Right, so those are small things that the kernel will ensure. So, kernel's job is protection, and that's for protection and low-level implementation of low-level API, that's all. So, that way the kernel becomes much thinner than the original thing. Anyway, protection was there, right? Whether you put it a higher layer, or you put it a lower layer, how do you implement protection? How do you implement protection at a lower layer? No, no. So, the question is, in the previous case, in the monolithic kernel, the abstractions were identical for different processes. Here, the abstractions are not identical for different processes. That's not true. The abstractions are still identical for different processes. The abstractions are these low-level APIs. What's happening between the app and the, these layers, kernel doesn't care. Okay, so, you know, the kernel has to keep track of how long the app has been running, but that's true even for monolithic, right? It's just slightly more complex. We're going to discuss it with more examples. Let's look at this. So, we were looking at the exokernel VM example, and said, okay, so, because, unlike Unix, where the application has no idea that there's something called a physical address space, the application only knows about a virtual address space, right? And it's the operating system that's basically managing this mapping between the virtual address space and the physical address space. In the exokernel, you actually expose this entire mapping to the application, and you can, and the interface that you gave is, so these are down calls, which are, you know, system calls, basically, that the application can make. So, you can allocate a physical address page, and you can deallocate a physical page. So, notice that I'm saying PA, which basically means physical address. So, the application knows that it's a physical address space. So, the application has full knowledge about what's the footprint in the physical memory, what's its footprint in the physical memory. In Unix, an application has no idea, right? It's the OS who is managing how much physical footprint you have. And then there is this down call called create mapping from virtual address to physical address. Of course, you know, there are some protection mechanisms that the kernel must ensure to make sure that it can only create mappings to physical addresses that it owns, right, or it has allocated, so that there's no security problem of that sort. And then there are up calls. Up calls are similar to signals in Unix, where the kernel can actually ask the application to execute some code, and there's some entry points that the application must have pre-registered. And so, some up calls are page fault. So, the kernel tells the application that there's a page fault on this virtual address, and the application can decide that it wants to allocate one more physical address, physical page, and then create this mapping, and then resume the execution. In doing so, it may want to first de-alloc some other physical address space, because, you know, the application, the kernel is not allowing you to alloc more than a certain number of physical address spaces, et cetera. And then, so that's page fault. And the other thing that the kernel will want to make sure is that the application cannot just run away with this memory. And so, in this case, let's say there are other applications that are running, and you want to do some kind of peer allocation of memory, then the kernel can make an up call saying, please leave the page. So, notice that this is unlike Unix, where the kernel would just take away the page. It will run some algorithm internally to figure out which page to take, and just take it away, right? Without consulting the application at all. And that was the problem, right? The problem was that I was not consulting the application. The application was perhaps the best judge of which page to actually throw away, right? And whether it needs to be written to the swap device or not, okay? In this case, you just tell the application, I want you to release a page. And so, the application then, you know, chooses which page to release, and then calls the alloc page on that particular page. And then, you can, you know, to make sure that this is safe, you can have another up call which says, you know, force me the page, and you give a particular physical address that I'm releasing this, and he should, he should, he better honor that. If he does not honoring that, he'll probably get a seg fault very soon, right? Yes? Okay. So, fair point. The question is, there are some algorithms or some policies that are best implemented at a global level, and not at a local level, right? For example, I want to run my page replacement algorithm at a global level, right? That may be the most efficient thing to do, as opposed to say that I want to give, take a page from you, and you decide which page to give, right? And we want to consider all the pages across all the processes as a global pool, and then choose the one that's least recently used, let's say. In this case, I'm, you know, I'm basically pre-committing to a process that this is the process I want to take a page from, and then I make an up call saying, please release a page. So isn't there, isn't there an inefficiency there? Well, yes, I mean, so there's a trade-off. Firstly, notice that there's, there are two kinds of policies here. One is the policy that the application will implement internally, and then there's another policy at the OS level, anyways, which is choosing which process to ask, right? So it's already, the kernel knows that each process has these many pages, so there's, there are two levels of policies, and so there's still a policy inside the kernel that cannot be overridden, which basically saying, which is a global policy, right? So what the ExoKernel is doing is separating the global policy from the local policy. It's saying, let's, let the application have the flexibility to at least decide the local policy. Global policy is still in the hands of the kernel. For example, it will choose, it will say which process to give this up call to, right? So that's, that's the global policy. Similarly, you know, any other shared device, like the disk, you took the example of disk scheduling, same thing, right? So there's a, there's a global policy engine running in the kernel level, but you are giving the application the flexibility to at least decide the local policy. You're right, the application can still have no control over the global policy. Perfectly fine, perfectly valid point. Okay, let's take another example. ExoKernel CPU. So, you know, you have down calls like yield and block. These are similar to a unit. They basically say, I want to yield the CPU. By yield, you mean, I want to stop running on the CPU, somebody else can run. At the same time, I'm ready to run. So if there's nobody else to run, I can, you know, I can be rescheduled back. And block basically means that I'm blocked. So, you know, till I'm woken up by somebody, I'm not available to run. These are similar. But the interesting thing is up call. Unlike Unix, where the kernel would just take away the CPU from the application, and we saw some problems with that, right? So the kernel can just say, I need the CPU back, just take it away, right? So irrespective of where the application was in this execution, you could just take away the CPU from the application. And there was a problem with that. Let's say the application was actually holding a spin lock, right? And now, within the critical section, the CPU was taken away from the application. Then all the other processes, which were, you know, depending on the spin lock, will just waste their time spinning on this spin lock for the time quantum, right? So that would have, that was, we used that example as a schedule when we were discussing scheduling, right? And that problem was basically because the application had no control over when there was a context switch. Here, you're going to make an up call instead, asking the application to please leave the CPU. The nice thing about this is when you ask the application to say, please leave the CPU, the application knows that I'm in the middle of a critical section. It can actually execute the minimum number of instructions required to come out of the critical section, and then call eat, right? That's a much better scheduling paradigm than what we saw in Unix, right? So it doesn't have this problem of getting interrupted in the middle of a critical section. And so everybody else just keeps wasting the CPU because it spins on the lock. Also, you can, you know, the application knows exactly which registers to save. So let's say, you know, the application is just using three registers. So when it actually calls eat, it just needs to save those three registers before it actually says, I want to eat the CPU. Similarly, there's an up call called resume CPU. So unlike Unix, where when you resume, you just start from where you preempted the process last time, right? In case of, in the case of exokernel, you instead make an up call saying resume CPU. And the up call is going to go to a certain point in the application, and the resume CPU up call will know exactly which registers to reload. Depending on which registers were saved in my previous yield call, right? So you have saved on the amount of data that you need to save and restore also. That data is being stored by the user program itself. Yes. So the user program's responsibility to save and restore its own data, right? As opposed to, and it has this, you know, you can give it that flexibility because you are making, you know, you're requesting and resuming through, you're giving him control over it. As opposed to Unix, where you would just take it away and give it back, in which case it becomes the kernel's responsibility to save everything. Is that much of an advantage? Well, you know, yeah, registers are very small. It's not, it's not a big deal, really. But no, in certain cases, it may be a significant advantage. But I think the main, the most important thing is the scheduling point that I discussed, which is basically that you don't have problems like the Convoy effect, where basically you, if one process gets interrupted in the middle of a critical section, then there's a very bad scheduling behavior that you can get. Okay, good. All right. So with that, I'm going to move to my next topic, which is virtualization. This is a somewhat relatively modern topic. Are there practical examples of exokernels? Well, so exokernels were first proposed in late 90s as a research paper. And, and, you know, many ideas from the exokernel paper have been used in modern kernels, especially in the modern kernels, where we are basically looking at kernels that scale across lots of CPUs, like 60 to 80 CPUs. You know, these kind of ideas are actually proving very useful. And so many ideas from the exokernel paper are being used in modern kernels, especially research kernels today, which are targeting large multicore machines. Okay. All right. So let's talk about virtualization. So, so far, we saw that there was a kernel, and then there was a process. And there was an abstraction between the process and the kernel. And this abstraction was designed so that the process can do what it wants to do, and also it's safe. This abstraction depended on the kernel. It depends on the OS, let's say. For example, Windows will have a different abstraction from Linux. Linux version 2.2 will have a different abstraction from Linux version 3.0, and so on. So these abstractions are relatively very fluid. It's very difficult to take a process that would run on Linux 3.0 and make it run on Linux 2.2. Even more difficult to take a process that runs on Windows and make it run on Linux, let's say. Or vice versa, right? So all these things are relatively difficult. Also, it's very difficult, so let's say I have two machines. A process has lots of bindings with the kernel. For example, it has a file descriptor table, it has virtual memory, etc. So it's very difficult to migrate processes between live, between two different machines. So you have two different machines running two different kernels. Let's say they're even running the same kernel. I cannot just take a process here and just say, you know, let's start running it here. I cannot take a running process here and then start and resume it on the other side, because a lot of the state of the process is actually inside the kernel. And there's a lot of state that a process can have. And, you know, so far we have seen, given that it's a monolithic kernel, a monolithic kernel has a file system, a virtual memory subsystem, and so there are all kinds of data structures that are living inside the kernel. And so all those data structures need to be migrated as well. And that seems like a very hard problem, right? Okay. So virtual machines is a concept that says, let this abstraction that I'm calling a process be somewhat equal to the hardware abstraction, machine abstraction. If I could do this efficiently, then, you know, the abstraction that I have with whatever is running beneath it, let's call it the kernel for now, it's going to be simply that of a CPU, a raw disk, a raw disk, devices like the disk controller, right, network card, network interface, and so on, and physical memory, and virtual memory subsystems, which whatever the hardware supports, for example, all the registers like CR3, which is pointing to the page table, right? So what if the abstraction that the kernel provides to the process is identical to the abstraction that the kernel sees itself on the hardware, right? The kernel is written to assume a certain abstraction, which is a hardware abstraction. The hardware abstraction is, there's a CPU that runs one instruction after another, it has a certain instruction set, jump allows you to change the program counter, and so on, there are memory accesses, and then there is a virtual memory subsystem, which can be controlled by going through CR3, and you can configure page tables, there are devices, hardware devices that can be accessed using in and out instructions, and so on, right? This is an interface that the hardware provides, and is specified in the hardware manual of the manufacturer. Can the same interface be the one that the kernel provides to the process? And if it is able to do this efficiently, then the interface between the process and the application and the kernel becomes much more solid, or much more compact, and much less fluid, in the sense that hardware abstractions seldom change, or change at a much slower pace than the kernel abstraction change, right? You get a kernel version every few months, but the hardware version changes at a much slower rate. Moreover, hardware preserves what's called backward compatibility, you know, anything that you could have run on a previous generation of a processor, or a previous generation of a disk controller, will run on the new generation of the disk controller, and so on, right? So if you could do that, then this abstraction becomes relatively solid, and so it will be possible to take a process and, you know, run it on a different kernel. And so you basically know that the state that this process has is very compact. It consists of the physical memory footprint, the page tables, the disk contents, and the network interface state, and other registers, right? So this is basically, you can draw a line, you know, that this is exactly the state of this process, and if I can save it and restore it somewhere else, I can migrate this process from one machine to another, all right? And I can also migrate it across different versions of the kernel. So this abstraction is called the virtual machine abstraction, because it's just a machine that's implemented in a virtual manner, in a process container, and so you could run multiple virtual machines on one physical machine, right? It's just like running multiple processes on one kernel, except that the abstraction has changed. The abstraction is no longer that of a unique process, the abstraction is that of a machine, of hardware, that you're providing it. So let's see, does it mean that we are exposing all the hardware to the process directly, or not? So well, I mean, firstly, just like Unix could not trust its processes, we don't want to trust our virtual machines, right? So you cannot just expose the hardware directly. The question is, how does one implement these virtual machines efficiently, okay? So let me just also say that these containers are called virtual machines, and the kernel is called the virtual machine monitor. If I'm doing something like this, firstly, let's understand what are the advantages of doing something like this. The advantages of doing something like this are, firstly, because the abstraction of the container is very solid, it's very well-defined, it's written in the hardware manual, and it's basically something that has a guarantee of backward compatibility. You can actually take a virtual machine container and move it at will to some other virtual machine monitor, and still expect it to run it exactly like it was running in the previous machine, right? So you can actually do migration across different machines, which would be a harder thing to do for something as amorphous as a Unix process, all right? So it can, it's actually a very good fit for a distributed system, where you can imagine that you have different, lots of different machines, and you can take one virtual machine and just decide where to schedule this process, right? You can schedule it on this physical machine, or you can schedule it on that physical machine, and so on. It also allows you to do other interesting things. For example, you could run a full operating system within this container, right? So you could install, let's say, a Linux system, and spawn a virtual machine, and install a Windows operating system inside that virtual machine. Because the abstraction is exactly that of the hardware, the Windows operating system running inside this container will never actually know that it's actually running inside a virtual machine, because it will see exactly the same abstractions that it would have seen at the hardware level, right? So there are lots of advantages to doing this, and this virtualization forms the building block of what's also called as cloud computing, and I'm going to discuss that more in a little bit. But let's first discuss how virtual machines are implemented, right? So it comes to your question. Firstly, I need to implement them in some efficient manner, otherwise the whole thing is not practical. So what's the simplest way to implement virtual machines? Well, one way to implement virtual machines is interpretation, or interpreter-based, right? You just write an interpreter, which basically behaves exactly like what your hardware does, and just takes one instruction, executes it, takes the next instruction, executes it, and so on, right? And so you basically have an interpreter, and you run this, the code of the machine which is executable now inside this interpreter, and this code should run, right? So for example, a software like Box is a virtual machine monitor, could be called a virtual machine monitor, at least in the definition that we have so far, except that it's too slow, right? It's going to take one instruction from the process, it's going to decode it, it's going to see what registers it accesses, it's going to emulate those registers in memory, it's going to perform this operation, and then go to the next instruction, and so on, right? This is an interpreter loop. Each instruction has to internally execute 50 to 100 more instructions, and so this has an overhead of 50 to 100x. Notice that this interpretation-based approach is completely safe. You don't need to expose raw hardware to the executable. The executable sees virtual hardware, and this virtual hardware is a virtual CPU, you could emulate a virtual device like a virtual network card, and so on, and they are basically just talking to this virtual network card with the same interface that you would have had for a physical network card, right? And the virtual network card internally is using, let's say, system calls exported by the virtual machine monitor to actually send packets on the physical wire, okay? So that's an interpretation-based virtual machine monitor. It's really slow, but it's completely safe. I don't need to trust anything. The other approach is basically what's called binary translation. Here the idea is that if there's a piece of code that gets executed 100 times or a million times, that's more likely, then you don't need to run the interpreter loop every time you see that instruction. You can take those, let's say, you can take those 100 instructions and translate them to some safe counterpart, and then execute those 100, you know, execute the translated counterpart a million times, right? So it's a fast way of doing interpretation. Let's say it's a fast interpreter, a faster interpreter, where basically instead of taking an instruction every time, decoding it, and performing its operations, you translate that instruction into the operations that need to be performed each time that instruction gets executed, and you just jump to the translation, right? That's basically an example of this kind of a system is QMU, which is basically, which you've used in your assignments. I guess this is roughly 5 to 10x overheads, okay? So it's an improvement over something like Box, but still not good enough, right? I won't use something like QMU to run virtual machines for which I care about performance. So because they have such high overheads, these software systems are not called virtual machine monitors. So the additional requirement on a virtual machine monitor is that the performance overhead of virtualization should be very small. It cannot be, it should be in some percentage points, it cannot be 5 to 10x, for example. So the third way of, and the most common way of doing virtualization is what's called trap and emulate. What's done here is, here you allow, so before I go forward, let me also point out that in both these approaches, interpreter and binary translation, I could have run a virtual machine for one architecture on the physical machine of another architecture. Nothing stops me from doing that, right? I could run the virtual machine for the x86 architecture on a physical machine for PowerPC architecture, right? Because I'm just completely emulating the x86 architecture on PowerPC, it doesn't matter, you know, what the underlying machine is. Well, the monitor, you know, will be modified in the sense that, let's say, if you're writing it in a higher level language like C, then it should be compiled for PowerPC, that's all. The underlying machine is PowerPC, then you'll compile it for PowerPC, that's all. Otherwise, the monitors remain the same. But something like trap and emulate allows only virtualization of same architecture, which basically means that you can only run the virtual machine of the same architecture as the underlying physical architecture. And the way it works is, the key observation is that most instructions executed by a VM, and by VM I basically mean the operating system that's running and the applications, are unprivileged instructions. So, you know, a huge majority, 99.9% or more, of the instructions that get executed inside a virtual machine are instructions that are very regular in nature, like move, add, subtract, load, store, etc., right? So these are very regular instructions. These instructions would have had the same effect whether they are run in the user space or whether they run in the kernel space. And so these instructions, the emulation of these instructions can, or the translation of these instructions, can be made identity. So if the guest, so if the process, which I'm also going to call the guest, so the guest executes an instruction called an unprivileged instruction, I can just execute the same unprivileged instruction in the host and get the same behavior. Right. And so what you do is, you take the operating system and the applications, so guest operating system and applications, and run it in unprivileged mode. And the virtual machine monitor runs in privileged mode. Because most of the instructions are unprivileged in nature, when they execute, they just execute as though they are running natively. If they execute any privileged instruction, for example, if they try to access CR3, right, or if they try to execute an in instruction or an out instruction to access a device, you trap, just like, and then you emulate, just like you were doing in Box and QMU, you emulate inside the VMM, and then you return. What you do is, you take, so consider this disk image of a virtual machine as your executable, right, that's the, that's the disk image that's basically, that basically lives on your hard disk for a physical machine, that's the hard disk. So the hard disk is basically the executable, and so now this hard disk in the virtual world is going to live in a file, that's what we're going to call your virtual disk, and I'm going to run this virtual disk inside this container called the virtual machine. And this virtual disk is going to start running some instructions, one after another. What you're going to do is, you're going to run these instructions, all these instructions in unprivileged mode, even though these instructions were meant to, some of these instructions were meant to be run in privileged mode. For example, when you boot a machine, the first few instructions assume there's a privileged mode. Anytime you transition to the kernel, you assume that these instructions are running in the privileged mode, but the change you make is, you run all these instructions in the unprivileged mode. Most of these instructions execute without any problem, because most of these instructions are unprivileged instructions. Any instruction that's a privileged instruction, that must have been an instruction inside the guest OS kernel, will cause a trap, right? So you rely on some hardware properties that any, which basically say that if you try to execute a privileged instruction and in unprivileged mode, you always get a trap, right? So it causes a trap, and you handle the trap by emulating that instruction in software, just like you had done for Box or QMU, and then you return back to your guest, right? So this kind of virtualization is called trap and emulate virtualization, and significantly faster than either interpretation or binary translation, and you will, for something which is compute-intensive, you will have less than 5% overhead for compute-intensive applications, okay? On the other hand, something that's very I.O. intensive, it's very likely that you're going to be executing lots of privileged instructions. If the guest is trying to access lots of, you know, virtual devices, then it's probably executing lots of in-and-out instructions or something that's privileged, and anything that's privileged is going to cause a trap, it's going to execute some emulation logic inside the virtual machine monitor, and then return back, and so this extra overhead of doing this trap and emulation is going to become visible on I.O. intensive applications. So on the I.O. intensive applications, you can have overheads of up to, let's say, 2 to 3x or 4x, you know, you could go back, get back to the QMU kind of performance for I.O. intensive applications, and this is just a basic trap and emulate-style virtualization. There are many optimizations that can be done over it, and today, you know, with hardware support and all that, you can do virtual machine monitorâ€”virtual machines at almost negligible overhead. So what have we achieved? We basically have a process abstraction that looks very much like the hardware abstraction, and it runs with almost 0% overhead, right? That was, you know, there's a whole idea of a process abstraction in the first place, right? The process should not be running too much slower than when you're running it over the kernel, as opposed to not running it over the kernel. Is the VMM running on top of the kernel? Well, consider VMM as a part of the kernel. The VMM is yet another subsystem inside the kernel, just like FS, virtual memory, drivers. There's yet another subsystem inside the kernel called the virtual machine monitor, right? It's a relatively modern subsystem, because, you know, virtualization is a relatively modern thing. It was very popular in the early days of computing in the 1960s, when machines used to be very large, like the IBM mainframes, and used to be really expensive. So the only way to actually make full use of these machines was basically lots of people sharing these machines, and at that time, virtual machines was a very popular concept, and lots of research was done on, you know, using, implementing fast virtual machines. Over the years, computers became smaller, they became personal, things didn't have to be shared, computers didn't have to be shared, and so virtualized, the technology of virtualization was lost. What do I mean by technology of virtualization was lost? The hardware interfaces were not virtualization compatible. For example, recall that one of the requirements to implement this trap and emulate cell virtualization is that if I run a privileged instruction in an unprivileged mode, it must trap, right? So, but the hardware designers just didn't care about it, because they thought these are personal computers, and similarly, the software stacks were not tuned for virtualization, etc., but it was rediscovered, because now, today, even though our hardware is relatively much cheaper, the cost of actually managing this hardware, which includes power, power space, people who know how to manage these things, software maintenance, upgradation, security, viruses, etc., all these problems basically make, you know, have moved us back to, you know, have made the case for, again, a centralized computing infrastructure that lots of people are sharing, and that's what we are calling cloud computing today, right? And so one of the building blocks of cloud computing is virtualization. So let's see why virtualization is helpful, why, you know, why cloud computing is a useful thing. If you look at the utilization levels, so let's say this is time, and let's say this was utilization, right? So what do I mean? Let me take one, any one machine, let's say, let's take the example of a web server, let's take the example of the web server of IIT Delhi, right? So if I look at the CPU utilization of IIT Delhi's web server, I'll probably see, you know, let's say this is 100 percent, and this is zero, then it'll probably be somewhere like this, sometimes there'll be a surge, some results are getting announced, and then there's like this, and then there's a surge again, let's say, etc. So this will be the typical curve for CPU utilization of a server. Same thing for, let's say, memory, or disk, bandwidth, or network, right? So usually you will get some kind of behavior where, on average, the utilization levels of the machine are very low, but sometimes there is a very high surge in the demand, and you get very, relatively higher utilization level. And so what we typically do is we provision for the maximum. We try to provision for the maximum so that when there is a very high load, I don't see problems. But most of the time this maximum is actually getting wasted. It may not, it's getting wasted because I'm probably consuming a lot of power, still. I'm consuming less power at 10 percent utilization than what I was consuming at 100 percent, but still I'm consuming power, and I'm consuming the equal amount of space, definitely, and so on, right? What would have been better was if I had, if I was able to run multiple machines on the same physical machine, so let's say I have, you know, I have the IATDs, web server, and let's say I have mail server, which is, let's say, my department's mail server, the computer science department's mail server, and let's say it has behavior like this, then these can easily be multiplied on the same physical machine without seeing performance degradation on either application, and yet having better utilization over for my physical machine overall, right? So virtual machines allow this, okay? And they allow this without having to worry about software compatibility. You may say, oh, what's the big deal? Why don't I just have one Linux machine and run the mail server as one process, my department's mail server as one process, and your institute's web server as another process, and let them, you know, coexist, and it's going to do exactly what you want it to do. It's very difficult to, because the abstractions are so fragile, it's very difficult to ensure that the department's web server can be run on the same operating system as the institute's mail server without causing any problems with each other, and without having any security implications, right? So the department's web server may be less critical than the institute's web server, and so you may want that, you know, they should be completely isolated from each other. Something may not be that carefully written, other things may be carefully written, and because abstractions of the Unix abstraction, let's say, are very fragile, you know, for example, the configuration files, etc., you know, these processes will depend on the configuration files, the slash etc folder, etc. It becomes a lot of, it becomes a mess after a time to manage lots of different processes running on a single operating system. On the other hand, the virtual machine abstraction is very clean. It's a hardware abstraction, so I can just take one hardware abstraction and run it, and I can, I can not have to, I do not have to worry about security problems or interference problems between these two virtual machines. Okay, so what this allows me to do is basically, let's say, I have, this is a physical machine, let me call it a physical host, and these are virtual machines. VM, VM, there's another host, VM. So I'm, I'm showing you a cloud computing scenario, so what's, what's called cloud computing in the modern world. So this is what a cloud computing environment will look like. You will have multiple physical hosts, and you would be running typically multiple virtual machines within a physical host. Typically, you would be, you could, depending on the utilization levels of each VM, you know, you could pack multiple VMs on the single, single host. It's very common to see tens of VMs packed on a single physical host, because, because the utilization levels of each of these VMs is very low, right? And so you could, you could be running these things together. The nice thing is, because these abstractions are so tight, I can, at any time, decide to move this virtual machine from here to here, right? So this capability is what's called live migration. This can be done in a way such that the virtual machine has near zero downtime, right? So it has, it appears that the virtual machine never switched off, right? So there's a, you know, one way to show this is basically, there's an interesting demo where, let's say, you're watching a movie on a virtual machine. A live migration happens, which means the virtual machine shifts from one physical host to another. There is some amount of copying that's going on for all the memory state, etc. It's all happening in the background, and at some point, the entire virtual machine shifts from one host to the other, and you do not see a blip in your, in your movie, right? So that's basically, that's what I mean by a near zero downtime live migration. That's a very powerful capability. What it allows me to do is, it basically allows me to dynamically schedule my work across my physical hosts at will, and that's one of the main strengths of cloud computing. Let's say I have, you know, let's say these three VMs become very highly utilized suddenly, so I can choose to move this VM from here to here, right? Or let's say it's in the middle of the night, and none of these VMs are actually using any resources at all, so I can pack all these VMs on a single physical host and actually switch off this particular physical host and save power, right? Also, I can do fault tolerance, which basically means, now your virtual machine state can be encapsulated as a file, so a running virtual machine can be snapshotted, and its state can be preserved, its live state can be preserved as a file, and it can be loaded from where it was at will to basically get exactly what, how it was when it was snapshotted, right? So that allows me to do fault tolerance. Fault tolerance basically means that if there was an error, either a hardware error or a network error or a power failure, I can revert to the snapshot of a virtual machine and basically get the same kind of data. So cloud computing has its advantages, basically that I have talked about, but the more interesting thing from an operating system standpoint is that it brings open all the issues that we thought were dead for a long time. For example, scheduling now becomes a very important issue, right? If you are running a cloud computing facility, if you're running a data center, it becomes very important that you manage the data center with maximum utilization, and the maximum utilization depends on what scheduling algorithm you're using, right? Unlike the personal computer world where generally the hardware resources are in plenty, in a cloud computing environment, by definition, you're basically sharing these resources across lots of people, and you're minimize, trying to minimize your costs, so resources are never plenty. You're basically always trying to optimize your resources in some sense. Good. All right. Okay. So with that, we'll finish this course."}