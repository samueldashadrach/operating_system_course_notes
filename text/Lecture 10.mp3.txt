{"text":"Okay, so welcome to operating systems lecture 10. So far we were looking at segmentation as a way of implementing virtual memory, right. This notion where there is a separation between a virtual address and a physical address and there is some translation logic that goes to convert a virtual address to a physical address, this system is called a virtual memory system and we saw segmentation as one way of implementing virtual memory, right, where the translation was rather simple, we just added base to virtual address to get a physical address and we said the way it works is that there is a table which is stored in memory which is called the global descriptor table which contains these descriptors, right and this table can be as large as 2 to the power 13 entries where you know each of these is one entry, each of these entries contains a base address which will be a physical address and a limit which will say you know what is the size of this particular segment. So each descriptor defines a segment and the program itself is going to you know is going to dereference one of these descriptors using the segment selectors which are these segment which are stored in these segment registers, they are CS, DS, ES, FS, etc. and you know they are going to dereference these descriptors here and we also said one typical way of organizing the segment table is the following, you know you have some extra entries in this to store the task state segment and null entry, we can ignore this for the time being but there will be you know 2 descriptors, one will be for the kernel, the kernel's base and the kernel's limit and the other one will be for the user, user's base and user limit, right, at any point of time. So at any instant of time, you will have these 2 entries, you know in a typical operating system which is using segmentation to implement virtual memory and what happens is typically that the kernel base and kernel limit remain constant throughout the execution of the system, right, because they refer to the kernel's space and the kernel always maps itself in a certain address space and you in the physical memory and the kernel is always you know is always mapped and it is always the same contents of the kernel that are always mapped, but the user base and user limit descriptor keeps changing over time because you know context which will happen between one process to another process. So you know for at one instance, it could be the P1's U base and U limit which are loaded in this descriptor, after a context switch, it will be P2's U base and U limit which are getting stored in this overwritten in this particular descriptor, right, so the kernel remains constant and the users, the processes, descriptor or the processes, the address space which is for the user is defined by which process is currently running and that keeps, that gets swapped on every context switch and that happens by the kernel. Both of these need to be mapped at any point of time because you know as we have seen control can transfer from user to kernel if some event occurs like an exception or an interrupt or a software interrupt and in which case immediately you need to dereference the kernel side of the address space, for example the CS in the IDC table could be a kernel, must be pointing to the kernel descriptor and so both of these are mapped. The way you prevent the user from accessing the kernel address space is basically by having another field here which is the privilege level, they call it P, so one of them will say you know I can only be accessed in privilege level 0 and the other one can say I can be accessed in either privilege level 0 or 3, so you know it does not, so that way this you know when the processor is executing an unprivileged model only be able to access this dereference through this descriptor and not others, all right. So typically you know I said that there are 6 segment registers, CS, DS and so on, right and each of them could actually be pointing to a different descriptor here potentially, right, CS could say I want to point to descriptor number 2, DS could say I want to point to descriptor number 3 and so on, right, assuming you know privileges are respected etc. And so that can potentially allow one process to have 6 different segments in physical memory and so that can solve some of the fragmentation problem that we discussed last time. But this kind of an organization where you divide your address space into multiple segments actually complicates the programming model because now the compiler needs to worry about exactly does this address live in this segment or that segment etc. and it needs to generate code appropriately, right. So typically what's done is that all of these actually map to the same segment to have the, to give the abstraction of a flat address space, right. So the compiler can assume a flat address space and all of these actually map to the same sort of descriptor and so you, the process just sort of sees one flat address space from 0 to max, right. So it simplifies the programming model but it has the problem of fragmentation as we discussed last time. Now the entire process has to live in one contiguous segment, okay. Also there was a question last time about you know the GDT being too large, you know 2 to the power 13 entries is too large and I'm only going to use 5 entries out of these 2 to the power 13 entries. So actually you know I omitted a piece of detail. The GDTR actually doesn't point to the base directly. It points to another memory location which contains the GDT base, you know let's call it base GDT and limit GDT. The base GDT points to the base of GDT and limit says what the size of GDT. So that way you know you don't need to waste all this space. You can say that you know my GDT is only 5 entries or 10 entries and so the hardware actually gives you a way of saying that, of specifying the size of the GDT and the maximum value of that size can be 2 to the power 13, okay. So you're not really wasting space in that sense. Okay. So question. Will there be any time, can it happen that I use all the space in my GDT? Depends on your kernel model. In the model that I have described so far, it's not possible, right? Because the number of descriptors that you can have is actually constant, right? You just see 4 registers, descriptors for example. So it's not possible in this model. In another model it may be possible and in which case the kernel should have logic to handle it. But let's just live with this model and say no, it's not possible. Okay. Another question. Is the kbase and klimit constant? Are the kbase and klimit constant for all the processes? Is that your question? Yes. Yes. The same address. Yeah. So kbase and klimit don't change on a context switch. Okay. So on a context switch, the kernel remains as it is. It's the processes address space that you are changing. One more question. What do you mean by base, base and limit are? Yeah. So these are actually physical addresses, all right? So these are, you know, let's call them physical addresses for now. And base GDT is saying here's where the GDT starts and limit GDT is saying this is where the GDT ends. That's all. You can just have limit GDT. How do you specify where it starts? Because we have GDT. No. So I'm saying GDT will point to this structure and this will contain the base and limit. Oh, okay. You're saying, you know, why do I need this kind of thing? I could have, you know, GDT could itself have some notion here which is limit. It's a matter of, you know, it's a matter of fact that this is how it's done on x86. But, you know, you could design something else. Okay? So it basically says that, you know, everything above this is not possible. So for example, if I didn't have a limit GDT and let's say that I hardly assume that the limit GDT will be 2 to the power 13 always, then what will happen is the kernel will have to reserve that extra space and, you know, invalidate all these entries. So, you know, there's a certain bit in these descriptors that says this descriptor is invalid. So you'll have to say invalid on all these things. If he doesn't do that, then that space can be used for some other data structure. And if that data structure, you know, so a process can now manipulate those things, right? Because let's say there was some other data structure living in this space and now the process actually sets his pointer to point here, then, you know, he can now access something he's not supposed to access, for example. So if there was no limit, then the kernel would have had to allocate the entire 2 to the power 13. And just to sort of not waste that space, the hardware allows you to specify a limit. Okay? Question? So how are U-base and U-limit accessed? They are basically accessed using a virtual address, right? So virtual address in this case is basically of the form of a segment register colon some offset. And the offset could be specified in one of the modes, images, you know, indirect or displaced, whatever. So in this case, CS actually goes here. So what happens is the offset gets added to base and offset gets compared to limit, right? And that's how you're actually dereferencing or accessing the GDT entries on each MMU access. The user is not allowed to read this GDT directly. It's only allowed, it only gets dereferenced through this mechanism of virtual memory translation or MMU. The user cannot access these entries directly, no. In case of context switch, does the limit GDT change, you mean, does U-base and U-limit change? No, limit GDT. Oh, you mean this? No, no. This doesn't change. So these remain constant. I mean, so it depends. Different operating systems can implement it in a different way. Let's say the way we are talking about it is that the GDT remains constant. It's just this entry that's changing. Everything else remains constant, right? It's just this particular descriptor that's changing on a context switch. Question? Why do we need to store limit GDT? Because we need to tell the hardware what's the size of our GDT, right? Because the hardware that's going to actually dereference, so how are they going to do this? He's going to say, let's look at the selected in CS and add it to base, right? And he's going to compare it against limit. So if there was no limit, then, you know, CS could point here. And so now it's the responsibility of the kernel to ensure that, you know, all these entries are invalid. Okay. Let's say in our model, the size of the GDT always stays constant. Okay. So, yes, absolutely. So in that case... No. So this limit GDT is a way for the kernel to tell the hardware. That's all. I mean, the kernel is not storing it for its own purpose. The kernel already knows that it's six. It's telling the hardware that it's six. You know, it has to tell the hardware in some way. And this is a way of telling the hardware that, look, you know, there's only six entries that you should be accessing. Okay. So in this model... CS, DS, yes. Everything remains the same. In fact, you know, in this model, when I context switch between two processes, the segment selectors remain the same. They're always pointing to, let's say, the third entry in this table, right? It's just the entry that's changing every time. Okay. But, you know, yeah. It's possible that you have, you know, it's possible that the kernel allows you multiple U ways and U limits, in which case the program is allowed to change between multiple things. Okay. All right. Okay, great. The next question, the next thing I wanted to point out was that on every memory access of this type, right, let's say CS or DS, offset, whatever, what's required is CS is stored on chip. So, you know, there'll be some logic that dereferences CS to get a selector. But the GDT itself, now that CS, the selector's going to get added to the base of, you know, of GDT. But to get to the base, you need to dereference GDTR. And the base is actually stored on memory. So you actually go to memory to get the base, right? Then you add it to CS. Then you get a descriptor. Then you get the base on the descriptor. Once again, you make a memory access to get the base on the descriptor, because the descriptor is stored in memory. And now you add the offset to the base and compare the offset to the limit. And now you do your final memory access, which is base plus offset. So, you know, one memory access had how many more memory accesses? Two extra memory accesses, right? One for dereferencing the base and the other for dereferencing the base of user. One for dereferencing the base for the GDT and one for base of the descriptor, right? So that sounds very wasteful, right? So what's the typical thing that's done? Caching, right? So you have already done your computer architecture course. You've seen caching. So what's done is when you execute the load GDT instruction, these values get cached on chip. So there is some space on the chip which basically allows you to cache these values on chip. And plus, you know, the semantics of this are that if you change, if the kernel, you know, let's say changes the base limit to something else, then it doesn't immediately get, you know, updated in the chip because there will be a lot of logic that's required to do that. Instead, what happens is, you know, GDT, the cache can remain out of sync from the memory. So if you want to actually, you know, recache it, then you should execute that LGDT instruction again on the new base and limit, right? So it's caching, but it's not, you know, they don't remain consistent. So the cache value and the real value in memory don't necessarily remain consistent. You can reload the cache by re-executing the load GDT instruction, okay? So what is this kind of caching? Is it write back, write through? You haven't done write back or write through? Is it write back? Is this write through? No, it's not write through, right, because I can actually go and write to the memory without the cache knowing anything, right? Write through basically means that I have, all my writes have to go through the, so I go through the cache and I first update the cache and then all the things that I update in the cache go down. So actually I can, I have direct access to the main memory, and so cache is actually, can be out of sync. So it's not write through. Is it write back? It's not write back either, because once again, I actually change the value underneath and then update it by using the LGDT instruction, right? So it's not, it's neither write back nor write through, it's, you know, it's another kind of caching where, you know, basically you explicitly invalidate or explicitly load new values into the cache, right? The cache is actually just acting as a read-only cache for, you know, so the cache is accessed in only read-only mode. The only way you write it through explicit invalidation of the old value by re-executing the LGDT instruction or, you know, call it the invalidate instruction. It's neither write back nor write through, but, you know, it's a different type of cache. All right, similarly, when you load values into the registers, Cs, Ds, et cetera, these, the descriptor values, base and limit, get cached on chip, right? So just like this, the descriptor values get cached on chip, so you don't actually make memory access to each time you make a, you don't actually dereference memory to get these values each time you make a memory access. You have it on the processor cache. Once again, the semantics are that when I load it, it gets loaded into the cache locally on the processor. But if you change it later on, that cache still retains the old value, right? If you want to override that old value, you need to re-execute, you know, you need to reload the segment register using some instruction like move, let's say, Ax to Cs, right? So when you reload the segment register, the cache gets freshly loaded. But if you change the memory underneath, the cache remains, keeps the old value, okay? So this is the caching model. So with this caching model, you know, every memory access will actually make only at most one memory access, usually, typically, right? Because you know, each time you execute this address, the GDTR's base is stored and limit is stored locally. You can add it to the selector to get the descriptor value. The descriptor value itself actually is stored locally, and so you can just get the U base from there, or K base, depending on whether it's a kernel segment or a user segment. And you actually just go to the memory for the real physical address and nothing else, right? So this way, you know, you also have to worry about performance, and so you care about performance. This caching model also means that the OS needs to be careful that the cache value and the memory value don't go out of sync, right? So GDT, in this kind of model that we are talking about, GDT remains constant, so it only gets initialized once. It gets cached once. OS doesn't need to bother, really. But, and in fact, even K base and other descriptors are actually only, you know, remain pretty much constant. It's only the user base and user limit that are actually changing. And so on every context switch, what the OS will need to do is reload these segment registers again, right, with that particular descriptor value, so that they actually now get the new cache values, before it actually context switches back to the process, okay? All right, okay. So that was segmentation, and segmentation allows you, it's a nice thing about segmentation or virtual memory in general, is that it allows separation between physical address space and virtual address space. And now virtual address space can be this uniform address space, starting from, let's say, zero to some value, and you can place the processes anywhere you like at runtime, and the same program is going to run, assuming that it's actually running on a uniform address space. But it has problems. The first problem we saw was fragmentation, right? So because a process's address space needs to be contiguous, if different processes of different sizes are created, then very soon my physical memory can get fragmented, and I could be wasting space. So new processes may not be able to get admitted, because, you know, the spaces that I have, the holes that I have, are smaller than the size of the process, even though the total space I have could have accommodated that process. So that's problems of fragmentation. It has problems with, you know, things like process growth. If I want to grow a process, it's very difficult, because I've already placed the process somewhere, and now there are other processes which are, you know, around this process. So now I want to grow the process. The only option that I have is either move other processes up or down, or move this process somewhere else, right? And in both cases, it requires physical copying of the entire memory of the process from one region to another in physical memory, and that's a lot of work. Okay. And thirdly, you know, some of these problems can be alleviated by having multiple segments, but multiple segments has a problem in that the programming model becomes complicated. Programming model, if you have multiple segments, then the compiler has to worry about, you know, which segment I have to access, as opposed to just saying, here's an offset, and that's it. Okay. So paging is another way of implementing virtual memory, all right? Here, what's done is, let's say this is a physical address space, I'm going to call it PA, physical address space, PA space, all right? What it does is, it divides the physical address space into fixed-size chunks, right, called pages. And each of these sort of frame is called a page, right? And then it divides the virtual address space also into pages of the same size, also a page. And then it has some function in the middle, F, which is implemented through a table, a lookup table, which maps a page in virtual physical address to a page in virtual address, a page in virtual address to a page in physical address, completely arbitrary mapping. So the virtual address space also, you know, just like before, starts from, let's say, zero to some value, let's say, you know, let's say it goes all the way to two to the power 32 minus one, because that's the maximum addressing that you can do anyways, right? Because it's a 32-bit machine, so the maximum size of the operand that you can have for a memory address is 32 bits, because the registers are 32 bits, the immediate operand is 32 bits, even when you execute in displaced mode, it gets wrapped around to 32 bits, all right? So the virtual address space goes from zero to two power 32 minus one, and the physical address space goes from zero to whatever maximum M you have, right, depending on how much memory your machine has, whether it has 512 MB, 1 GB, so that's the value of M, right? And this function is going to map these pages in this address space to pages in the virtual physical address space. Now the program just works as before, the program just assumes that it has this virtual address space, it just, you know, once again it just says, I want to access address vadder, I want to access an address called vadder. What happens is you go, you say, oh, vadder is here, let's say, so you say, oh, it belongs to page number this, right? You basically do some calculation to figure out, this is the page number that it belongs to, and you figure out, oh, this page number is living in this page number of physical memory, so you do that translation, and then you convert that vadder into a PRR, okay? So what's the nature of this translation? Well, let's say the page size was P, and let's say the page size was P, and P was, you know, equal to 2 to the power of small p, so let's say P was some multiple of, some power of 2, just to make things simple, because, you know, we are working on a binary machine, where bits are binary, and so, so if I want to convert vadder to P adder, what I'm going to do is, I'm going to say vadder divided by capital P, that's going to give me my page number in virtual space, so I'm going to call this the virtual page number, VPN, right? And I'm going to convert VPN, I'm going to look up a table to convert VPN to PPN, okay? That's a physical page number. And now I'm going to say, you know, P adder is equal to PPN into P plus vadder mod P, right? That gives you a one-to-one mapping between, assuming there was a one-to-one mapping between VPN and PPN, this gives you a one-to-one mapping between vadder and P adder. So basically, you divide vadder into a virtual page number and an offset inside that virtual page, right? You convert VPN to PPN, and then you add that offset, so this is the offset, this is called the page offset, right? So you divided vadder into a virtual page number and a page offset. Then you converted VPN to PPN using a lookup table, and then you computed P adder as this, right? You just dereferenced memory at that page and added the page offset to that. If P is a power of two, then these operations are very simple, vadder by P is nothing but vadder shift left by P bits, small P bits, right? And the vadder percentage P is just vadder and, you know, one less than P minus one, right? So this gives you, it's a map which has the last small P bits set to one and everything else set to zero, right? And so, and PPN into P is nothing but PPN shifted left by small P bits, okay? So now the question is how big should this capital P be, or, you know, consequently small P be? How big should it be? If it's zero, if small P is zero, which means capital P is one, then, you know, I have extreme flexibility in where I can place each and every byte, right? I can place each byte completely anywhere in the thing, but the lookup table, how big is the lookup table going to be? Pretty big, right, because for each byte I need an entry which maps it. So on the other hand, if I have P as a very large number, let's say P was equal to, let's say, let's say P was equal to 10 megabytes, or, you know, power of two, let's say 32 megabytes, okay? Let's say P was 32 megabytes, then, you know, firstly I can, now my lookup table becomes really small, right, because what's the size of this address space? It's zero to two to the power of 32 minus one, which is four gigabytes, and the size of memory is also limited by whatever physical memory I have. So let's say if I had only 512 MB of physical memory, then the number of pages I'll have, if I have a 32 megabyte page, is 512 divided by 32, which is, let's say, 16, right, okay. So 16 pages in the entire thing, so my lookup table is going to be just mapping 16 integers here to 16 integers there, right, so my lookup table is only going to need 16 entries if my page is that big. But the problem with that is, if most of my processes are really small, then each time I allocate a process, I need to actually allocate at least 32 megabytes of space, number one. So I'm wasting a lot of space in the middle, right? And okay, so, you know, here's an engineering decision to be made, what's the size of a page, okay? And the typical size of a page that's used in modern hardware is 4 kilobytes, so capital P is equal to 4 kilobytes, and small p is equal to 12, so 2 to the power 12, okay. So we're going to, you know, as we go along the course, we're going to see why, you know, a number like this makes sense, you know, it actually depends on the physical characteristics of the hardware, et cetera. But let's just first understand, assuming that a page is 4 kilobytes, how does this mechanism work? So a page is 4 kilobytes, what happens is a virtual address is 32 bits, of which the last 12 bits are called the page offset, and the top 20 bits is called the VPN, right? And similarly, in the physical address, this is the PPN, right? Now I need to design a lookup table, which maps VPN to PPN, in other words, I need a mapping from 2 to the power 20 numbers to another 2 to the power 20 numbers in the most general case. So one way to do that is to have a table of 2 to the power 20 entries, right? So I could have one big table, let's call this the page table, which has 0 to 2 to the power 20 minus 1 entries. And I'm going to take the VPN and use it to index this table, right? And whatever value is stored there, that's going to be called my PPN, okay? That's one way to do this. How big is this table? Each entry will need to be 32 bits, right? So that's 2 to the power 5, into the number of entries, into 2 to the power 20, that's equal to 2 to the power 25, or 8 MB. The question is, shouldn't each entry have 40 bits instead of 20 bits? No, because the VPN is actually being used to index the thing, right? And so you save space in that sense, by doing this kind of thing. Another question could have been, you know, why do I use this thing where I have a huge page table, where I'm actually assuming that VPN can be used to index it? Why not have something more efficient, like a link list, which has all these VPN to PPN mappings, right? That's likely to be smaller in the common space. But you know, it's a time to actually walk through that link list that's going to become the bottleneck. So as you can imagine, you're going to actually do this translation on each and every memory access. And so the most important engineering decision you have to take is basically to minimize this translation overhead. You want to minimize the translation overhead, it doesn't matter, and space can be optimized in other ways. It's possible. Why is the VPN 32 bits? It could have been 20 bits, but the memory itself is byte-addressable. So if I say it's 20 bits here, then the address of the first entry becomes 20th bit. And so it doesn't make—I cannot access a bit in memory. I can only access memory at the granularity of a byte, number one. You could say, oh, why does it need to be 32 bits, why can't it be 24 bits, for example, right? Which is three bytes. Well, that's possible, but actually that also complicates hardware. You know, it's better to actually have power of twos. The other thing is you actually use the extra 12 bits for other purposes, which we're going to discuss later. Okay. So that's a lot of space. So let's say this is 32 MB, right? I'm getting it right? Okay. So 32—hold on, hold on. So there's 2 to the power of 2, right? So 4 MB, right? That's 32 bits, which is 4 bytes. So 4 MB of space. So clearly 4 MB of space cannot be allocated on chip, right? Because we said chips can only have some registers which are a few tens to hundreds of bytes. So 4 MB of space cannot be allocated on chip. So this data structure has to live on memory. But if this data structure has to live in memory, you're saying that every process will have its own page table, because every process will have its own virtual address or physical address mapping. So every process will need at least 4 MB of space to at least store the page table. Typical process sizes will be a few hundreds of KBs to 1 MB to 10 KBs. You know, processes like GREP and LS and all these, these are very small processes. And you know, if for every process I'm actually creating a page table that's 4 MB of size, that's a lot of wastage. So what's done is basically you divide the page table into a 2-level page table. The first level is called a page directory. And the second level are called page tables, where each of these entries in the page directory points to a page table. So once again, what I do is I look at the VPN, and I take the top 10 bits of VPN. So let's say VPN bits 31 colon 21, let's say, 22 to 31, right? And we use that to index the page directory. I get some address. This contains the physical address of the page table. And then I look, and then I use VPN 21 colon 12 to access the corresponding page. And so I get a page. And then I use VPN 11 colon 0 to offset into the page, right? So I've just, I mean, what I've done is I've divided the page table into a 2-level hierarchy. I said, why not just have a list or a tree to store this? And we said, you know, having a list or a tree is actually going to increase complexity of hardware. But at the same time, you need to strike a trade-off between how complex the hardware needs to be and how much space you are allowed to waste. And so, you know, the designers thought, let's just divide the table, this large table of 4 MB, into a tree of tables, but allow the tree to be, the tree must be of size 2, or depth 2. So use it, so in this case, I was using 20 bits to index into this page table. In this case, I'm using the first 10 bits to index into the first page directory, and using the next 10 bits to index into the second page directory, or page table, okay? So this way, you know, if I have a process which is actually taking only, you know, let's say 100 KB of space, so it only needs 0 to 100 KB mapped in the address space. So what will happen is, you know, one or two of the entries here are going to be filled, and all the other entries are going to say they are not mapped, right? And that way you save space. So only one or two entries are filled in the page directory, and you only have one or two page tables in that case, and the total amount of space that you're using for this page table structure is, let's say, three of these, right? One here, and one of each. Let's say two of these, or one or two of these, depending on the size of the process. Let's see what the size of the page directory itself. I'm using 10 bits to index into the page directory, so how many entries does it have? 2 to the power 10, right? And how big is each entry? Each entry contains a physical address of the page table, and how big can a physical address be? 32 bits. So it has, but a physical address can be 32 bits, but you have also segmented the physical address space into pages, right? And so the number of pages you can have in the physical address space is 2 to the power 20. So really, each entry needs to be 20 bits, right? Each entry needs to be 20 bits? And so, but once again, 20 bits is not a very nice number, so we extend it to 32 bits, and so it becomes, 32 bits is 4 bytes, right? So it becomes 4 into 2 to the power 10, which is 4 KB. So that's the size of the page directory. And interestingly, that's the same size of the page, right? So a page directory itself lives on a single page. Entry, what's the size of the page table? Same thing, right? You're using 10 bits to index in the page table, and each entry is 4 bytes, so it's 4 KB again. So once again, a page table itself lives on a single page, right? Then you use that to index a page, and you get the page. Okay. So let me just review this. This is the virtual address, VA. You divide it into 10, 10, and 12, so it's a 32-bit address. You use the first 10 bits to index into a page directory, I'm going to call it a PD, page directory, and you're going to get a PDE, page directory entry, using this. You're going to get the contents of the PDE to dereference a page table, call it a PT, and you're going to get the next 10 bits from here to index into the page table, and you're going to get a page table entry. And now you're going to get a page from here, finally. You've got a page, and you're going to use the last 12 bits to index into this page to get your final physical address. Let's see how this works on x86. So x86 actually uses both segmentation and paging. So a virtual address of the form segment colon offset, this is called a virtual address, VA, goes through the segmentation hardware, as we discussed earlier, to give you another address, which we have been calling the physical address so far, but actually gives you what's called a linear address, right? So let's call it the linear address, linear address. The reason it's called a linear address is because now this address has no segment colon offset, it's just one number, it's one 32-bit number, so it's a linear address space. Now this goes through paging hardware to give you a physical address. So it gives you both, an operating system is free to use both, an operating system is free to use only segmentation, in which case all it needs to do is disable paging. So how do you disable paging? It has certain registers which are called control registers, CR0, control register 0, CR1, CR3, let's say CR2, maybe more, and you know there's a bit in CR0 which says enable or disable paging. If you disable paging, you're only using the segmentation hardware, and that's likely to be quite fast, right, because you are not going through the paging thing, which involves looking through the page table. But actually, as we're going to discuss later, paging is also not all that slow, and there are ways to make that fast, and this is basically what's typically used, it has advantages. So an operating system is free to use only segmentation, or an operating system, by disabling this bit, or an operating system is free to use only paging, right, in which case what does it need to do? All it needs to do is basically set all its, set its segment selector that we discussed in our first slide, okay, so all it needs to do is set its, set all its base and limit to 0 and 2 to the power 32 minus 1, right. If in all its segment descriptors, it sets its base and limit to 0 to 2 to the power 32 minus 1, it's effectively disabled segmentation. So in the next few lectures, I'm going to assume that the base and limit have been set to 0 and 2 to the power 32 minus 1, which means the virtual address is equal to the linear address, okay. And most operating systems actually do this, right. So most operating systems like Linux, Windows, and the operating system that we're going to do in this class, which are x86, PsyOS, and Pintos, will all set base and limit to these values, and so only use paging for implementing virtual memory. What is the use of having segmentation? Well, the hardware designer has given you a choice, basically, right. And the hardware designer designed the hardware before the operating systems were written on it. So there was a, I mean, he basically tried to say that, I don't know what the operating system may want in future, and so he designed it, and because of backward compatibility, it remained in the hardware. In fact, on the 64-bit architecture that have, you know, come up recently, segmentation is not present. So you don't have segmentation in this form. It's a very, very thin form of segmentation that you need, because most operating systems are not using segmentation. Okay. All right. So I'll stop here, and we're going to discuss more about paging in the next lecture."}