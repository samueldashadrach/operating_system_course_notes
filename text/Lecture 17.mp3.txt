{"text":"Okay, welcome to operating systems lecture 17. So far, we have looked at how the kernel initializes itself and initializes address space. And we have seen that the kernel basically initializes address space such that it firstly starts itself at current base, which on x86 is this address hexadecimal 8 and 7 0, so it is 2 GB. It is a 32 bit address space, so it is right in the middle of the 4 GB address space on 32 bits. The first one MB is for low memory devices, it just maps it one to one to the physical memory. In fact, the entire space here is from current base to current base plus fizztop is mapped one to one to physical memory from 0 to fizztop, right. It loads the kernel's code and data in this area, right. And the rest of the space which is available from this space onwards is what is called as heap. And that heap can be, is managed using these functions called kalloc and kfree, right. So if I want to allocate some space, I will call kalloc and it will allocate a page from the space for me. And that page I can safely assume that will not be overwritten by anybody else. And then kfree is adding that page back to the free list so that it can be used. So memory can be reused and that way you can sort of not run out of memory. And this lower address space from 0 to current base is reserved for user processes. So, so far we have not seen any user processes, but now we are going to talk about how user processes are created, loaded and all that. We already have some idea. The user process will be executing in this space. So the user processes code and data will live in this space. The user processes stack will live in this space and user processes heap will also live in this space, right. If a user ever makes a system call, it will be executed like as a software interrupt instruction and the software interrupt instruction will dereference the IDT through the IDTR register and from that it will get the address of the handler. The kernel will set up the IDT in such a way that all the handlers are pointing in the kernel's code and data region. So the handler will execute in kernel mode, right, with kernel's privileges, right. So the CF in the IDT will also say that, execute this particular handler in privilege mode, right. So this is how things are going to work from now on and forever, really, right, as long as the system is going on, right. If I have to sort of show this in more detail, let's say there were two different processes. Both of them will have the same kernel mapped. So let's say this is kernel and kernel. So above kernel base, they have the same area mapped, identically, right. From kernel base to kernel base plus this top, same thing. The entire physical memory is mapped. Of course, the process cannot access it directly because these are privileged pages and only code that's executing in privileged mode can ever touch these pages, right. So the process is executing here. It may have, different processes may have different mappings. For example, in these red lines here, in process P1, let's say this page is mapped and this page is mapped. And in this process P2, let's say this page is mapped. So they are different address spaces in that sense, right. And they are completely independent address spaces. Also note that these pages, which are present in the address space of the user, are also coming from the physical memory, right. And the physical memory is also mapped in the kernel space. So the same pages in physical memory have two mappings in the virtual address space, one in the user side of things and another in the kernel side of things, right. So where will these pages be allocated from? From the kernel's heave, right. So there's the kernel's heave. So in the kernel space, there is some space that is static, which is for kernel's code and data. And then above that, all the other area is where I'm going to allocate a page and create this mapping for the user process so that the user process can implement its own address space, right. So all the user pages will have two mappings, one which will be the user's mapping so the user can access it, and one which will be the kernel's mapping for the kernel to do its own bookkeeping, right. Of course, the user can only access this page through its user address. The user will not be able to access the page through the kernel address because the kernel address is protected, it's privileged, right, okay. So by ensuring that only the pages which belong to the user are ever mapped in this area, the kernel basically ensures that the user cannot touch any privileged memory or the user cannot, the process cannot touch another process's memory, all right. Also, we said that if, once again, if a process needs to make a system call, then some code here is going to execute the interrupt instruction, let's say it says int some number which indicates the system call number. In Linux, it's the hexadecimal 80 number, right. So interrupt hexadecimal 80, what it's going to do is it's going to go through the IDT and it's going to transfer control to an EIP somewhere here, and that EIP should be the handler for that particular system call, right, or the system call in general. The user can give arguments to the system call by setting up its register in a certain way. So the first register can say, what is the name of the system call, right. So you can say that, let's say, the fork system call is number one, exit system call is number two, and so on, and that way you can basically set it up to say that this is the name of the system call, and other registers can say, give arguments, right. So integer arguments can be just given in register, for example, you know, ECX can contain the first integer argument, let's say file descriptor. But if I was supposed to give any string, let's say the exit system call takes a string argument, right, and a string can be pretty large, and registers are not enough to hold a string. So the way to do it is basically the string is going to live somewhere here, right. So let's say there is a file called foo, and the string called foo is living here. And so what the user does is he sets up the value of the pointer, which points to foo as a first argument, right, in the register, let's say ECX. And so the kernel looks at that pointer, and because the kernel has the same page directory mapped, so the same address space is mapped, when it dereferences the pointer, it can read the string foo, right. So this is important to understand. When the user makes a system call, it is possible for the user to actually give pointers to the kernel to specify its arguments, right. And the reason the pointers work is because the kernel will execute the system call in the same address space as that of the program, right. If the address spaces were different, then pointers couldn't have been passed. So this is a huge optimization. I mean, this is a big advantage of doing things in this way, as opposed to doing things in a way where you were using segmentation, for example, because segmentation will change the address space for the kernel, or using a separate page table for the kernel, right. So you know, often you may have wondered why I am eating up so much of space for the kernel in the virtual address space. That way I am basically squeezing out the process, right. A process could ideally have had as big as a 4GB space on 2 to the power of 32-bit machines, but now, you know, these kernels like XV6 or Linux or whatever else is basically constraining the process to live only in 2GB, right. But there's a big advantage to doing that. The advantage is that the user and kernel can now talk by pointers. If I want to communicate some information, I can just pass a pointer, and that pointer works. So the kernel can also reply by setting some value in the address space of the user and give a reply. On the other hand, if the kernel had a separate address space, then pointers wouldn't have worked. Right? Okay. All right. So... Yes, question. Sir, if the physical space is small... Okay. So the question is, if my physical memory is small, then can my virtual address space of a process be large, larger than the physical memory? The answer is... Yes. Yes. Why? Because... So you can make pointers, first of all. So, you know, the virtual memory doesn't need to be contiguous. It can have non-contiguous mapping. So even though the physical memory is contiguous, the virtual address space is non-contiguous. Some pages are mapped, some pages are not mapped. Also, you can map the same... A user process could very well say, I want these two addresses to map to the same physical page. Right? So you can basically say... You can have aliasing. Although it's not very useful to have, you know, two physical addresses... Two virtual addresses pointing to the same physical address in the user space itself, but it's possible to do that. So in any case... So, I mean, to answer your question, it's possible for the virtual address space to be bigger than the physical address space. Okay. So the question is, I mean, if I assume that my physical memory is never going to be greater than 2 GB, then this organization makes sense, right? But in real world, that's not true, number one. All right? So my physical memory could be greater than 2 GB, and still I'm restricting my process to have only 2 GB, at most. If you're using XP6. If you're using Linux, you are restricting it to 3 GB, right? That's not a great idea... Great design. I mean, today we have memories of 64 GB, even 128 GB. So restricting that a process can only access 3 GB basically means, if I want to actually write a program that needs that much memory, I need to have a set of processes, if I'm executing on a 32-bit machine, right? On the other hand, if I'm executing on a 64-bit machine, the problem gets solved automatically. Okay. Yes? Okay, so the question is, couldn't the user... So I said that the user can pass pointers to the kernel. It can set up some area in its own memory. It can initialize some values in its own memory, and use the address of that memory location and pass it to the kernel, and the kernel can dereference it, and because the kernel is executing the same address space, it is okay to dereference it. The question is, if the kernel was executing in a different address space, could the user have figured out what the pointer is in the kernel's address space, to the location that it has set up, and pass that value to the kernel? That's very difficult, right? How will the user know what is the value of the kernel's pointer in that address space? So you need some interface to basically be able to communicate that. And if you have to do it for all the addresses in the user space, that's very costly also. So, you know, these are interesting ideas, questions, and something to think about on your own for some time, and then we can discuss about it more. So a lot of thought has gone into what is the right operating system design, and some... So the mainstream kernels that we use today have taken this design because of its efficiency. Because the communication between user and kernel is very fast, just exchange a pointer. And that makes things, you know, speed has by far been the most crucial factor in deciding a design. There have been other operating systems which are actually popular, not necessarily in the mainstream desktops and, you know, space, and server space, but maybe in the embedded devices space, where security is more important, for example, right? And those kinds of organizations are what's called microkernels, right? Here the idea is that the kernel subsystems live in separate address spaces. And so this gives them very strong isolation from each other, right? So, for example, there will be one process, which is the system call handler, right? And so if you want to make a system call, what you do is you just pass the arguments, but now you need to copy your arguments from one address space to another address space, and the other process which is doing the system call handling is going to read those arguments and then execute it, right? So these are all organizations, but I mean, they have their pros and cons. If you basically divide the kernel into separate address spaces, it gives you very strong protection, isolation, but it decreases performance, right? So that's the tradeoff. Question? Why does, so why does the kernel of, I mean, so firstly, there's only one kernel, right? It doesn't mean there's no kernel of P1 and kernel of P2. It's one kernel, which is not common. Let's say, if you are working in the address space of P2, then why doesn't, why does it need to know? Right. So, so these mappings in the kernel side of things remain even in P2, right? These mappings get removed, but these mappings still remain, and these allocation data structures also remain. So it knows that this area has been allocated already, right? And so the data structure, the free list itself is living in the kernel space, right? Okay. So the free list itself is in the kernel space, and so even if you switch to P2, the free list remains just where it was, right? So all the data structures to do all this bookkeeping are living in the kernel space, and because kernel space is shared between all the processes, you basically have, you know, you don't, information is not lost. The kernel's information remains. Question? Okay. Very interesting question. If the pages in the user space are allocated from the kernel's heap, then shouldn't the kernel's heap be necessarily larger than the size of the user space? Really? So, does it need to be larger? Because as we said, that, you know, the address space can be bigger, the actual allocation size can be smaller. So you only say, you know, let's say you, the address space of the process could be 3 GB, but the allocation size could be, let's say, only 5 MB. If you wanted to make larger allocation sizes, let's say greater than, you know, 1 GB or 2 GB, then as we said before, you know, XP6 is a very simple operating system which requires that the entire physical memory is mapped in the kernel address space. But modern, like, full operating systems like Linux or, you know, anything else is basically going to, not going to map the entire physical memory in its address space, it's going to recycle the address space and make it point to different regions. And it's going to do bookkeeping at the physical page level that which pages have been used and which have not been used. Right? So it's not necessary. XP6 is one example of an operating system which maps it, which requires that the entire physical space is mapped in the kernel address space. But that's not necessary. Right? It makes things simple, and that's why we are discussing it first. All right? Okay? All right. So we have already seen this. There's an interrupt descriptor table that has CS colon EIP pointers. CS has the last two bits as 00, which means it runs in privileged mode. And EIP is somewhere in the kernel's code region, so the handler gets called. Right? And these pointers which are initialized, these are initialized by the operating system itself. And we have already seen that the user is not allowed to overwrite either the IDTR, which points to the IDT, or the entries in the IDT itself. So the IDT itself is going to live in the kernel space, so clearly the user cannot access it. If it was able to access it, then it can just bring down the system. All right. Okay. So... So let's look at how a system call works. Right? So let's say this is user space, and this is kernel space, and this is a process P1. Right? And in the kernel space, every process has a stack. Now let's assume xv6 is process's model, so every process has a separate stack, kernel stack. P1's kstack. Right? When the process makes... Uses the interrupt instruction to make a system call, immediately the stack shifts... The stack points to... Starts pointing to kstack, right? And the hardware pushes some things on the kstack. What are those? Let's say... CS, EIP. These are user's CS, right? User's EIP. Whatever was the old CS. Whatever was the old EIP. E-flags. So whatever were the values of these registers before the interrupt? S1, S2, S3, S4, S5, S6, S7, S8, S9, S10, S11, S12, S13, S14, S15, S16, S17, S17, S18, S18, S19, S19, S20, S21, S21, S21, S21, S21, S21, S21, S21, S21, S21, S21, S21, S21, And so these get saved on the... So let's say this was top of the kstack. So now the new ESP points here, right? So this is user's ESP. So that's what happens immediately after you make a system call. As soon as the interrupt instruction is executed, the state of the architecture starts looking like this. CS, EIP, E-flag. And stack has shifted. So whatever was the user's stack, that is of no consequence. It has just been saved, whatever. That's represented by ESP. And this ESP must necessarily be a pointer in the user's address space. Similarly, this EIP must necessarily be a pointer in the user's address space, right? Okay. So now the handler gets called. So at this point, the handler gets called. And the first few things it's going to do is it's going to say, you know, it's going to have instructions like push other registers. Let's say push EAX, ECX, you know, other segment registers, let's say DS, and so on, right? So it's going to push all these registers. And so eventually, at some point, ESP is going to come here, and you're going to have, let's say, user's EAX, and so on, on the stack, right? So the kernel stack is a nice place where you can save all this information about what were the values of the user registers at the time of this interrupt instruction, right? And so this entire structure is called the trap frame. It contains all the registers that have been saved by the handler. And it contains all the registers that were saved by the hardware, right? And this trap frame is important. So the reason it's called a trap frame is because it's a frame that is initialized on every trap, right? And the reason it is important is because from here, the kernel functions can figure out what were the values of the user registers at the time of the interrupt system calls. For example, if it wants to know the arguments, it just needs to look into the trap frame. Also, the return value can be passed to the trap frame. If I want to pass the return value in the EAX register, then all I need to do is just overwrite the trap frame, EAX, right? At the time of return from interrupt, this trap frame will get popped in the same order in which it was pushed, in the opposite order in which it was pushed. And so first few values will get pushed, popped by the software. And then the irate instruction is going to pop the last five values. And the user will continue as though it never went into the kernel, except that the return value would have changed. And maybe some other things would have changed, depending on the semantics of the system call. So basically, if I were to draw the K stack, at any point, it will initially have the trap frame, right? Then it will have some other, you know, whatever function call chain within the kernel. The handler will push all these registers, and then it will maybe call some function, depending on what vector it was. For example, it will call a syscall function. The syscall function will read the first argument. And let's say it was an exec system call, so it will call the exec system, exec function or something. So all these functions, their function call frames are also saved on the stack, right? So you basically keep pushing this stack like this, right? And at some point, you would have handled the system call, which may involve overwriting values of the trap frame. And then you will start returning from the function, just like you return from normal function calls, right? And at some point, you will return from the trap frame, return from the kernel to the user. And so the last function that should be called is a function that pops off the trap frame and then calls iret to return to the user mode, right? So on x36, this function is called the trapret function. So in other words, what should happen is that whatever is the function call that you called, the last return address should be trapret. So when the last function returns, it should return to trapret. And trapret will basically just pop off all these registers and then call iret to return back to user mode, right? So the stack does not only contain the values of the registers. It also contains the values of the instruction pointer that should get called and in what order, right? Well, the stack basically acts as a function stack. And so every time you return, you basically pop off a value from the stack and jump to it, right? So if your stack contains a reference to trapret, then when you return from the function chain, you are going to next execute trapret. And trapret is going to do all this for you, and then you're going to jump back, right? So in other words, the kstack, the kernel stack of a process contains enough information to say where should you start executing and what all should you execute next and with what values. And eventually, it should typically just go back to the user after it has finished execution. All right. Okay. Yes, question. Okay. The question is, will trapret pop the entire trap frame and then call iret? No. Trap frame will pop only the registers that were saved by software, right? And the last five registers will be popped by iret, okay? So the important thing is that the entire information about the kernel side execution of the process is encapsulated in the stack, kstack, right? So let's say a function wanted to call the yield system call. So so far, we have seen a system call that binds the kstack and then unwinds the kstack and then returns to the user mode. But yield system call is a call which requires you to switch the process, right? It's basically saying, I don't want to run anymore. Let somebody else run, right? So yield is an example of a system call which will enter the kernel on p1 kstack, but exit the kernel on p2 kstack, right? And in the middle, this switching of stacks is what's called a context switch, right? So basically, when a process calls yield, let's say p1 calls yield, p1's kstack will get formed. The entire chain will get saved on the kstack. And at some point, it's going to say, okay, let's switch the stack, right? And so you're going to go to p2's kstack. You're going to go to p2's kstack, and instead of unwinding p1's kstack, as in the previous example, I'm going to unwind p2's kstack. And then I unwind p2's kstack, and also I'm going to change to p2's page directory, right? So this was p1's page directory. And so what happens? So this process of shifting from one process's kstack and one process's page directory to another process's kstack and page directory is called a context switch. So you change the kstack from p1 to p2. You change the page directory from p1 to p2. And then you just execute on this kstack. And then when this kstack gets unwound, just like before, so by returns, you actually end up in p2, exactly as it was executing the last time it got preempted, right? So what has happened is this kstack will get saved in memory, right? So this kstack will get saved, and this kstack will become active. At some later point, if let's say this process calls yield, and then it wants to shift to this kstack, all it needs to do is change to this kstack, change to this page there, and now this one will get unwound, just like it would have gotten unwound even if there was no context switch, right? So that's how a context switch works. One process makes a system call. You execute in the kernel. You form some state in the kernel, which is saved in the kstack. You switch the kstack. You switch the page directory. You unwind the kstack of the other process, and you are suddenly executing exactly as you left the other process in some previous iteration of this procedure, right? So the page directory of a process and the kstack of a process contain the state of the process, the running state of the process as it was left the last time it was context switched out, right? So this process is called switched-out process, right? And this one is called switched-in. So the kernel maintains a list of all the switched-out processes. The scheduler picks one of those switched-out processes, and a switched-out process must have a kstack and a page directory. You switch to that kstack, and you switch to that page directory, and you are now running in that particular process. That's how a context switch happens at the process level. So this list which contains all these process states is basically the list of PCBs, right? So typically the process control block will have pointers to its kstack and pointer to its page directory. So there's a list of PCBs, so there's a list of PCBs, and the scheduler will pick one PCB and then switch to its kstack and its page directory, and that's it, that's a context switch. So all the information that was necessary to encapsulate the information exactly where that process got context switched out is available in the kstack. Why can't we save the pointer to the page directory on the stack? Okay, that's a very interesting question. Can you? Can you not? Well, actually, in theory you can, nothing stops you, you just have to make sure that it never gets overwritten, right? Recall that when there's an interrupt, immediately you switch to a kstack, and things get pushed on the kstack. So you have to make sure that the top of the kstack that you initialize, that you put it inside the TSS, the task statement, is not such that you ever overwrite that page directory. So you can use one, you know, the top plus one area to basically contain the page directory also, right? Is it a big win? Perhaps not, because, I mean, what have you saved? You know, you're still using the same amount of space overall. You could have saved it in the PCV, you're saving it in the kstack, perhaps complicating things a little bit for yourself. Okay, so here's the interesting thing. Because each process has the same mapping of the kernel everywhere, when you context switch the page there, it's okay, right? So the nice thing about this picture here is that in this context switch, when you shift it from P1's page day to P2's page day, because you are executing in kernel mode, and the kernel is mapped identically in all the address spaces, there's no problem. You know, it's not like the EIP has changed, or the ESP has changed, or the contents at EIP have changed, or the contents at ESP have changed. All the values currently are kernel values, and even if you change the page directories, you're okay, right? You couldn't have done this, for example, if you were executing in user side, right? Because you have different address spaces. The moment you change the page day, your EIP will no longer be valid, the next EIP. But that does not mean that they have the same page day, right? I mean, it's the same space from which they have allocated different regions for different processes. So one case, so in the same K heap, there will be one page for P1's kstack, and another page for P2's kstack. There will be yet another set of pages for P1's page days, and yet another set of pages for P2's page days. They have to be completely disjoint structures. It's in the same space, but they have different areas that have been allocated for different processes. Okay? Good? Okay. So we have seen how yield works, and yield is an example of a process saying, I want to give up the CPU, I want to be a good citizen, I want to give the CPU to somebody else. But we also said that, you know, if a process is not yielding the CPU, and the process does not oblige to yield the CPU really, then the OS should have a mechanism of taking away the CPU from the process. And the way to do that is the timer interrupt, right? So we said that the OS can, you know, there's a device called a timer in the hardware, and the OS can configure its frequency, can say that fire every X number of milliseconds. And so each time the timer interrupt fires, it has the same effect of an interrupt, right? So once again, the IDT gets consulted, and the appropriate handler gets called. Earlier, it was the syscall handler that was getting called. This time it will be the timer handler which will get called, let's say. Okay? So again, the mechanism will be identical. So instead of interrupt 0x80, let's just say an external timer interrupt occurred. And once again, P1 will shift to kstack, and it will push all these values on the – so these five values will be pushed by the hardware, other things will be pushed by the timer handler. And now it will call the scheduler, right? And it will be the identical thing. I mean, just like yield – I mean, the only difference between a timer interrupt and the yield system call is that a timer actually forced the preemption. Yield was a voluntary preemption. The mechanism is identical, right? Once again, the kstack will contain all the information about exactly where the process got preempted and what were the values of its registers in the trap frame, right? And the kernel will execute. So as the kernels call chain, that will also get saved in the trap frame. And then you context switch, just like before, right? You will context switch. Another process is kstack, would be saved. The other process may have been preempted by a timer interrupt or may have been preempted because of a yield call. In either case, the unwinding process is similar, and you go back to the yield, right? So that's how an involuntary context switch happens. The previous case was a voluntary context switch where somebody called yield. This is an involuntary context switch where a timer interrupt occurs. And the scheduler decides that you need to be switched off because you've been running for long. Are the handlers the same? No. I mean, they are roughly calling the same functions at the end, but initially they are slightly different. Okay. One is a system call. One is not a system call. Okay. All right. Very good. So now we understand how timer interrupt works. Let's say a timer interrupt occurred, and the scheduler decides that it does not want to context switch. That's a possibility. No problem. You will not switch the stack. You will just unwind the same stack again, and you are back where you were interrupted. All right. So one question that you may have is, you know, this whole business of actually a timer interrupt coming and, you know, this kernel code getting called is an unnecessary nuisance if there's only one process that's going to run. You know, let's say you are running some high-performance computation. You have written some very fancy program, and you're running it, and you know it's the only program that's running in the system. And now this timer interrupt comes and bothers you every time, and only for the scheduler to say, oh, this is the only process running, and I'm just, you know, I just bind the stack and then unwind the stack. And what is the overhead of this? All right. So the overhead depends on what? On the frequency of the timer interrupt, right? How fast is the timer interrupt occurring, right? So and so let me just first tell you, you know, what kind of numbers are there. Let's say 10 milliseconds, 200 milliseconds is the duration between two timer interrupts, right? And let's say, you know, 10 to 100 hertz is basically the frequency, typical frequency that you use for your timer interrupt, right? On modern machines, one instruction, so this is timer interrupt, this is one by timer interrupt frequency. So let's say 10 to 100 hertz, right? And this frequency, and how long does it take to execute one instruction on a modern processor? Let's say, you know, two point some gigahertz or whatever, so it's, you know, roughly on the order of one nanosecond. So how many instructions can execute in one timer slot? Let's say 10 to the power of 6 to 10 to the power of 7 instructions, or let's say 10 to the power of 5 to 10 to the power of 7, even if you want to be, you know, if you want to say that memory access is going to take a lot of time, you know, memory is not so fast and whatever. So 10 to the power of 5 to 10 to the power of 7, right? So that is the number of instructions that get executed, roughly speaking, in one timer slot. What is the overhead of one timer interrupt if you were not going to contact switch? It is, you know, one switch into the kernel, executing a few function calls, returning from a few confunction calls, and going back. How many instructions do you think will get executed in doing this? Somebody says 100. 20? Okay, let's say, you know, 1,000 is, you know, a very conservative number, so 1,000 instructions. So that's basically, let's say, 10 to the power of 3, right? So the overhead is something like 10 to the power of 3 upon, you know, very conservative figure, 10 to the power of 3 upon 10 to the power of 5, that's 1%, right? Actually it's going to be less than 0.1%, you know, actually, so this is 1%, of course, but roughly the overhead is less than 0.1% of doing this time on modern hardware, right? Of course, you know, you may say, oh, but, so what happened in the early days, you know, so we right now have two gigahertz machines, let's say, not very, not too long back, let's say, 20 years back, we had only 100 megahertz machines, maximum, right? So when you had 100 megahertz machines, this overhead may have increased because the number of instructions you can execute is smaller in a time interrupt, but then, you know, you also used, you know, larger values of the frequency, let's say 100 milliseconds. In fact, you know, you can safely decrease the timer interrupt frequency from 10 milliseconds to even 1 millisecond and still not see any overhead, right? But, you know, there is no real advantage of doing that, and exactly why you will, why, you know, what other factors go into deciding what the timer interrupt frequency is, we are going to discuss as we go along the course. Okay, shouldn't the timer interrupt frequency depend on the number of processors that are running? It's mostly, why, no? Firstly, let's understand that this timer interrupt is being generated per processor, right? So every processor has a separate timer device attached to it, in theory, right? How it works in physical hardware is a different thing, but let's just assume that each CPU has a separate timer device attached to it, right? Each CPU is doing its own scheduling, basically. And of course, the scheduler is a common scheduler that understands that there are many processors in the system, and it basically chooses whether, you know, this process is supposed to run on the CPU or not, et cetera, right? So every CPU has one timer device. So, I mean, a timer interrupt frequency should really have no relation to the number of CPUs in the system, right? In fact, each CPU can have a different timer frequency, for that matter. Does it make sense? Maybe not, but, you know, it's possible. Wouldn't it become hard to synchronize all these things if each of them had a different timer interrupt frequency? What do you mean by synchronize? Okay, so the question is, do these processors need to be synchronized? Is it necessary that each processor gets a timer interrupt at exactly the same time and comes back exactly at the same time? No, not at all, right? So this processor could be executing, you know, could get a timer interrupt now, and another processor could get an interrupt nine milliseconds from now. Perfectly okay. Doesn't matter. They don't need to be synchronized with each other. In fact, synchronization will be very hard to implement in hardware. You know, that level of synchronization is almost impossible to achieve. They don't need to be synchronized. So when the timer interrupts are separate, they act as completely asynchronous devices. Okay. Finally, I gave you an example where the timer interrupt occurred while the processor was executing in the user mode, right? So I gave you an example where I said, okay, I gave you an example that P1 was running and the timer interrupt occurred in the user mode, right? And so all this started happening after that, right? But can a timer interrupt occur while the processor was already executing in the kernel mode? Yes. Why not? I mean, the timer interrupt has no idea whether you are executing in kernel mode or not. It's possible for the kernel to disable interrupts while it's executing the kernel mode, but on xv6, it doesn't do that, right? So while the processor is executing the kernel mode, a timer interrupt can come, in which case the thing will not be like this, right? What happens if a timer interrupt comes in the kernel mode? You don't need to push, you don't need to switch the stack, and you don't need to push these extra values, ss and esp, right? So if you were executing, let's say, let's say this was a kstack, and let's say this was the esp while you were executing in kernel mode, and a timer interrupt comes. Let's say there's a timer interrupt, right? So what will happen is esp will just get decremented, and cs, eip, and eflags of the kernel at that time will get saved, and the handler will get called, just like before, right? So it's possible that a kernel execution gets interrupted, in which case the kernel's eip gets saved, right? And all the other registers will also get saved, just like before. So kernels, eax, kernels, so this time you're going to save kernels, eax, and all the other registers on the stack, right? So you're going to save all these things, and then you may want to call the schedule function, just like before, right? So the only difference between the previous case was that you started at the top, and all the values that you were saving were user's values. In this case, you don't start at the top, you start at wherever you are currently. And whatever values you're saving now are actually kernel's values, but everything else remains the same. You're still going to call the scheduler. The scheduler may again attempt to do a context switch, but that's okay, because what'll happen is a context switch will happen, and this kernel stack will actually have two trap frames in it, right? So if a timer, let's say timer interrupt received in kernel mode, and let's say you context switched, then what'll happen is, this was P1's case stack, you will have user trap frame. After all, you know, if it was executing in kernel mode, it must have come from the user at some point, so the user's trap frame must be there. Some function called chain, and then it got trapped again, and so this will be the kernel trap frame, right? And at this point, let's say it decides to context switch, so from here, it just context switches to another process, P2's case stack, just like before. If ever it comes back, it's going to unwind the stack. Once again, the kernel's trap frame is going to have a trap rate here, so it's going to return to the trap rate function, it's going to return from the kernel trap, and resume execution at exactly the same point where it was interrupted in the kernel, right? And now it will execute again in the kernel, even after returning from the trap, and then it will again do trap rate, and then go back to the user. It's possible for the kernel stack to have two trap frames, one for the user, and one while it was executing the kernel, it got interrupted again, that's it. Okay, let's stop and discuss more next time."}