{"text":"Okay, let's start. So welcome to Operating Systems Lecture 33. Last time we were looking at file systems and how a file system could be organized. We looked at a few different types of files and we said let's look at the xv6 file system in more detail. Recall that the xv6 file system is an index file system in which each file is represented by an inode and an inode contains pointers to the data blocks, right? And we said that, you know, this has problems of the maximum, this limits the size of the maximum file. So one simple way to deal with it is to have a two-level hierarchy. But given that most files are small, having a two-level hierarchy for all files is not a good idea. So the common optimization is that the first few data pointers or the first few block pointers are direct pointers. So it's a one-level hierarchy for the first few bytes of the file. And after a certain size of the file, you have a two-level hierarchy. And still further, you have a three-level hierarchy. So that allows you a very large file overall, yet having a very small access latency for small files, which is a common case. Even for large files, the average access latency is small because the cost of index lookup gets amortized over a large amount of data, hopefully. All right. So we were looking at the xv6 file system. And the xv6 file system, let's say this were the disk sectors or disk blocks. The 0th sector is the boot sector that's basically completely independent of the file system. The file system doesn't touch it. The sector number one is the super block that contains meta-level information about what this file system is. For example, for a fat file system, it may contain things like what's the block size. So this is sort of written at format time. And then from sector two to some value, which is hard-coded, you have what's inodes, right? So the idea is that you're only going to allocate inodes from this area. So that immediately puts a limit on the maximum number of files you can have in the file system. And then there's a region which is the free block bitmap. You basically store one bit per block to indicate whether the block is used or free. And then you actually have the blocks themselves. Actually, I shouldn't say free blocks. These are basically all the blocks, free or used blocks. This is the place where all the data is stored. This is the place where you basically store whether the block is valid or not free. And this is the place where you store inodes for the file. Multiple organizations you could do here. You could have, for example, interspersed inodes and blocks. That would have had the advantage that you could have had probably better spatial locality between the file index and the file data blocks. On the other hand, if you want to go through a lot of files, for example, you want to do a scan of the entire data directory structure, and all you care about is looking at the inode of each file. For example, you just want to count the number of files, let's say. Then this is a much better structure because all the inodes are located in this region. So you can actually do one sequential read, assuming your memory is large enough, to read all the inodes and then process them together. So given that we are implementing such file systems on a magnetic disk, the most important optimization in general is to reduce the number of random disk accesses, be it reads or writes. Because a random access involves a seq and a rotation, a sequential access is relatively much cheaper. And this constant IPB basically says how many inodes can you store per block. So inodes per block. That depends on the size of inode you choose. If I recall correctly, the size of an inode on x86 was 64 bytes, and the size of a block is 512 bytes. We are also looking at what happens if I want to create a file. So let's look at some operations and let's look at what are the writes in the disk that need to happen for each operation. So if I want to create a file, let's say I create a file using this command echo greater than a. That just creates an empty file called a in the current working directory of the process. So that current working directory is also the parent directory of this file called a. So how am I going to do it? I'm going to first allocate an inode. I'm going to traverse the list of inodes and find one which is not used. Then you create the size, status, and adders of the inode. So just allocate the file and say that it's basically size zero, its status is allocated, read, write, who owns it, for example, when it was created, and so on, whatever else you want to write about it. And then you create a directory entry record in the parent directory. So you have created a new inode, but you also want to link it to the parent directory. In this case, the parent directory is the current working directory. Linking it to the parent directory basically means adding a directory entry record in the parent directory's file. Recall that a directory is nothing but a file with the contents of the file being directory entries that indicate the files that are present in that directory. Each directory entry is a mapping from a name to an inode number. And for simplicity, x86 allows only a 14 character name at most, so that ensures that a directory entry is a fixed-size directory entry, but of course, you could make it more complex to allow better, longer names, for example. So you add a directory entry record, I'm going to call it direct record, to the parent directory's data blocks. So this block is going to be added to the directory entries or directory's data blocks, and then you're going to update the parent directory's inode. So you have appended a block to the parent directory's file, and so you want to also update the parent directory's inode to indicate that the size has changed. So four or five or some number of writes, random writes, to the disk to be able to create a file. Let's see what happens on a file append. Well, in some sense, file creation was similar to saying, I want to append to a directory. So writing a file is nothing but saying, append to a directory. But let's see what happens on a file append. Here, instead of allocating an inode, I allocate a block first, which basically means write to the bitmap. Blocks are implemented differently from inodes. You have a separate bitmap, and you have the blocks themselves, as opposed to the inode was having a status field inside it, whether it was free or not. Then you zero out the block contents, you write x to the block, so all this is basically being written in the block area. This write was to the bitmap area, and these two writes are to the block area. And then finally, there's a write to the inode area, that's basically, you update the A's inode. To update the A's inode, basically, why do you need to update it? You need to update the size. There's one character in it, so you update the size from zero to one. And then you also want to update the pointers. Recall that's an index file system, you are going to update the pointer inside the inode to point to the first data block. Once again, all these are random writes, they are completely in different parts of the disk, and you're going to have to do four or five random writes to be able to implement file append in a file system like this. Let's look at file unlink. I want to remove a file from the current working directory, which should be the parent directory of A, and let's assume that A exists in this current working directory. One way to do it is, you first look at the current working directory, and find a file called A. If you don't find something, then you can return an error, assuming that there exists a file called A. So you find a file called A, and you zero it out, which basically means that it doesn't exist anymore. So basically, x86 is implementing directories as a file that contains a list of direct records, but the direct records could themselves have zero to indicate that it doesn't exist. So write zero to A's direct record in parent directory data block. So this is a write to the data block. Then you update the parent directory's inode. You may want, in this case, if it was the last one, you may want to update the size of the parent directory's inode. So first you wrote to the data block region, then you wrote to the inode region. And then you update A's inode to market-free, so you are deleting A, you have unlinked it from the parent, but you also want to market-free so that it's usable by other files or other operations in future. And then you update the free bitmap block for A's data block. Also mark the data blocks to be free. So there are, once again, a few different writes that are happening to the disk. So we get some sense of how costly these operations are to do read and write, create, read, write, et cetera. Let's first talk about how do you deal with concurrency. So let's say there are two threads or two processes who simultaneously execute, want to append to a file, or they simultaneously want to create two files with different names in the same directory. So one needs to have some kind of concurrency management. One very simple concurrency management is have a global lock over the entire file system. Does this global lock need... So you can have one global lock, let's call this file system lock, and only one thread can take this lock at any time by the definition of lock. And then each thread that completes this operation then gets out. But that's obviously not a very nice design. Anywhere the file system operations are so slow. And if only one thread can be inside the file system, then if there are multiple threads accessing completely independent parts of the file system, you have greatly serialized and made the execution and made execution really slow, right? We call that the most important... One of the most important optimizations is to allow the disk controller to have lots of IOs in flight. If you have one single serializing lock for the entire file system, you can only have a few, maybe even just one IO in flight at any time. So having one global lock is not a great idea. But let's first talk about if I had a global lock, where will I store this global lock? Is it enough to store it in memory or do I need to store this global lock on disk? Do I need to implement locks in memory or do I need to implement locks on disk? What does it mean to implement locks on disk? Implementing locks on disk means, you know, same thing, you assume that something on the disk is atomic. So recall that we said that there's some instruction that can atomically set locations in memory, bytes in memory, right? Similarly, we need some assumption on the disk. On the disk, let's assume that right of a sector on the disk is atomic. So if, you know, either the sector gets written or it doesn't get written, it's not like half of the sector gets written by one thread and half gets written by another thread. That's not going to happen. Also let's assume that if the power goes down in the middle of writing the sector, then the sector write will get completed, right? So the last sector will either get written or not get written at all, right? That's atomic. So back to my question, do I need the file system lock to be implemented as a disk variable, as an on disk variable, or is it OK to just have an in-memory variable to implement mutual exclusion? All I need to implement is mutual exclusion, right? So can I implement mutual exclusion by in-memory variable or on disk variable? Answer. I want synchronization to proceed even after power reboot. Well, I mean, so right now I'm just saying that I want mutual exclusion across multiple threads, and threads just die after a power reboot, right? So threads are basically just in-memory constructs. So they don't carry over across power reboot. So in-memory suffices if the only, you know, so if you are basically assuming that your disk is only accessed by this operating system that you're using, the disk is not shared between multiple machines, you know, in-memory locks are enough, right? Assuming that all accesses to the disk are going through this OS layer, the OS is using in-memory locks to synchronize accesses to the disk. So in-memory locks are enough, all right? Okay. So that's fine. Now let's look at, but now we are not happy with a global lock. So we need some kind of per-file locks, per-inode locks, per-block, per-data block locks or something of that sort, right? So how can this be done? Well, you will want to have one lock for every data block on the disk somehow, all right? So one way to think about it is that, let's say this is my OS, this is my disk, all accesses to the disk go through the OS, and the OS inside it maintains what's called a buffer cache. We've seen this before. So a thread cannot read or write the disk directly, the thread can make a request for a block to get read into the buffer cache, and then you act synchronized over the buffer cache. So you can only read or write blocks that are present in the buffer cache. So if you want to read something, you'll first bring it into the buffer cache and then read data from it. If you want to write something, you'll first bring data from it, first bring it into the buffer cache, then write to it. And you may want to implement write-through or write-back cache, that's a different matter. But in any case, you've extracted the disk blocks as a buffer cache, and now you need to synchronize on the buffer cache, instead of synchronizing on the data disk blocks themselves. Because nobody can actually touch the data blocks directly, you can only touch the data block by bringing the data into a buffer inside the buffer cache, and then you synchronize over the buffer cache, right? So basically you have a buffer cache, which is some set of buffers, let's say it's implemented as a list, and each buffer of buffers, and each buffer has a lock, lock per buffer. So if I want to, let's say, create a file into a directory, I need to make four operations. What I'll probably want to do is I'll first lock, let's say, let's look at this, look at this. I need to do four operations, these are four different sectors, four different blocks, so I'll probably want to lock all of them, and then do this operation. So basically we are moving from coarse-grained locking to fine-grained locking, there's an operation that I want to perform, this operation needs to be atomic with respect to all the other threads that may be doing this operation. So recall as a way to do it is basically, in fine-grained locking, identify all the data, all the data items or datums that need to be touched, and get all those locks, and then do your operation. That's what I'm going to do, get these locks, so get a lock for the sector or for the buffer that stores the inode, get a lock for the buffer that stores the data block of the directory that contains the directory entry, and so on, and then release those locks. That's fine-grained locking. How do I implement these locks? Should these be spin locks? I think it's a good idea to have them as spin locks. They should probably be blocking locks, right? I mean, if you have spin locks, then if I'm waiting for some buffer to get free, or some buffer to get released, then I have to just, I want to sleep, because these, my critical sections are likely to be very large. They involve disk accesses milliseconds of time, right? So you want blocking locks, right? So one way to implement blocking locks is to just use a bit called busy bit in the buffer structure to indicate whether this bit, this buffer is locked or not, right? So I'm going to use a buffer cache lock, a global buffer cache lock, to synchronize accesses to this busy bit per buffer, right? So the idea is that I'm going to take one global buffer cache lock, and then to do mutual exclusion over these busy bits. And then once I have locked, so these busy bits are going to act as per-buffer locks, and then once I have taken the lock, then I'm going to release the global buffer cache lock, right? So the global buffer cache lock is a spin lock that allows you to implement these blocking locks via these busy bits. So a typical operation would be that, let's say, I want to access a block. I will first acquire global buffer cache lock, search if block in cache. This is likely to be a fast operation. All you need to do is memory accesses to figure out whether the block that you're looking for is already in cache or not, right? So what are the few options? Let's say found and not busy, right? The best case, I found it in the cache, and it's not currently busy, mark it busy. Release buffer cache lock, that's a global buffer cache lock, and return the buffer. Notice I'm using the buffer cache lock to provide mutual exclusion over my index structure and to provide mutual exclusion to accesses to the busy bit. Now what's the other case? Let's say I found, let me write it on a different sheet, let's say, found and busy. I acquired the buffer cache lock, I went through the index, and I found it to be busy. Recall that these busy bits are being mutually exclusive, protected against concurrent accesses by the buffer cache lock. So you'll either find it busy or you'll not find it busy. It's not like the concurrent access is happening to the busy bit. If it's found and busy, then what do you need to do? You want to sleep, right? On x-ray spec, you want to sleep, that's block, or sleep. And you also want to release the buffer cache lock while you're sleeping, right? So sleep and release the buffer cache lock. That's the second argument of the sleep. What's the first argument of sleep? You just need to provide some channel on which you're going to sleep. So let's say the sleep is, let's say, buffer address, or address. Some channel, which makes sense. In this case, you're basically saying, I want to wait for this block to get freed, so let's just wait on the channel which is the address of this block, the disk lock address of this block. And so at some point, so while you sleep, you also release the buffer cache lock. So now other threads can be doing their operations on it, and at some point, somebody is going to mark this particular buffer as not busy. And it's that thread's responsibility to call wakeup on that particular address, and so I'm going to come out of sleep. So after coming out, I can be sure that I've reacquired the buffer cache lock. And what should I do this time? Again go to the step one and check if the buffer still exists, right? The moment I release the buffer cache lock, I cannot assume anything about the state of the buffer cache on reacquisition of the lock. It's possible that after I release the buffer cache lock, somebody came in and not only marked it not busy, but also evicted it from the cache, right? So after coming out, re-scan buffer cache, re-search, and do the same thing, okay? And let's see what's the fifth case, or what's the third case? Not found. In which case, you will just read, you'll find a replacement buffer. Once again, you're going to find the replacement with the buffer cache lock held, so you can be sure that there's no, so that's guaranteeing mutual exclusion. So recall that all the in-memory data structures are being protected by this global buffer cache lock. It's the individual buffers that are getting protected by fine-grained locks. So you find a replacement holding the buffer cache lock, and what, so firstly, and make sure that the replacement should be not busy. Replacement must be not busy. If it's busy, somebody's actually using that block, so I cannot just replace it, right? So the busy is also making sure that while you are doing something, while you are actually accessing a buffer, when some other thread is accessing the buffer, nobody else can actually evict it, right? So that's not busy. Once you found the replacement, you mark it busy, mark it busy because now you have decided that you're going to operate on this particular block, and what you're going to do, so once you've marked it busy, at this point, you can release the buffer cache lock. So basically, you have to use the global lock to get the fine-grained lock, and then once you've got the fine-grained lock, which basically means you marked it busy, you release the global lock, right? Now you have the busy bit set, so nobody else can touch it until you make it not busy, right? Now while you're holding the busy bit on that block, you can write it to disk, assuming it's dirty. So if dirty, read from disk the new block that you wanted to read, you know, set its metadata, sample block number, and the contents. I'm using, so here's a very good example where fine-grained locking is absolutely necessary to have any kind of performance, and you're using a global lock to get these fine-grained locks. You don't want these fine-grained locks to be spin locks. You want these fine-grained locks to be blocking locks, and so to do that, you need a global spin lock to synchronize accesses to these blocking locks, and these fine-grained locks are basically making sure that while you are working on something, it doesn't get evicted, nobody else touches it while you're working on it. Right, so but what's the problem with fine-grained locks? Deadlocks, yeah, ordering, yeah, so deadlocks. So any time you have fine-grained locks, there should be an alarm bell that, you know, I have to worry about deadlocks. So what are some bad things that can happen? Let's just take this example. Let's say somebody is creating a file. He allocates an inode, so he takes a lock on the block that contains the inode, right, and then he adds a directory entry record, then he tries to take a lock on the directory entry record. So first he takes a lock on the inode, then he takes a lock on the directory entry lock. If there's some other thread, let's say unlink, that first takes a lock on the directory entry record, and then takes a lock on the inode, then there's a possible deadlock, right? Well, I mean, if you want this entire creation to be atomic, you would want to hold both locks at the same time, right? Otherwise what will happen is if you release a lock at this point, so you allocate an inode and then you release a lock, in the middle some other thread comes in, so it's going to see some inconsistency in your data structure, in your file structure. So if you want that, you know, this entire creation, it's possible that the entire creation need not be atomic to ensure that inconsistencies don't, people, no thread can see inconsistent data structure. So there are some invariants that have to be maintained, right? So in general, when you're talking about concurrent access and synchronization, you basically first want to think about what are the invariants that need to be maintained. And the invariant typically looks like this. If I have been able to acquire the lock, then I can assume that the data structure looks like this, right? For example, if I've been able to acquire a lock on the tree, then the tree must be a tree. It should not have any cycles, right? Or there should not be any dangling pointers and things like that, right? So if you release the lock somewhere here, then you are releasing the lock at an inconsistent state. So the idea should be that you'll take a lock and then you perform some operations and only release the lock when you leave the system in a consistent state. All right? So you take two locks here, let's say, to make sure that, and so there can be deadlocks, right? So you need some kind of global orderings, right? So some orderings that x86 follows, and this is something that I was just looking through, is that, you know, if there's some operation that needs to touch the superblock, then mark superblock first. Right? So let me just do some kind of ordering, so let the superblock before anything else, before everything else. So this is, the less than sign basically says, if you want this operation to do something on the superblock sector, then it should be taken first. Right? Okay? Then inode before data. If you want to do something on the inode block and the corresponding data block, you first take a block, lock on the inode, and then take a lock on the data. And the programmer needs to be careful about doing this, right? Let me look at some examples. For example, here, file unlink is an example where you first write to the data block, and then you write to the inode. But locking should be in the opposite order, otherwise, you know, you have deadlocks. So programmer needs to be careful, all right? Then parents, parent before child, right? So not only do you have a hierarchy in the file system metadata, the inode and the data blocks, you also have a hierarchy in the directory structure, right? So there are parent inodes, and then there are child inodes, so parent directory inodes and the child directory inodes. Parent directory is blocks, child directory is blocks. And so on. Right? So there needs to be some order on that as well. And let's say everything, so bitmap blocks, nothing after bitmap blocks. Everything else before bitmap blocks. You shouldn't hold a bitmap block and then try to hold a lock on something else. So just a very good example of, you know, of a relatively complex system where you have find and lock, and then you need to have some global order, and there are lots of different types of entities, and you need to have some global order on these different types of entities. So this is something, I mean, these are some invariants that xv6 code follows. You can check it for yourself, you know, if there are more or there are less, let's find out. Let's know. Okay. All right. Finally, let's look at the cache replacement policy. So recall that unlike the virtual memory subsystem, where you really cared about the speed of the hit path, right? The hit path was a memory access. And you didn't want anything, any overhead on the hit path. The maximum you tolerated was setting up of a, setting a bit, the access bit. And that too was done by the hardware. In case of file system, you have more flexibility. Assuming that the file system is being accessed by system calls like read and write, there's anyways a lot of overhead of taking a trap, making function calls, and potentially even making a disk access. So the whole path of the hit path itself is quite expensive. So you can actually do much more than just one bit for the access bit, right? So in fact, the xv6 file system implements LRU for the buffer cache. And this is the way it works. Each time you release a buffer, so maintain, how is it implemented? Maintain buffer cache as a list, as a doubly linked list, right? And each time you, instead of access, I'm going to say release a buffer. So at the time of access, you know, when you first access the buffer, you mark it busy. As long as it's busy, it cannot be replaced. So you don't need to do anything at the time of access. At the time of release, that's the time you can basically say that I want to do something about this buffer to make sure that it gets released, it gets evicted the last. Because I just accessed it, so it's most recent access. So it should be released the last. So each time you release a buffer, move it to front of list, of this doubly linked list, right? And start, any time you want to replace, you start replacement from back of list. That's LRU. Each time you access something, you just move it to the front of list. You want to start replacement, you start from the back of list. Recall that when you start replacement, you look at the last element, and you get the first one starting from the last. That's not busy. That's the one you're going to release. Anything that's busy is currently getting access. So you know, that's not a good candidate for replacement anyways. So that's LRU cache replacement for the buffer cache in xv6. Right? It has a write-through cache. Write-through cache is there for simplicity reasons, but let's look at, is this a great policy? It's actually a very poor policy from a performance standpoint. Each time you write something to a block, it goes all the way to the disk. Even if you overwrite something a hundred times, there are a hundred writes to the disk. If it was a write-back policy, then you could have absorbed 99 of those writes in memory and just given one write to the disk. So we all know that write-back has this nice thing that it absorbs multiple writes to the same data block. More importantly, given the physical characteristics of a magnetic disk, write-back cache is a much better option than write-through cache. Write-through cache basically means that the disk write is on the hit path or on the critical path. You can only write, you basically have only one outstanding I or a very small number of outstanding IOs at a time. With the write-back cache, you can batch a large number of writes together and that way you can get much more efficient utilization of the disk bandwidth, all that elevator scheduler inside the disk controller. So if you give lots of things together, that's a much better option. So a write-back cache is even more relevant in the case of a magnetic disk because batching writes is a very good optimization. Secondly, it has no prefetching. Let's see what it means. Let's just say this is that. A real file system would probably not want to write-through cache, it will want to write-back cache and we'll look at some issues there. Secondly, the x86 has no prefetching. What does it mean? Quite likely, many workloads have a very sequential behavior, sequential write. So let's say I execute a grep command. Grep command is going to go through the file in sequential order from byte 0 to byte last. And so there's a very sequential thing. Let's say I implement, I execute grep on top of the x86 file system, what's going to happen is it's going to ask for block 0, block 0 is going to get read, and by the time I actually start processing the block 0, the disk is already rotating, so assuming that the disk was contiguously laid out on disk, or the file was contiguously laid out on disk, you're basically wasting rotations. If the disk, if the file system could instead say I want block 0 through 10, then all those 0 through 10 could have been read in one go. But now if you're going to say 0, then 1, then 2, then each time you read a block, you waste a full rotation before you get to 1. Then you waste a full rotation, you get to 2, and so on. It's much more, it's milliseconds more expensive than what could have been done if you had prefetching. So it's, so that's a, that's a thing. And let's see, there are actually double copies. Let's, there's another fact about, about, about the file system that is, as we have seen it so far. There's double copying. What do I mean? If an application says I want to read sector X, then first the sector X gets copied from the disk device to the buffer cache. The buffer cache is an in-kernel data structure. The user cannot see it directly. And then from the buffer cache to the user's address space, right? Recall that a user will probably want to say something like read FD buff size, right? And this buff will be a user pointer. To implement this, basically the file system has to first read from disk to the buffer cache, and then from buffer cache to the user address space. Is that a big problem? Well, assuming for something like a magnetic disk, it's not a problem because the disk access itself is the bigger bottleneck, right? Memory copying is not that big a bottleneck. Let's say you are actually running on a very fast network device, right? And these kind of double copying costs actually do become bottlenecks. So, you know, think about 10 gigabit networking. So if, if the file descriptor was not pointing to a file on disk, it was pointing to some network device and you wanted to read data from the network device at a very fast speed, then you're first copying it to the buffer cache and then copying it to the, to the user space. Could you have done better? Well, one way to do this is let's say you can say that when the user says read FD buff, I take this pointer and I pass it all the way to the disk driver, right? Recall that the pointer that I give to the disk driver is that of the buffer cache. And that's having this problem of double copying. The other option could have been, let's take the pointer from the user, converted it to its physical address, and then pass it all the way to the disk driver. And so the disk driver directly writes to the user mapped memory. And then I just return. And so there's just one copy. The disk driver directly writes to the user memory. What are some things I need to be careful about? Recall that the virtual memory subsystem does swapping, right? So it can actually swap out data, swap out address space regions from the user space. So I need to make sure that this region pointed to by buff, fill size, cannot be swapped out. So I need to lock it in memory or, you know, the term used is basically pin it in memory. You pin them in memory. You basically don't allow it to go through disk. You don't allow that to get to disk, number one. But more importantly, you disallow... So this read, if there was locality of accesses by multiple processes, then you're also getting rid of... You're basically also hampering locality. So if there's one process, temporal locality is not getting properly added because buffer cache has this nice property that it's a shared cache between multiple processes. Now if you bypass the buffer cache and directly start reading to user address spaces, if some other process also wants to read the same data, then he'll have to make another disk access. So you cannot use the cache. So in some situations, you want to avoid double copy. And these situations are situations where you care about very high IO bandwidth. Think about network switches, network routers, et cetera. And you don't expect that much temporal locality of access across multiple processes. On the other hand, there are other cases where double copying may be a reasonable overhead to take, given its advantages. Finally, how big could the buffer cache be? What's the tradeoff? If I give the entire memory to the buffer cache, what happens? I'll have no memory for my virtual memory subsystem. So recall that the memory is being used for two purposes, as you've discussed so far. One was to act as a cache for the virtual address space, and another as a cache for the file system. There are two caches in the memory. One as a cache to the virtual memory address space, and another as a cache to the file system. And these are completely disjoint. One is caching swap space data, and memory map file data, et cetera. And another is caching file data. And both are completely different semantics, also. And also different access patterns and replacement policies. Performance tradeoffs, more importantly. You basically, the OS needs to decide how big should the buffer cache be, and if it makes it really big, then there's very little. So it's contending with page cache from the VM subsystem. So this decision can be made statically. You can say, I'll have half of my memory for buffer cache, and the rest half for the page cache. Or you could do some kind of dynamic thing based on user's patterns. So basically, you want to see if there's a lot of file system operations happening, then you want to make the buffer cache larger. On the other hand, if there are not that many page faults happening, so you want to make buffer cache larger. On the other hand, if there are lots of page faults happening, not that many buffer cache misses happening, then you want to make your VM cache larger. And you could do some coarse-grained dynamic tuning to figure this out. But of course, xv6 just uses a static file. Now let's look at durability. So one of the most important and interesting aspects of file system design is that state should not become inconsistent across power failures. So let's see what can happen. Let's say I wanted to create a file. And let's say I wanted to create a file, I needed to make at least four disk writes. These are the four disk writes. And at this point, after I made the first disk write, power caches, the power cache. Recall that all your busy bits, et cetera, were only in-memory data structures, and they were only there to protect against concurrent accesses by other threads. But now if there's a power failure, then you have left the file system in an inconsistent state, irrespective whether you use lots or not, whether you use global lots or not, it doesn't matter. The file system is still in an inconsistent state. So then it comes back again. You see that an inode has been allocated, but it's not pointing to anything. Right? First, okay, so let's see what are some bad things that can happen. Let's say I'm creating a file. Basically, you know, at the core level, there are two things I need to do. I need to allocate inode, and I need to create directory entry, right, which points to inode. Well, let's say a power failure happens, and I have created a directory entry pointing to inode, and I have, so I have, let's say this has been flushed to disk, and this has not been flushed to disk. So I did both these operations, but I did this, let's say, first from a disk standpoint. So I first created the directory entry, and now I was going to create, allocate an inode. But before I could allocate an inode, the power went off. Power comes back again. What do you have? You have a directory entry pointing to some dangling inode, dangling pointer. Right? This is a bad thing. Why is it bad? For example, let's say the inode that it's pointing to belongs to some other user. So now I can potentially read the contents of the other user's file, so it's a security flaw from that standpoint. Or it contains some garbage data, which makes me feel that the file is actually really large. Right? So basically, I have a dangling pointer, and the dangling pointer is pointing to some garbage. And this garbage could be security-sensitive data. It could be something that can very badly confuse me. Let's see if the other thing happens. Let's say this gets flushed, and this doesn't get flushed. So firstly, you know, yeah, so either of these things can happen. So let's say I've allocated an inode, all right, but I have not yet created an entry for it in the directory, and the power goes down. Power comes back again. What's the inconsistency in my system? The inconsistency is that I have some node that's not free, that's allocated, yet I have nobody pointing to it. Right? So that's not as bad as a dangling pointer. Right? In the first case, I had a dangling pointer. This we said was very bad. And in the other case, we have a leak. Why do I call it a leak? Basically, there's some data that's not usable anyway, or some blocks, so the inode block is actually not allocated, but it's not being actually used in the PHY system, and so it's a leak. It's a space leak. Right? I'll not be able to use it for anything else any longer. This is acceptable. So you have to, so one of these two things have to happen. So to make this acceptable, one way to do this is to order the writes. To say that whenever you create a file, I'm going to first allocate an inode, flash it to disk, and then create a directory entry. If I follow, obey this ordering, then I'm sure that there'll be never any, you know, this is disallowed, and this is possible, but this is acceptable. Only acceptable things are possible. To order the writes to disk, assuming, let's assume a write through cache for the timing, so you would basically say that I want to create something. I'm going to first, so in general, if there's a data structure that has something like this, then at creation time, let's say this is, you know, all these are parent-child relationships, so this is, let's say, child, this is parent, and this is grandparent, so there's some kind of parent-child relationship between them. In case of a directory and subdirectory, it's a parent-child relationship between parent subdirectory and directory, directory and subdirectory. In the case of a file, it's a, there's a parent-child relationship between the inode and the data block, inode being the parent, data block being the child, right? In the case of doubly indirect blocks, there's a three-level hierarchy, the top-level inode, the indirect block, and the data block, and so on, right? So there's some parent-child hierarchy, and at the creation, at the creation time, you're going to move in this direction, right? First child, then parent. That's going to ensure that you can only have leaks, you'll never have dangling pointers. At the time of unlink, or remove, what? The opposite direction, first parent, then child. If I want to remove something, let's say I wanted to remove this, so I'm going to first update the parent to say that, you know, there's no pointer here, so I'm going to first remove this pointer, and at this point, if there's a cache, then I have a leak, that child's data blocks are not accessible anymore, but there's no dangling pointer, right? On the other hand, if I had first unallocated the child, and there is a power cache after that, then the parent would have had a dangling pointer, okay? All right, let's continue this discussion on durability further in the next lecture."}