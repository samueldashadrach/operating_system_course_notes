{"text":"Welcome to Operating Systems lecture 37 and today we are going to talk about scheduling. So one of the central themes of an operating system is scheduling, we have seen that an operating system runs many processes, handles many devices and maps processes to devices and one of the central things that an operating system is really doing is scheduling. So let us first understand what are the objectives of a scheduler. So let us say scheduling objective, so let us say throughput, maximization, by this I mean that I want to do the maximum possible work in the minimum possible time. So there is a lot of work to be done and I want to make sure that I finish as many jobs as possible in the minimum amount of time and so I have these many resources and I want to judiciously use all these resources to do everything I can, right. The other thing is minimize response time, this means that if something needs to respond to something, if there is an input output operation, then that response should be as fast as possible. For example, if the user presses a key, then he should see the key character getting printed on the screen as soon as possible, right. There should be some metric which says this is the response time, the response time is the time it took from between the pressing of the key and the display of the character on the monitor or you know the arrival of a packet, the arrival of a request over the network and the response or the reply over the network, right. So the user pressing a key and the character appearing on a monitor is a very easy constraint really because humans' perception time is very large, you know, so humans cannot perceive more than, less than 1 millisecond granularity and computers are already as fast as nanoseconds, so that's not that big a deal. But if I talk about network response time, that becomes more of an issue, right. And it's not just network, other IO devices also, right, okay. And you're going to see that how, you know, these two requirements can conflict with each other, so if you have maximum response, if you need maximum response time, it does not, you know, it may actually come at the expense of reduced throughput. And then you want some level of fairness. When doing all this, you make, want to make sure that if there are many, many jobs that are running, there's some amount of fairness in the system which, it shouldn't be that if a job is giving you a lot of throughput, you just keep running that particular job and not run any other job, so there should be some level of fairness. In particular, there shouldn't be any starvation, right, so there should be no starvation. By starvation, I mean there's some job that doesn't get to run at all or there's no guarantee on how frequently it gets to run, right, so it can keep waiting for an unbounded amount of time, that will be called starvation, and this, you know, a scheduler should actually never allow something like starvation, right, so those are some of our objectives, and let's look at some types of scheduling policies, right. So simplest policy is first in, first out, or first come, first served, right, that's your typical scheduling policy, you queue up the job in arrival order, and you keep doing the jobs in the order that they arrived to you, arrived. This is okay, but, you know, the process length can be actually very large, you know, some process lengths can be very small, other process lengths can be very large, for example, if you run a compute bound process and the resource that you're scheduling is a CPU, then the process, compute bound process could probably take, you know, minutes, and you don't, you cannot afford to make other jobs wait for minutes, right, so there's an obvious problem with that. Well, in the same vein, there's something called shortest time to completion first, or shortest job first, let's say. Here again, instead of serving the jobs in the arrival order, you basically say here are all my jobs that have come, let's say I wait for some time, and I collect some jobs, and I say what's the shortest job in this buffer, and I pick that, and I do that, and then I pick the next shortest job, and so on, right. So if I do that, then, you know, arguably, I will minimize the amount of response time for the, you know, the total average response time per job, if I do that, so this is, you know, provably optimal in terms of response time, average response time per job, but once again, it's too unfair to be actually practical, right, it's firstly, you know, job, even the shortest job could be very long, which means that other people will have to wait, and secondly, if, you know, there's a lot of short jobs that are arriving all the time, then the long job can just keep starving forever, right, so these are all, this is also not really very practical. For something like a CPU, you could do something like Ron Robbins, and define some kind of a time quantum, so recall that we said that there's some amount of, there's a scheduling quantum, and we define the scheduling quantum based on some metrics, one of the scheduling quantum that we sort of looked at was 10 milliseconds, or 100 milliseconds, how was the scheduling quantum decided? Firstly, the cost of doing a context switch should be very small with respect to the scheduling quantum, so that the overhead of scheduling is small, number one, and number two, the scheduling quantum should be small enough, such that, so it cannot be too large, let's say the scheduling quantum was two seconds, then, you know, if I press a key, and there was some job that's running currently, then I'll have to wait on average one second before I actually see the key to be displayed, right, so whatever are my input-output devices, the response times should be proportional, roughly proportional to the scheduling quantum that you choose, right, so if you are, you know, if you're worrying about human users, then human users need, you know, millisecond response times, or 10 millisecond response times, and so scheduling quantum should have some proportionality to the amount of response times that you are looking for, or some relation with the amount of response times you are looking for. As an interesting piece of fact, you know, even in the 1970s, the scheduling quantum used to be 10 milliseconds, and even in, you know, today, 2014, the scheduling quantums are still 10 milliseconds, even though the computers have become really faster, right, and at that time, the computers were 60 megahertz, today the computers are 2 gigahertz, so, you know, there's a 100x speed-up, roughly speaking, and so what's, so, but the scheduling quantum haven't changed, what's the reason? Humans' response times haven't changed, right, so, okay, alright, alright, so that's round-robin, but round-robin completely ignores the type of the job, so you basically give all jobs completely equal, you know, it's completely fair, and that may not be the best thing. For example, let's say there was an I-O job, I-O process, and there was a compute process, and let's say there were 10 compute processes, right, so now you're doing round-robin between these compute processes and this one I-O process, and what you care about is not just your throughput, but also your response time, right? Compute processes mean there's some, you know, long-running computation that's going on, so there's no I-O in the compute process, and so there's nothing like, there's no notion of response time in the compute-bound process, but for the I-O bound process, there's something called the response time, and so what you would want is that whenever the I-O bound process is ready, you would want to run it first, irrespective of how many compute-bound processes are running at that point, right, because if I press a key, let's say the I-O bound process is your editor, and it's, you know, it's currently waiting for a keyboard key press, and I press the key, now if I just do pure round-robin scheduling, then this I-O bound process will have, may have to wait for 10 compute-bound processes to finish, or 10 scheduling quantums to elapse before I get to run, right, so this is also called the convoy effect, where one process, interactive process, queues up behind non-interactive processes, or compute processes, this isn't computes, but compute processes, so that's not a good thing, and we'll see how one can solve that, so if I do a pure round-robin scheduling, I can actually have a very poor response time, so let's say I have a system that's running lots of compute-bound jobs, let's say it's running simulations at the background, that shouldn't mean that if I press a key, or if I log in into the system, you know, the time it takes for the next prompt to show is really high, right, even if there are lots of jobs running at the background, my system should appear interactive, and the cost I pay for making the system interactive is really small, right, so it should, it should both be doing the compute-bound jobs, and yet give me a very high level of interactivity, and that's what I would like to have, okay, also round-robin is just too fair, you would probably want some level of proportional CPU, depending on the job, right, so for example, some processes are more, you know, are more important than other processes, system-level processes are more important than user-level processes, let's say there's a process that's going to handle the disk, it's going to flush the disk, or it's responsible for doing something on which lots of different user-level processes rely, and you may want to say that the system-level process should get more CPU than the user-level process on average, right, so you could say something like, you know, let's say process P1 is to process P2 is, you know, the proportion should be X is to 1, and then you can just modify this round-robin to basically do this X is to 1 proportional scheduling, how will you do that, well, one way to do that is what's called stride scheduling, where you schedule P1 X times before scheduling P2, right, so that would, that's just a straightforward extension to round-robin, you just add some proportionality to it, and you basically say I'm going to schedule this X times before I actually schedule, the other way to do that is what's called lottery scheduling, and lottery scheduling basically is a probabilistic algorithm, where you say that, you know, you have a pool of tickets, lottery tickets, and each ticket has a label, which is let's say P1, P2, P1, and so on, and on each quantum, the OS draws one ticket at random from this pool, and depending on whose ticket it is, that's the CPU that gets to run, the randomized algorithm, which is randomized version of stride scheduling, where you basically just pick up one random ticket from the pool, and just say, just say that whoever comes is basically the one that gets to run, so if I want to implement proportional share scheduling, all I need to do is have X tickets for P1, and for every one ticket of P2, and so on. So I'm not solving the convoy effect right now, I'm just talking about, you know, proportional share scheduling, and we're going to talk about convoy effect very soon. Okay, the nice thing about these scheduling algorithms are they are tolerant to dynamic admission of processes, and dynamic exit of the processes, the processes are coming in and going out, and that's completely okay, right? So if a process comes in, for lottery, for something like lottery scheduling, I will just add that many tickets to my pool, and you know, it's a completely local operation, and now, you know, it becomes proportional in that sense. A process exit, I remove the tickets for that particular process, and similarly stride scheduling and round robin, etc. Okay, but that's not, no, that doesn't solve our convoy effect, and so what we really need is some sort of priority scheduling. The idea is that P1 has higher priority than P2, implies, means that if both P1 and P2 are ready, then P1 will win. So in the proportional share, it was just an average thing, you know, basically, P1 will get x times more CPU than P2, but priority scheduling is a strict priority, basically says if P1 and P2 both are ready, then I'll always give the CPU to P1 before I give the CPU to P2. So that's a strict priority scheduling. So some jobs have a higher priority than other jobs. Some examples of this are, let's say, interrupts or device, if a device needs attention, I want to give it attention as soon as possible. So interrupts, you know, we already know that interrupts are higher priority than other jobs. So let's say there's some job running, but somebody requests an interrupt, the job gets preempted and the interrupt gets raised. That's basically like, that's basically saying that interrupts have higher priority, or the device, if the device needs attention, the device will have higher priority than anything else. Or you could say, you could solve the convoy effect by saying IO job bound has higher priority than compute bound job. So if the IO bound job is actually ready to run on the CPU, which means there's some input that has been received from the keyboard, now it needs to get down on the CPU to actually do something, compute something, and maybe display something on the monitor, then, you know, it should get higher priority. So even if there are 100 compute bound jobs, if there's IO bound job that's running, then it will get higher priority over the compute bound jobs. But you could do something like that. Won't IO jobs be a special case of device interrupts? Well, no, not, not exactly. Because a device interrupt may just mean that the device needs attention. And the device needs attention may mean that I just copy that character from there to my kernel buffer. And now, you know, I know that this character is meant for this particular process. So this process has become ready. And now that process becomes, you know, so now, now the regular scheduling algorithm will pick up that process on the next scheduling quantum. So both are examples of priority scheduling, but, you know, in different contexts. In one case, you are dealing with the device, in the other case, you're dealing with the process that's handling that device. Okay, so that's priority scheduling. You may also want priority scheduling for things like, you know, so some things are much more important than other things, for example, real-time operating systems. So, you know, if there's an operating system in the car, then somebody presses the brake, then that should have the highest priority among all the other things like playing the music player, etc. So that's another thing, example of a priority scheduling. Now, you know, when you do priority scheduling, you basically want strict priority, strict priorities, priority orders between these different jobs. But, you know, there are issues with that. So let's say there were three processes, P1, P2, and P3. And let's say the priority order was, let's say, low priority to high priority. And let's say P2 is medium priority. Now, let's say, let's say P1 acquires a lock or acquires any resource. I'm using the lock as an example of a resource. It requires a lock. And now P2 comes in and P2 gets to run. Then P3 comes in and P3 also tries to acquire the same lock or same resource. So the timeline is, this is my timeline. So P1 acquires resource or lock. P2 runs. P3 runs. So P3 is high priority. So P3 comes in, P3 starts running. And P3 runs and tries to acquire the lock. But now, because P, the lock is already held by P1, P3 will have to block, let's say, it's a blocking lock. And so P3 blocks and tries to acquire a lock and blocks. And so P2 keeps running and keeps running. At this stage, you have a situation where P3 is waiting for P2 to finish. And this violates the priority order. You always wanted that if P3 is ready to run, then P3 should get to run. But because P3, there's a dependency between P3 and a lower priority process, it's possible that P3 actually waits for a process that's lower priority than it. And this thing is called priority inversion. This problem is, you know, so this problem is basically, once again, the problem is that a higher priority process is waiting for a resource that's being held by a lower priority process. And but the lower priority process never gets to run because there's a medium priority process that's already in the system. And so the medium priority process keeps getting to run. And so effectively, what's happening is that the higher priority process is waiting for the medium priority process to finish, whereas that shouldn't have happened. But the ideal thing to do here, so this is what's called priority inversion. So priorities have been inverted. And so the ideal thing here would have been that the system, if the system could figure out that the higher priority process is waiting for a resource that that's currently held by the lower priority process. And if that's the case, then the priority of the lower priority process is bumped up to the priority of the waiting process so that, you know, it gets higher priority than P2. So this problem can be solved by what's called priority donation. It says if P3 waits for a resource held by P1, increase P1's priority to P3's. And so you could solve this priority inversion problem by using priority donation. But that's a way that provided that this lock was something that was visible to the scheduler. So I'm assuming here that the scheduler or the operating system could figure out that P1 acquired a lock and now P3 is waiting for a lock that's acquired by P1. That's only possible if the lock was a blocking lock or if it was a lock that involved OS activity. For example, the lock involved a system call. But if the OS has no idea about this dependency of between P3 and P1, then what's the worst thing that can happen? That P1 holds the lock, P3 waits for P1 to complete, and P3 keeps waiting. You know, let's say the lock was a spin lock, then P3 gets to run and P3 just keeps running forever. And the lower priority thread, which is actually holding the lock, doesn't get to run. So strict priorities can actually also result in deadlocks if the scheduler has no idea about the, or can opaque to the dependencies or resource dependencies between the different processes. Okay. So using this priority idea, what's used in operating systems is what is popularly used in operating systems is what's called a multi-level feedback queue. What's this? You basically maintain multiple priority levels. So this is highest priority, and let's say this is lowest priority. And within each priority level, you maintain the jobs in a round-robin queue. So these are lower priority threads. You run the lower priority thread only if, so you obey, so by priority I mean, you know, the strict priority that we have discussed. So you run a lower priority thread only if there's no higher priority thread available. So you will run this only if all these other processes or jobs are not ready. Okay. And so basically what you would want to do is that all the IO jobs are at the highest priority, and all the compute jobs come at the lowest priority. So basically what the system does is it organizes all these jobs into IO bound jobs and compute bound jobs. It gives highest priority to the IO bound jobs and serves all the jobs within the same priority in round-robin order. And then all the compute bound jobs are given lower priority than the IO bound jobs. And so if there's an IO bound job that's ready, then that will get served before there's a compute bound job that gets run. But then how does the system know what's an IO bound job and what's a compute bound job? Number of? Number of IO syscalls. Okay. Number of IO syscalls. Well, yes, sort of. So basically you just, you know, like as with everything else in the OS, you just use the past to approximate the future. All right. So basically say what has been the behavior of this application previously. And by IO, I basically mean something that actually blocks, you know, for whatever reason, either IO or lock or something. And so what you do is you start a job at the highest priority. And at each priority, you give lesser time quantum. So let's say there's a time quantum Q here. Then you will give 2Q here and let's say 4Q here. You gave it a quantum Q to run. If it exhausts the time quantum, which means it actually doesn't block within the time quantum that you gave it. Let's say you gave it 10 milliseconds to run. And it didn't, at an exhausted the 10 milliseconds without blocking. If it's an IO bound job, it's like it's very likely that it will block before the 10 milliseconds expire. Right. For example, it will go and say, I want to read the next character from the keyboard. So if it's an IO bound job, it will not exhaust the time quantum. If it exhausts the time quantum, it basically means that this job average length is longer than the time quantum that's given to it. And so what you do is if you exhaust, then you move it to the lower priority Q. So you started from the highest priority Q and give it a time quantum Q. If the job expires at time quantum, then you move it to a lower priority and also increase the time quantum. If it doesn't exhaust the time quantum, which means it blocks before it wanted, it could expire at time quantum. You leave it where it was. So this basically ensures, and also you basically, as you move the, so the logic is if exhaust time quantum, then priority minus minus and quantum is multiplied by two. Right. So you also multiply the quantum by two to make sure that, you know, the scheduling overhead is even less. So it basically says that it seems like this job is there to, you know, take large chunks of CPU. And so why to keep interrupting it if it needs a larger chunk of CPU, you not just decrease the priority, but also give it higher amount of CPU. So when it gets scheduled, it will actually get a larger chunk of CPU whenever it gets scheduled. Right. And you maintain some levels to do this. So you also, it's not an unbounded list, so you have a maximum bound on how much time quantum you're going to have for a job. Okay. So the question is that if there's a compute bound process that's running and there was an interrupt that occurred in the middle of this compute bound process, then the interrupt will get served. And won't that serving of the interrupt cause problems with the calculation of this quantum? Is that your question? No. So hold on. An interrupt will not know. So a network interrupt will not cause a scheduling behavior. Right. Let's just work in this model that device interrupts don't cause scheduling changes. Right. Device interrupts are meant for device attention. For example, a device interrupt has occurred. Let's say a network card created an interrupt. All it means is that the device needs some attention. Maybe it wants that I should copy some buffer from there to here. And then I'll go resume the process that was already running. That's all. Okay. And the only time that you do a scheduling switch is either when the process actually voluntarily eats the CPU or if there was a time interrupt, which caused a preemptive schedule. Okay. All right. So, but what this is going to do is that any process that's very IO bound. So let's take an example. Let's say you are running GCC on your system and you're running an editor like VI. So VI is an IO bound process and GCC is a compute bound process. And so let's say these are both relatively long running processes. So eventually what will happen is that GCC will have a lower priority, but a larger time quantum. And VI will have a higher priority and a smaller time quantum. And that's exactly what you wanted. Right. VI doesn't need that much CPU, but it needs very high attention spans. GCC needs lots of CPU. It doesn't care about response times. Okay. Right. So with that, you can basically have some level of, you know, you can have good levels of throughput and yet have acceptable levels of response times. But let's say, you know, if I have something like this, then isn't it possible that all the lower priority threads keep starving? So let's say there are lots of IO bound jobs that are waiting in the queue, then the compute bound job will actually never get to run. Right. So this is a nice idea in principle, but, you know, one needs to do some modifications to make sure that there's no starvation possible. Anytime you have a strict priority system, starvation is always possible for the lower priority processes. So the way to deal with this is that you basically, if there's some process, you basically keep increasing the priority of a process at some rate with time. So if a process hasn't been scheduled, so for example, one way to implement this is on every scheduling quantum, if you don't choose a process, implicitly you increase the priority of that process by some constant number. So all the processes that didn't get chosen on that quantum, their priority will get increased. And so eventually a process will reach the highest priority and will get to run. Right. So that way you deal with starvation. But its quantum will be decreased. Yes. Okay. Also, if, you know, if you have decided that a job is compute bound, so let's say it has moved down. So it has, you know, its priority has been decreased. Its quantum has been increased. But, you know, let's say there's a process that was compute bound and then it suddenly became IO bound. So, you know, we are using this assumption that past is equal to future, but past is not always equal to future. And so the system should be able to adapt to such changes. And so there should be a way for the process to move up. So we have discussed the way to, for the process to move down in the priority hierarchy, there should always also be a way for the process to move up. For example, you could say, if a thread blocks without exhausting its time quantum, then, you know, you move it up with some kind of policy that you want to use. And you can choose different policies. So in general, when you're choosing these policies, you will favor recency over frequency. So you will look at the most recent behavior to decide whether it's IO bound or compute bound. You know, you won't, so just like, just like a caching, cache replacement algorithm, in general, you know, recency is a much better metric to predict the future than frequency. So we don't look at, you know, what's its behavior over the history of its execution. We just look at its behavior over the last time quantum or last five time quantum. And that's good enough. All right. Okay. Great. So, so that's, you know, a very simple notion of scheduling, but actually scheduling is, is a relatively complex topic and, you know, subject of much research over so many years and still not completely mastered. So let's see. Let's say these are two CPUs and there are lots of jobs that need to run. So the question is, how should one choose which job to run on which CPU and when? So we have answered when to run which job. So there's some, some, some idea we have there. We're going to do it in round robin order and you're going to have some priorities and we're going to give higher priority to IO bound jobs and we're going to give lower priority to lower bound jobs. But does it matter which CPU I schedule them on? Well, actually it turns out it matters because CPUs have caches. And so it's, it's much more beneficial to schedule the same process on the same CPU repeatedly, because what will happen is the, the, the working set, the memory set of that particular process will, will, the respective cache will get warmed up with that working set. And so you will have much fewer cache misses. And, and, you know, saving the number of cache misses is a very significant optimization for any compute bound process, especially on modern systems. So, so then there is something that's implemented in, in operating systems today is what's called affinity scheduling. So you, you run some sort of, you know, logic to basically try and ensure that the same process gets to run on the same CPU if it gets rescheduled, right? It's not a strict affinity, but it's a high probability affinity scheduling, which means that a process has an affinity to a particular CPU. So whenever it gets, it, it, it gets, it's scheduled, it will, it will, it will, you know, one CPU will be preferred over the other CPU. And can be implemented in multiple ways. You could have separate queues for each scheduler, each CPU. And, you know, so that these processes only get to schedule, get to schedule on these CPUs, this CPU, and these processes also get to schedule on all these CPUs. Notice that this cache, caching also has a significant bearing on the time quantum. Your time quantum should be large enough such that the process that gets to run has enough time to warm up its cache and then run on a warm cache. If most of the time on, in that time quantum is just spent in warming up the cache and then it gets scheduled out, that's a very wasteful system, right? So you should have enough time in your scheduling quantum to warm up the cache and then run on a warm cache for 90% of the time. So let's say 10% of the time is taken to warm up the cache and then 90% of the time is actually spent in running on a warm cache. So you can have much better cache utilization if you are doing affinity scheduling. There's also something called gang scheduling. So if there are processes P1 and P2 that communicate with each other, let's say they have a producer-consumer queue between each other. Let's say one P1 is a network thread and P2 is a server thread and they need to talk to each other because they have a queue in the middle and they will just keep exchanging information between each other, right? So there's a, let's take an example, let's say there's a producer-consumer queue or any other thing, so they're communicating, talking. And now there is P3 and P4 that are also talking to each other. So it will be a much better scheduling policy to basically say that P1 and P2 will get scheduled together. So they will get scheduled as a gang, so that's called gang scheduling. So you know, P1 and P2 will get scheduled together, so when they run they actually, you know, simultaneously, he produces, he consumes, and so this buffer doesn't get full, there is no waiting and spinning, and so it becomes very fast. As opposed to, let's say, I didn't schedule P1 and P2 together, so P1 gets to run, P1 fills up the producer queue, and then spins for some time, just wasting blocks, which basically means there's extra overhead. Then P2 gets to run, he empties the queue, then he again spins on an empty queue or blocks, and these are all wasteful operations, as opposed to, you know, P1 and P2 running simultaneously. So it can actually significantly increase throughput for communicating processes, so gang scheduling. So the question is, can the operating system always figure out that these are communicating processes? So let's say there were two processes that were using a producer-consumer queue, that producer-consumer queue was, or there were two threads that were using a producer-consumer queue, and the producer-consumer queue was completely implemented in user space. The OS has absolutely no knowledge about this sharing behavior. You know, there are some methods to figure out that there is some sharing, but those are too long-winded to basically be really practical. If, for example, you know, these processes are communicating using a pipe, using an operating system pipe, then yes, the operating system has full knowledge that P1 and P2 are communicating with each other, he's writing to the pipe, and he's reading to the pipe, and so the operating system can actually schedule them together, and get much better performance. Okay, but in general, you know, I'm going to give you some examples to show that scheduling is actually complex. The one example is thread blocks after holding spin lock, or while holding, instead of saying after, let's say while holding spin lock. So let's say there are, you know, there are 10 threads in the system, and they are all wanting to operate on some shared data, and so they need to have a common lock, right? And so there's one thread that takes a spin lock, and before it could actually release a spin lock, it either blocks, which means it actually makes a, you know, it blocks voluntarily, which means it makes an I-O call, let's say it makes a disk operation, or there was a page fault, right? So page fault will also cause the thread to block, right? What will happen if there's a thread which is running, and there's a page fault? You basically just say that the thread is no longer ready, you wait for the disk to bring the page into memory, and you let other processes run in the meanwhile, right? That's what you're going to do. So even if there's a page fault, or if the process calls the system call, which is disk read, or any other I-O call, or if there was a preemption, there's a timer interrupt while you are holding the spin lock, all these cases are examples where you're holding the spin lock, and you have been rescheduled, you're no longer running on the CPU. And so what will happen is all these other nine threads will get to run, and they will just probably spin on the spin lock for the entire duration of the time quantum. So they will exhaust, so spin lock is an example of synchronization, which does not involve the operating system, right? Just like a producer space, producer-consumer queue has absolutely no visibility for the operating system, spin lock is an example of a synchronization primitive that has no visibility for the operating system. The operating system has no idea that there's something called a spin lock, right? A spin lock is completely existing in the user space, now this thread actually takes the spin lock and gets rescheduled, all the other nine threads just keep spinning on the spin lock, absolutely doing no useful work, but the operating system schedules them one after another, and wasting a lot of CPU cycles. What would have been a better way to do this? Well, if the operating, firstly, if the operating system knew that there's something called a spin lock, that would have been the better thing to do. What does it mean for the programmer? So, you know, and there are other ways to deal with these problems, and those ways to deal with this has to do with operating system design. So in the Unix semantics, as we have discussed, you know, this is possible, and this is a very bad thing that can happen, and the only way the programmer can safeguard herself from, you know, not having this kind of problem is to make sure that your critical, if you're only protecting critical sections, which are very small, using spin locks, all right? Also, you make sure that within those critical sections, you're not making any blocking calls. For example, you're not making any disk reads, or disk types, or any other system calls that can take a long time. Also, try to make sure that you're not accessing too much memory, so that the probability of a page fault in that critical section is small. These are some safeguards that you can take to ensure that, you know, with Unix semantics of an operating system, you basically don't have these kind of problems where, you know, all threads wait for the spin lock. What could have been, you know, I'm constantly using the word Unix semantics. What could have been the other semantics? So, in the semantics that we have discussed so far, we are saying that the operating system has full power. It can take away the CPU from the operating system any time it likes, right? The other way to do it is, you can tell the process, I want to take the CPU back from you, and the process has some idea of what are the resources it's holding. For example, it's holding spin lock, and it can release those spin locks, or, you know, finish up the critical section before it releases those spin locks, and then say, here I am. So, instead of a preemptive snatching of the resource from the process, it can be a more civilized, you know, requesting of the resource. You basically tell, ask the process, you know, I need the resource back. So, you say, okay, give me a second, I'll just clean up a thing so that, you know, you have best throughput, and then, you know, he gives it to you. Of course, you know, this more civilized thing has the problem that the process is not trusted. So, you know, if you've asked him, you know, what's the guarantee that he'll actually obey you. So, in this case, you know, you also have some fallback mechanism. So, you basically, you know, after you've requested him, you wait for some time to, you know, and if he doesn't behave as a good citizen, you actually snatch it from him, you know. So, these are ways to try to prevent this problem. I'm going to discuss some operating system models that actually do this kind of thing, and it's very useful. Okay, so question is, you know, spin lock is required by CPU, not by a thread. Well, that's not true. You know, the spin lock that we discussed in xv6 was a per-CPU spin lock because, you know, we also disabled interrupts, so that meant there was no context switching. So, the spin lock was also associated with the CPU. But in general, spin locks need not be per-CPU, they can be per-thread. So, here's an example. Let's say, you know, there are threads, and these are, this is kernel space, and this is user space, and they are basically having a spin lock here, right? And so, and this thread can call acquire on this, and this thread can call acquire on this, and the kernel has no idea that there's a spin lock that's going on. This acquire has nothing to do with clearing of interrupts or anything else. These are spin locks for threads. All right, just like threads, you know, spin locks, they can be producer-consumer relationships. So, spin lock is just one example, but, you know, there can be any type of synchronization between processes. For example, there's a producer thread, and there's a consumer thread, and, you know, the producer thread hasn't actually filled up anything, and it gets switched out, and the consumer thread gets to run, and there's lots of consumer threads. So, all the consumer threads will just, and let's say the way you are implemented the producer-consumer queue is completely at the user level, and these threads spin, if you, a consumer thread spins if it finds the queue to be empty, and the producer thread spins if it finds the queue to be full, and so the operating system or the scheduler has no idea that there's a producer-consumer relationship, and it will keep scheduling the consumer thread only for them to spin and do really no useful work, right? It would have been much better if, if, you know, if there was a way for the operating system to know this, or better, the operating system would tell, indicate its intentions to the process, that I want to take the CPU away from you, or I want to schedule you, and get its feedback on, you know, how much time, you know, when I should do that, what's the right time to do that. Also, there are multiple resources, so I'm talking about CPU, but there are actually multiple resources, like memory, so if I, you know, if I have a high-priority process, let's say I have VI and GCC example, and I'm giving a lot of CPU, I'm giving VI higher priority over GCC, but that's of no use if I'm not giving VI also the amount of memory that it needs, right? If the higher priority process is not getting the same amount of memory that it needs, then the first thing it'll do when it gets scheduled is block by taking a page fault. So priority does not need to be obeyed only on the, on one resource, which is CPU, it also needs to be obeyed, or there needs to be some reflection of it on the other resources, in this case, the physical memory, right? So, for example, if there's a higher priority process, perhaps it's, you know, its working set should be in memory, as opposed to the lower priority process. Of course, once again, there's an issue of, you know, one process shouldn't get, be able to run away with all the memory, and so there has to be some, some safeguards in scheduling the memory. So there's not just scheduling, so there's scheduling in time of CPU, but there's also scheduling in space of the memory, and these two need to work in tandem with each other, that makes things a little difficult. And we've also talked about cache scheduling. So cache is basically, cache is an even scarce resource than memory. So, you know, for best performance, you would want that the cache is properly scheduled, affinity scheduling being one example of a good way of scheduling the cache. And there, you know, similarly, if there is a producer-consumer relationship, for example, let's say, you know, you have, you have CPUs. So, so these days, you get multi-core CPUs. So let's say there are two CPUs, which have two cores each. This is CPU 0, and this is CPU 1, and this is core 0.0, and this is core 0.1, this is 1.0, and 1.1. If there are two processes that have a producer-consumer relationship between them, then it's better to schedule both of them on the same CPU together, right? So that way, you are efficiently using the cache, because each CPU here will have an L2 cache that they can share between themselves, as opposed to going to the main memory to implement the, to do this producer-consumer relationship, right? So there's a very complex thing here that, you know, you first need to figure out that who are having the producer-consumer relationship, then you need to figure out, you know, what are the parts of the CPU, which CPUs have, have shared caches, and then schedule those processes in tandem, in a gang scheduling way to do that. All right. Then, you know, similarly, there's disk. So for example, you know, there are page faults from multiple processes, there's a page fault from the VI process, and there's a page fault from the GCC process. The disk scheduler has no idea which has, which, which request has higher priority, and which should get scheduled first. And, you know, either I, you know, either I don't care about it, in which case I'm not actually weighing strict priorities, there are other parts of the system that have no idea about your priority system, or I, you know, flow this information down, in which case I can severely impact my throughput, right? So if I do, if I do strict priority scheduling at the disk level, then, you know, it's, I cannot, I can no longer follow the elevator algorithm and maximize my throughput. Also, there are server processes, like, for example, the X server, right? So X server runs as a separate process, and all the other processes talk to the X server to, you know, make their request for displaying or rendering their screens. And so the X server has no notion of priority, or, you know, it's very, you know, it'll require more interfaces to ensure that the X server also has some notion of priority. But ideally, if, you know, one process has higher priority over the other process, his request should be given more priority over the other process's priority. So I don't care about GCC's screen so much, but I definitely care about VI's screen, for example. And so, you know, how is this kind of a thing done? From a scheduler's point of perspective, he's lying to, you know, WhatsApp. He just sees that the X server wants to access the display device, but the X server internally has no idea that, you know, which process has higher priority or not. One could build it in, but, you know, that makes life complex. There are multiple things that are happening here. Another example is an NFS server, right? So NFS server is a network file system. You could have a separate server, and there are multiple processes that are accessing this NFS server. But the NFS server needs to be either made aware of the priorities, or you are not actually following strict priorities in your system. Okay? All right. Okay, so good. So that's it for scheduling. I'm going to discuss one very interesting example of nonlinear effects of scheduling. So scheduling in general is a problem if the resources are relatively scarce, right? So if you have lots of resources, let's say CPU is plentiful and you don't care about it, there's no problem, right? Scheduling is not an issue. But the moment resources become scarce, it becomes an issue. If you're running a web server that's handling a hundred or a thousand requests per day, the performance of a scheduler will actually not matter at all. But if you're running a web server that you want to operate at peak capacity, that's hundreds of thousands of requests per second, then you know, the way you schedule your requests and the way you schedule your resources becomes very important. You are, you know, you're bottlenecked by your caches, you're bottlenecked by your disk throughput, you're bottlenecked by the CPU, and so an efficient scheduling and an inefficient scheduling, there can be a large gap in performance between the two. In fact, the gap can be so large that at high throughput, I'm going to show, you know, we're going to discuss an example of where poor scheduling can actually lead to zero throughput at very high loads, okay? So there's a nonlinear effect, it's not necessarily, whereas a good scheduler can actually, you know, saturate very gracefully in presence of high load. Okay, right, so overall, I mean, scheduling was a very important topic in the early days of computers, 1970s, where there used to be large shared computers that had very low computing power and lots of people sharing them. Over the years, the computers became more and more powerful and more and more distributed. We had personal computers and so fewer users and more power, and so scheduling was less of a problem, is less of a problem on a desktop or laptop PCs. But, you know, we are moving back to, you know, let's say what's called cloud computing, where you have a centralized computer or centralized data center and being shared by lots of people, and once again, to maximize the utilization of your resources. So, and so, once again, you know, to operate your centralized systems shared by lots of different processes, scheduling becomes, again, very, very important in the modern world as well. Okay, good. Let's continue the discussion next time."}