{"text":"Welcome to operating systems lecture 29. We were discussing demand paging last time and we said that demand paging is a way for the OS to fool the process into believing that it has more memory than it actually is occupying in physical sense, right. So a process may have, may see a very large virtual address space and recall that the virtual address space is implemented by a page table assuming you are using paging and so it may see a very large virtual address space but only some part of it may actually be present in the physical memory. Other parts of this virtual address space may be present in the disk, right. And so if this is the page table and these slots are the pages and some of these pages will have mappings to the physical memory and some of them will not have mappings to the physical memory and logically speaking they will have mappings to the disk, right. The, you know, this area where you store the virtual memory pages in the disk is also called the swap space, right, or the swap partition if you may have heard on different operating systems, all right. But it is not necessary that these pages point to the swap space. The swap space is basically extra space reserved in the disk to allow pages to get saved from physical memory and so on but these pages may actually even be mapping into the file system, right. For example, if you loaded an A.out file then some pages have not been loaded at load time and so these pointers will actually not be pointing to the swap space, they will be pointing to the file system space in the disk. In either case basically these pages from the hardware standpoint don't exist, right. And so if a user ever executes an instruction that tries to access a virtual address somewhere here it will result in an exception and this exception is called a page fault, right. So a page fault will occur and as we know if an exception occurs control transfers to the operating system and the operating system's page fault handler will get to run. Without demand paging the page fault handler would have probably just killed the process but if the operating system is implementing demand paging then the page fault handler should additionally check if the faulting address or the address on which the page fault was generated, if that actually has points to a disk. So internally the operating system is maintaining some data structure which basically says, you know, which pages are mapping to the disk and which pages are not present at all, right. So if this page is actually mapping to the disk then the operating system will load this page from the disk to the physical memory and then create a mapping from the page table to the physical memory and restart the instruction, right. Now restarting the instruction needs to be safe as we had discussed last time and we, you know, typically one way to do this is basically to rely on the hardware to provide certain guarantees. So one guarantee that provides the hardware is that if an instruction generates an exception and page fault is one example of an exception, then before the exception handler is called the machine state will appear as though the instruction didn't execute at all, right. So it's not like the instruction is partially executed. So the page, even though the instruction generated an exception in the middle of its execution, the responsibility of the hardware to basically roll back all execution state of that instruction before it invokes the exception handler. So all the instructions previous to that instruction would have executed and that instruction won't have executed at all. And of course no instruction after that would have executed. So now the page, so that allows the operating system to just restart that instruction and how does the operating system restart the instruction? Just by putting the return address in the PC, in the trap frame, to the instruction that faulted, right. And so when the operating system does an irate it just goes back to the same instruction and that instruction gets to run again. Because the page table mapping has been created by the operating system, this time this particular instruction does not fault, right. And so it will continue. And so this is, this is demand paging, the idea, so using this demand paging mechanism, an operating system basically in some sense is using the physical memory as a cache for the disk. So you can imagine that this is, you know, one way to think about it is that the entire physical memory pages are actually on the disk. But the physical memory, the entire virtual memory pages are actually on the disk, but the physical memory is acting as a cache to that space, right. So it's just that some pages have been loaded into physical memory and, and so, you know, you basically, so most of the, so, so whichever pages have been loaded to the physical memory execute at full speed. Any pages that have not been loaded into the physical memory or the cache have to take a page fault and you basically access the disk in that case. Let's understand what some hardware characteristics of physical memory and disk to understand, you know, what are the tradeoffs involved in this, in this caching setup. The physical memory is, you know, one common technology is DRAM. Today you will get 10 to 100 GBs of, hundreds of GBs of DRAM, but it's more expensive. It's usually 100 times more expensive than, than this, 100x more expensive, roughly speaking. And disk on the other hand, you can get, you know, hundreds of GBs to terabytes and so, but it is much cheaper. So you can, for the same, so byte, cost per byte is 100 times more in physical memory than it is in disk. So you can afford to have much larger disks than you can have physical memory. The more important tradeoff point is that a disk access takes milliseconds to access, while a memory access takes nanoseconds to access. So there's a, you know, 10 to the power of 5 or 10 to the power of 6 performance difference between, between these things, right? Okay, so you can imagine that if there is such a huge, so basically a cache hit is only going to take 10 to 100 nanoseconds, let's say, but a cache miss is going to take milliseconds. So, you know, miss cost is much higher than hit cost and so for the scheme to be successful, you would want that the hit rate is very high, right? If the hit rate is small, then you would basically be always, you know, you'll be executing at the speed of the disk. So let's take an example. Let's say you have a hit rate of, let's say you have a hit rate of 90%, all right? So 90% hit rate, okay? So let's say I had a 90% hit rate and a 10% miss rate, right? So what is the, what is my average memory access time? Average memory access time would be 90% into, let's say, 100 nanoseconds plus 10% into, let's say, 10 milliseconds, which is roughly equal to, actually this cost is almost zero, this is actually 10% into 10 milliseconds, that's roughly equal to 1 millisecond, right? So on average, every memory access is going to take 1 millisecond of time, right? That's a very bad, I mean, your actual physical memory was actually only 10 to 100 nanoseconds and now, you know, if you have a 90% hit rate, you are basically executing at the speed of the disk, completely unacceptable, right? So these kind of numbers are absolutely not acceptable, what you would be perhaps okay with is something like a 99.9% hit rate or something, right? So less than 0.01% miss rate, and so that will basically give you the illusion that yes, you have the space of the disk, but the access time of memory, right? So really, the OS is trying to give the illusion that you have the space of the disk, which is, you know, 100 GBs to TVs, but the access time of memory, right? And fortunately, in practice, it's possible to do that, and it's possible to do that because of this 90-10 rule, I mean, also called the 90-10 rule, it basically says that there's a high amount of locality in accesses by typical programs. So programs typically tend to access the same memory locations over and over again, so that's called temporal locality. Also programs tend to access locations close to the ones that they have access. So if they access location X, then it's quite likely that it'll access either X plus 4 or X minus 4, so that's called spatial locality, right? So programs exhibit a lot of spatial and temporal locality, and so caching, the whole basis of caching is that there needs to be some locality, right? And because, and there's a huge amount of skew, so basically the 90-10 rule says that roughly speaking, typical programs, 10% of memory gets 90% of the references. So if this 10% of memory is brought into the physical memory, then, you know, you already have a very high hit rate. The 10% of memory gets 90% of the references, so if I were to just plot a graph which says, you know, let's say this is the memory address 0 to, let's say, whatever is the maximum virtual address you have, then, you know, and you were to plot what memory addresses are accessed how many times. So on the Y axis, you're basically saying, how many times was this memory address accessed throughout the life of the program? Then you know, you have some small thing here, which is saying that these memory accesses are accessed sometimes, once or twice or thrice or 10 times or whatever, but there's some memory accesses which are, which have a huge number of accesses, right? You can, what are these memory addresses likely to be? Let's say the code pointer, right? So if you are executing a program in a loop, the same EIP or the same program counter is going to get accessed over and over again, right? Or the stack pointer, it's likely that the stack is basically, the stack pointer is moving in the same range, so you're basically accessing the same page over and over again, right? Or even data, so you know, even the data structures of the heap, it's quite likely that, you know, the root of, let's say you are maintaining a tree in your, you know, in your program, let's say a binary search tree or something, then the root of the tree is likely to be very hot, you're going to access that root each time you do anything on the tree, right? So in general, there's a high amount of locality, and you know, the higher this bar, it basically shows, you know, temporal locality, because, you know, the same location is being accessed longer than, and the spread of it can be, you know, called spatial locality, because if you are accessing this, you are also accessing somewhere, something close to it, so it sort of seems to have this kind of behavior. So an operating system would be doing its job correctly with respect to demand paging if these pages, which are very hot, are mapped into physical memory, and these pages, which are cold, are mapped into disk, right? So you can also call these addresses cold addresses, and these addresses hot addresses, right? So the hot addresses should be mapped into physical memory, and the cold addresses should be mapped into disk. And if you can do that, then you basically have a system where you have the speed of physical memory and the size of the disk, right? Okay. Right. Okay, so we're saying that the physical memory has cache, right? So one way to think about this entire demand paging scenario is to think of the physical memory as a cache, as cache to the disk, which is much bigger. So clearly, you know, anytime you talk about a cache, we basically know that the cache has to be smaller than the entire data set. And so whenever you talk about a cache, you have to worry about, you know, something. For example, what is the block size? By block size, it means what is the unit of data which is cached, what's the smallest unit of data that is cached? For an L1 cache, for example, it will be a cache line, which is, you know, 16 to 64 bytes, depending on your architecture. But for this, what is the block size? One page, right? So one page, right? How does one choose the right block size? Well, it shouldn't, if the block size is really big, then you may be getting too much extraneous data. So let's say in this figure, you are accessing address X, and your block size was, let's say, four megabytes. Then you're going to, you know, you're going to bring in X to X plus four megabytes into the memory. And it's quite possible that, you know, all that data is not, it's very cold. It's just that X and X, some data around X was hot, and everything around it was cold. So you're basically polluting your cache with cold data. So if you choose a very large block size, you have that problem, that you're basically having cold data sharing. On the other hand, if the block size is really small, then firstly, you will hurt spatial locality, right? So let's say X and X plus four are very likely to get access together. But because your block size is very small, you don't get X plus four, you just get X. And so for X plus four, you take another page fault, right? So that's one thing. But more importantly, recall that our disk has a very long sort of access time, right? And also, the access time is basically dominated by the seek and the rotational latency, right? So it's better that if you have paid that price, you get a large chunk of data from the disk at once, rather than just getting one byte, you get a larger chunk of data from the disk. So there has to be some trade-off between these two constraining forces, and the one page seems to be a reasonable value, and we'll discuss more about this later. The other thing is, what is the organization of the cache, right? By organization, I mean associativity. In your computer architecture course, you must have studied, and a cache can be direct mapped, set associative, or fully associative, right? So what do you think makes most sense in this case? Should you have a direct mapped cache, or should you have a fully associative cache? Okay, so let's see, what's the trade-off, let's say direct mapped versus on one side, and fully associative on the other side. So in direct mapped, I basically know that here's the location for these set of addresses. And so when you get a miss, the one that you replace is the, it's very clear whom you're going to replace. There's the, you know, there's a conflict between the addresses of the one that's present, and the one that's accessed, and the one that's present gets replaced, and the one that's being accessed gets to run, right? So in other words, you know, replacement is very fast, alright? But the price you pay is that you may get extra misses, conflict misses, even though you could have adjusted both pages in your memory, because you're doing direct mapped, you are getting more conflict misses, right? On the other hand, fully associative has full freedom in choosing the page that needs to get evicted, right? So in this scenario, where the miss cost is really high, and maximizing your hit rate is of prime importance, and the cost of actually finding the page to replace is not going to be very high compared to the miss cost, right? How many instructions will need to be executed to find the page, to find the best candidate for replacement? It's only some memory accesses after all, right? And whatever those memory accesses are, you can actually execute a million memory accesses before you actually even reach close to the miss cost, right? So you know, because the cost to actually find the replacement element, even in a fully associative cache, is not very high compared to the miss cost, and because hit rate is of prime importance, it's important, a fully associative cache makes sense, right? As opposed to a set associative or a direct mapped cache, all right, okay, all right. So let's say, you know, how does one, so what's the hit path? Well, we want that the hit path should be really fast, which means how does one, how does the, how does the system check whether the access is a hit? Well, a hit actually has no software involvement, it goes purely in hardware. So an instruction executes, it has a memory address, the first thing you do is you check the memory address, translate the memory address using the TLB, right? If you hit from that, you have just done the translation in one nanosecond. So that's the fastest hit you can have, right, that's one nanosecond. If you don't hit the TLB, then you do the page table walking. Page table walking will be 10 to 100 nanoseconds, so that's the second level hit, in some sense. And if you hit, if you miss there, then you do the disk access, which is millisecond, right? So hit path is basically completely implemented in hardware. And miss path will require software involvement, because the miss path will require the execution of the exception handler, right? What's the miss path? Well, miss path is page false, right? And this is much slower, okay? All right, and let's see what's the page replacement policy, or replacement policy. Every cache must have a replacement policy, right? So what's the replacement policy? Basically you want to bring a page into the cache, and you have to figure out, and your cache is already full, so you have to figure out one page whom you're going to replace. And as I said, it's very important to maximize hit rate, so what should be the replacement policy? You know, it should be some kind of fully-associated policy. Requires more discussion, but sort of LRU. Least recently used. You've seen least recently used before in computer architecture or something, right? So sort of LRU, and we're going to discuss this more. Basically, what happens on a write? So recall that a cache needs to have a write policy. So if you write something, should it be a write through, which means it goes all the way, on every write, you go all the way to the bottom? Or should it be a write back? Definitely write back, right? Because if it's write through, then every write is going to be accessed at this space, so definitely write back. Now, all right, so let's see, how does the operating system implement this? So block size one page, this is handled by the page table. Associativity is fully-associated. This is, you know, this is the page, this is handled in the software by the page table handler. So this is wrong and this is right. Hit path is hardware. Now, but in the hit path, you also need to, so the hardware should also tell whether it was a hit or a miss, right? So the hardware is telling whether it's a hit or a miss using what? Using the, yeah, using the page fault, and how does it know whether it needs to generate the page fault? By looking at the page table entry and the present bit in the page table entry. So that's the way, you need a fast way of figuring out whether it's a hit or a miss, and the fast way is basically for the software to set the present bit and the hardware to read the present bit. So basically, the present bit in the, in the page table entry is helping you to figure out whether it's a hit or a miss, and you need to do it fast, so it's happening in hardware, right? And miss path is the page fault handler. Replacement policy is implemented in this, but what happens on the right? Definitely write back. How does, you know, how does the hardware indicate that this page is actually needs to be written back? So how does the, how can the hardware indicate? Basically, if it's a write back policy, the program should execute at full speed, and it should just keep writing to these pages, and at cache replacement time, you will basically figure out whether this page needs to be written back to the disk or not. One conservative policy is that whenever you replace a page, you always write it back. But that's unnecessary, because it's quite possible that the page was only read from and never written to, and so the contents haven't changed, so you don't need to write it back. And so you can save disk accesses, and recall that saving disk accesses is a huge optimization. So what's some other way of letting the, for the hardware to let us know whether this page has been written to or not, or modified or not? Maintain a dirty bit. Okay, maintain a dirty bit in the page table. So one option is, so what's done on x86 is that the page table entry has another bit called the dirty bit. The operating system, when it brings the page into the cache, clears the dirty bit, and if the page is written to, the hardware sets the dirty bit. Notice that because, because the setting of the dirty bit is on the hit path, it should be passed and it's done by the hardware. At cache replacement time, the replacement, the page fault handler will basically look at the dirty bit to figure out whether it needs to write it back to disk or not write it back to disk. Alright. So, the dirty bit will be set by the hardware. It will be cleared initially by the software. When it brings the page in, it will be cleared, because at that time the page is clean. And by clean, I mean that the contents of the page are identical to what they exist on the disk. Right? So as soon as you bring the page into memory, it's clean. If you, if the program ever writes to that page, the dirty bit in the page table entry gets set. Okay? Alright. Okay. Right. So now, let's also look at what to fetch and when. Right? So, for example, what should I fetch at load time? Right? So this is a design question that an operating system designer has to figure out that, you know, at load time, if you basically say I want to run this process, what pages should be fetched into physical memory a priori? One option could have been that I just ask the user. So provide an interface between the user and the operating system. For example, have an extra argument into your exec system call, saying this is the list of pages that you should load a priori. Right? This is possible. It has been tried. But the problem with this is, number one, you don't want to trust the user. The user may say load all the pages, and you know, you don't want to really satisfy his request because that can lead to other processes having poor performance. Secondly, the user can, the user himself actually doesn't know and doesn't want to care about this. Right? When you write a program, you don't care about what pages are hot and what are not. You basically want to basically say that, you know, here's my program, you run it, and you figure out what's hot and what's not, and just manage things efficiently for me. Right? So asking the user is both sort of not very trustworthy, and it complicates interfaces and puts more responsibility on the shoulders of the programmer than actually really needed. The other thing is you could load some initial pages at load time. You could say that, you know, whatever the first program counter that needs to get executed, I'll load one or two or three pages around that. So some kind of heuristic policy around that. Then you load some pages which are for your stack, and maybe some data pages based on some heuristic which an operating system is free to choose, and different operating systems will have different variants of this. And then, as the program executes, it's going to access the memory, and then you implement demand paging. Each time there's a page fault, once again, you have a choice. You only fetch the page that was faulted upon, or you fetch some pages around that also. So you can say, you can have something called read-ahead, which is basically for spatial locality. So whenever you fetch a page, you also fetch some pages around that page. So that's called read-ahead. And that's basically, that's going to help if you are likely to have a lot of spatial locality in your program. And most operating systems will do some sort of read-ahead somewhere, right? And when to do read-ahead and then not to do read-ahead is also a bit of an art, and you know, many heuristics are present in real operating systems to figure out this. What is the trade-off of doing too much read-ahead? You may be polluting the cache with unnecessary data. And what is the disadvantage of doing too little read-ahead? You may be incurring too many page faults unnecessarily, right? So that's basically. Also, you know, you could do what's called prefetching. So this is for temporal locality. If you basically say that every time you access page X, I see, I've noticed this pattern that every time you access page X, you also access page Y. So if the operating system under the covers has been noticing this pattern in your program, then it will basically say, okay, you know, each time you take a page fault on page X, let me also prefetch page Y. And this can happen, for example, you know, whenever you access this particular code region, this particular code always accesses this particular data structure, right? Or there can be many such examples. I'm just giving you one example, right? So notice that, you know, the operating system is inferring all this performance decision under the hood without the programmer having to know anything about it. And that makes the programmer, you know, really carefree about these things. And that's a nice thing. And it has worked for many years now. And it has some limitations. However, and for example, in modern hardware like multi-core, you know, these kind of inferences are harder and harder to make. And all kinds of ideas are being proposed on how to do this better with or without programmer involvement. Right. The other thing is cache replacement policy. So what page to evict? So you have figured out that this is the page I want to bring in, or this is a set of pages I want to bring in. So if you have decided that these are the set of pages you want to bring in, what are the pages you're going to throw out or eject, right? So the first policy is random. Just pick up any page or any set of pages in the thing and just, in your cache, and just eject them. The advantage of random is that it's very simple. You don't need to maintain any data structure. The disadvantage is that you may actually be ejecting hot pages. And that's not a good thing, right? The other approach is FIFO. Whichever page has been brought in last, you know, earliest, should be ejected. So basically, the way to implement it is basically have a queue in your memory. It basically says, this is, you know, every time you bring in a page, you append to the queue. And if you want to eject, you eject from the tail of the queue, right? Once again, the nice thing about FIFO is that it's very easy to implement. Each time you bring a page in, you add this to the data structure. Adding to the data structure is cheap with respect to the full cost of the page fault handler. You're also making a disk access. So, you know, just adding to the list is very easy. Also, you know, removing from the list is very easy. It's very cheap. You know, you're just anyways ejecting the page. You're probably going to have a disk slide or anyways taken a page fault. So you know, ejecting is easy. So but the disadvantage is that this organization completely ignores the usage. So let's say this page was hot and this one was cold, right? And you're bringing in some page. So because it's FIFO, the hot page gets removed and the cold page remains. So you're ignoring usage. Okay. So let's say, you know, let's turn it around. Let's say, what would have been the optimal? So let's say, what's the optimal page replacement policy, right? Assuming I had an oracle who can tell me, you know, who can give me every all the information I need, what would have been the optimal thing to do? So the optimal thing would have been that if I'm bringing a page in, I want and if I have an oracle who can tell me what the future is going to look like, I can ask the oracle, what's the page that's likely to be used furthest in the future, you know, so of all the pages that I have, what's the page that's going to likely to be used furthest in the future. So that's the page I want to replace, because all the pages, all the other pages will be used before that page. And so, you know, you want hits on those pages, and the one that's furthest, that's the one you want to replace. Right? So let's say, let's call this policy the minimum policy, which is basically replace the page not used for longest time in future, right? Of course, I'm, I'm assuming that there is some oracle that's telling me what the future is going to look like. And, you know, this is impractical. I don't know what the future is going to look like, which pages are going to get access depends on what part the program takes, right? And I cannot predict as an operating system, what part of the program is going to take, I can get them, I can ask the user for them. But these are all sort of just heuristics and asking the user is actually very error prone. So you don't want to do that. So LRU is an approximation to this, assuming so one common principle design principle is that if you don't know the future, assume that future will look something similar to your past. So what has happened in the past, the future is going to look something similar to that. All right. So if I were to, if I were to sort of make this assumption that past is equal to future, then instead of saying replace the page not used for longest time in future, they replace the page not used for longest time in past. So whatever is the page that has not been used for the longest time in the past, that's the page you will replace, right? So just replace past with future. If past is equal to future, then LRU is equal to min. If past is roughly equal to future, LRU is roughly equal to min. All right. So that's the idea. Okay. All right. Okay. So we looked at how random is implemented, we looked at how FIFO is implemented. Clearly you cannot implement min, it's a completely impractical algorithm. But you can implement LRU and let's take, let's look at what it takes to implement LRU. All right. So one way of implementing perfect LRU is that let's say you have a page table or virtual memory space, whatever you want to call it, and these are the pages. Each time you access a page, you also, you also store a timestamp with when it was accessed. Each time you access a page, you, you know, record the timestamp at, when it was accessed. And then when you want to replace, you go through all your pages and look for the page with the smallest timestamp. Now, who should put the timestamp on the page? Hardware, right, because it has to be on the hit path. So if I cannot have, I do not, on the hit path there should not be any software involvement, I want the hit path to be as fast as possible. So the timestamp has to be put by hardware. And so that doesn't sound very practical, right, I mean, what kind of timestamp should be put by the hardware and how big the timestamp should be. And each time if I have to put a timestamp, then there's extra overhead. So recall that, you know, each time I put a timestamp, I have to make a memory write in some sense to put the timestamp, and that's not, that's not great. Also at the cache replacement time, I have to go do a global sort on the timestamp and then figure out which is the least, right. So global sort may not be that much of a problem given that anyway there is a disk access involved, but, you know, on the hit path having to put this timestamp is not practical. So what's done is actually not perfect LRU, but, you know, let's say sort of LRU. Sort of LRU basically says that let's say instead of putting a full timestamp on your page on your access, let's approximate the timestamp by some number of bits, right. So let's say, let's say I have a one bit timestamp. Instead of a timestamp that's likely to be, you know, a very long timestamp, let's just have a one bit timestamp and the timestamp is going to say whether this page has been accessed recently or not, right. So each, so basically the idea is that there's just one bit inside this page table entry. If this bit is, and this bit is called the access bit, it says whether this page has been accessed or not. When you load the page initially, you clear this bit, you set it to zero. And then when it gets accessed, the hardware will set this bit to one. If this bit is already one, then it gets accessed, then it remains one. It doesn't change. So it's a, it's a one bit approximation to a, to a full timestamp, basically saying whether it was accessed at all since the last time it was loaded or last time it was checked. Right. So let's see what, how this works. So this algorithm is also called the clock algorithm, which is one bit, you know, can be thought of as a one bit LRU. So each page has an access bit or a reference bit, right? Hardware sets this on use and OS periodically clears it. So the idea is that the operating system will periodically look through all its pages and the pages which have the access bit set, it will clear those, that bit. And then it will again periodically look for it. And then if it finds that the access bit has been set between the last time it looked at it, then it basically means that it was accessed in this region. So it basically means it was accessed recently. Something that was not accessed in this time period was not accessed recently. So it's a one bit approximation to LRU in the sense that you're differentiating between a page that was accessed recently, whose access bit is one, and a page that was not accessed recently, whose access bit is zero. The notion of recently is basically the time between two checks by an operating system. So instead of doing a full timestamp and sorting on the timestamp, you just basically divide the pages into two categories, accessed recently and not accessed recently, and you replace the page which is not accessed recently. And you don't replace the page that has been accessed recently. So the way it works is that replace pages not accessed recently in FIFO order. So FIFO had some merit to it, right? FIFO basically said that whoever comes first goes first, right? And it has some merit because it's quite likely that your program first is looking at this data structure, and so it loaded a lot of pages, and now you're looking at this. So FIFO does capture some part of recency in its thing. It's the recency of the first access to the page. And the nice thing about FIFO was that it was very easy to implement, right? LRU was much more fine-grained in figuring out who accessed what recently, but the problem with LRU is that it's too expensive to implement. So CLOCK is a one bit approximation to LRU that does one bit of differentiation between what's accessed recently and what's not. And for everything else, and so for pages in the same category, it just uses FIFO, right? So in a purely implementation standpoint, one way to think about the CLOCK algorithm is that you arrange all the pages in a CLOCK, in a circle, and you have a hand that's a CLOCK hand that just moves, let's say, clockwise, right? So let's say you want to replace a page. You just move the hand until you find a page that has access bit equal to zero. So each of these pages has an access bit equal to 1, 0, 1, 1, 0, 1, and so on, let's say. And so the CLOCK hand is going to, so let's say I want to replace a page, I will move the CLOCK hand. Any page that I find that it has been, its access bit is 1, I will skip it. I will not just skip it, I will also change it to 0. Basically means that I have seen it in one revolution, I've seen it and I saw it as access, and so I clear it to 0 to basically see that by the time I reach it next, has it been converted to 1 or not? The moment I see a page that has not been accessed since the last time I cleared it, which means its access bit is still 0, I'll take that for replacement. So this algorithm is basically doing what I said before, it's differentiating between pages that have been accessed recently, and now here recently means accessed in the last revolution of the CLOCK hand, in the last one full revolution of the CLOCK hand. So it's distinguishing between pages that have been accessed recently and pages that have not been accessed recently. For the pages that have not been accessed recently, it's replacing them in a FIFO order. So basically, why is it a FIFO order? Any time you pick a page to replace, so let's say you pick this page to replace, you will add the new page at this location and move the CLOCK hand next. So the new page will get added just before the CLOCK hand. So all the not accessed pages are replaced in FIFO order because of that. So the CLOCK hand just sweeps through all the pages, the algorithm is, if it finds a page that has bit equal to 1, it marks it to 0 and skips it. If it finds a page that has bit equal to 0, it replaces it, and it puts the new page at that position, and the CLOCK hand just points to the next page after that. So it's pushed at the tail of the queue in some sense. The pages that are not sweeped are not set to 0, but they will be set to 0 the next time the CLOCK moves in that direction. So even the pages that have been set, that have been accessed, are being examined in FIFO order. There's an order in which they are basically being examined. You basically have two categories, recently accessed, not recently accessed, and you're processing both of them in FIFO order. And one way to think about it is basically you arrange the pages in a CLOCK, and you let the CLOCK hand rotate. And you use this algorithm that if accessed is equal to 1, skip, set accessed is equal to 0. Set A is equal to 0, let's just say that, right? Else, replace. So each time you need to replace a page, you just move the CLOCK hand, and you basically figure out which page to replace, all right? When you replace, should you set A is equal to 1, or should you set A is equal to 0? Both are possibilities, but typically you would set A is equal to 0. We'll let it remain as it is, and see whether the program accesses it or not. Likely that program is going to access it, after all, that's the reason it's being brought in. But let's say you were doing read-ahead, then you brought in some extra pages, and so you have set the access bit to 0, and if let's say the read-ahead pages were not, the read-ahead pages were not accessed, so then they will get replaced in the next revolution. Okay. All right, so this is how it works. So what does it mean to say that the hand is sweeping really fast? It's just going, so each time, by saying that the hand is sweeping really fast, I'm saying if there was a page fault, and I wanted to bring a page in, I had to go through a lot of pages before I could find something that could be replaced. What does that mean? That basically means that the pages are really being used. Many pages are being used. In other words, the hot memory footprint of the program is very large. It's probably larger than what the memory can actually support. In other words, you need to increase the size of your physical memory. Right? So if the physical memory is smaller than the size of the hot memory footprint, also called the working set of the program, then you will see that the hand has to move a lot of, a huge amount, very fast. Right? Let's say I'm doing a completely synchronous replacement. Basically, it may happen that each time I want to replace a page, all the pages have been accessed before I get the page, before I basically see a page. So I have to do one full revolution before I actually get to replace a page. So for every page that I replace, I basically do one full revolution. So that's, you know, that's really, really bad, basically doing a full global operation going through all the pages. And it also means that you're basically having a lot of, you know, you're likely to have a lot of misses. The miss rates are very high, and it shows that you need to increase your memory size. On the other hand, if the hand is sweeping really slow, it basically means that your memory is larger than your working set size, or the set size of the hot memory region in your code. Right? So the question is that if I have to make a full revolution, that basically means that all these pages were hit before I actually find something that I actually get to, get to replace. Right? So doesn't it mean that because all these pages were hit, doesn't that mean that the miss rate is small? Well, no, not really, right? Because all these pages are hit, but there are other pages that are also getting accessed. Basically means that all these pages are, all the pages in your memory are hot. And yet you are basically bringing in, you know, taking misses on other pages that also might be hot. Right? So you're basically taking misses, and you are, your working set size seems to be bigger than your physical memory, so you're likely taking a lot of capacity misses. Because all these pages are hot, so you're finding it difficult which page to replace. That clearly shows that, you know, your miss rates are likely to be high. Okay? Another sort of modification to this is what's called a two-hand clock. So if, if your memory size is really big, then you may not want to make a full evolution on your entire memory on every page fault. And so what's done is basically instead of having one hand, you have two hands. One is called the leading edge, and the other is called the trailing edge. The leading edge clears access bit, and the trailing edge evicts if access bit is equal to zero. Right? So what have you done here? You have basically said that the leading edge clears the access bit, and the trailing edge evicts if access is equal to zero. You're saying that the page will get evicted if it gets accessed in this interval. In the one-hand clock, I was saying that the page will get evicted if it was accessed in one evolution. Here I'm saying the page will get evicted if it gets accessed in this interval. And both of these, so this angle is fixed. Let's say this angle is theta, so the theta is fixed. So whenever the leading edge moves, the trailing edge moves along with it. Right? And so a page gets replaced if it was not accessed within this theta interval. So instead of theta being 360, as it was in the case of one-hand clock, now theta is something smaller. And you can choose theta depending on what your memory size is and what's the maximum overhead you want to have on your page fault. And so this basically makes sure that, you know, you will examine only that many number of pages before you actually find something to replace. Okay? If the angle is too small, what does it mean? Let's say theta is equal to zero. It basically means it's a FIFO. Okay? Because the leading edge clears the access bit and trailing edge just evicts it immediately. So basically it means you didn't care about the access bit at all. Right? On the other hand, if theta is equal to 360, you are back to one-hand clock. All right. Okay? Okay. So, you know, just as an example, let's say, you know, I had a machine. You may find, you know, if you – there's a command called vmstat on Sun OS and which allows you to check how many pages were scanned by the clock algorithm on operating systems. So clock algorithm is a commonly used algorithm inside operating systems. And you can check what are the statistics of your clock algorithm on your operating system, which by using this command on some operating systems that tells you what are the pages scanned by the clock per second. So, for example, you know, here is some example data. So on a big machine – big machine means lots of memory – the slightly old data, but let's say, you know, roughly 200,000 pages examined, six revolutions of clock hand and 120K pages freed, roughly. So, you know, the six revolutions basically shows that the clock hand is moving relatively moderately. Memory pressure is small – is not that much. On the other hand, if you have a small machine, you know, here is another piece of data. You could have something like, let's say, 15,000 revolutions per second. This basically shows that your memory is basically too small and you basically need to increase your memory to reduce your memory pressure and so that your clock hand moves slowly. Okay. All right. So let's stop here and continue the discussion next time."}