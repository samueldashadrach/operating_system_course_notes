{"text":"We welcome to operating systems lecture 4. In the last lecture, we looked at Unix signals. Signals is another way of doing inter-process communication or communication from the OS to the process as we saw and useful if the communication indicates a rare event, a relatively rare event for which the process may not be ready and so it is delivered to the process in a very asynchronous fashion. You can think of it like there is a process that is running and another process wants to bring, to send something to another process. The process P1 wants to send some message to process P2 and process P2 may not be expecting that message. Process P2 just does not think that it is worthwhile to keep trying to pull for that message every few, you know, at periodic time intervals. So one way to design your processes would be that you basically install a signal handler in the receiving process and in the sending process, you send something. Let us say you send something to the pipe and then you send a signal to the receiving process. The signal handler of the receiving process is going to read from the pipe and so in that sense, you have an event-driven message transfer between the sender and the receiver and the same concept can apply for communication between the OS to the process and various signals we saw last time like interrupt, segmentation fault, et cetera, et cetera are ways of doing it. So it is another way of inter-process communication which is useful in settings where the communication is relatively rare and it needs to be handled in an event-driven fashion. Okay, so we saw two system calls, signal which installs a signal handler and kill which allows you to send a signal to another process. So in fact, when you, for example, type control C on the shell, what happens is that the shell parses the control C command and makes a kill system call to send sigint to the process that is currently running in the foreground of that shell. We also looked at, you know, we also talked about this concept of zombie processes. So we have seen that the two system calls, there is exit and there is wait. A process can call exit, but whatever is the exit code that is returned by the process should be communicated to the parent and the parent can read that through the wait system call, right? Now there is this small dilemma that if the process has exited and the parent hasn't called wait, what should the OS do, right? The OS has no idea when the parent may call wait. The parent may call wait one second from now, the parent may call wait 10 years from now or the parent may actually never call wait, right? So all these are possibilities and the OS should maintain its semantics in all these possibilities. So what the OS needs to do is it needs to store this return value somewhere in its data structure so that when the parent calls wait, it can serve it to it, right? And so this extra state of storing this information of the process, of the exited process, a process that's no longer living, is extra overhead on the OS. And so these kind of processes which are not really living, but some of the state is still living in the OS are called zombie processes, right? And we also said that zombie processes are a very common programming bug because programmers often forget to call, you know, wait in the parent. They just spawn a process, something exits. And what happens is eventually, although the state is very small, eventually the state is going to keep leaking. And so at some point in future, your OS data structures may actually find very little space for themselves, right? Then we also talked about the slash proxy to file system. Here's an example of a file system which is actually not a file system. It's not, it doesn't have real files. It actually just shares namespace with the file system, which means it has, you know, it has a name which nests inside the file system namespace. A process can call open, read, write, close on these names, just like it can call open, read, write, close on names of files. And the OS can use this file system to expose its own data structures to the process, right? So for example, the OS can expose the list of processes or, you know, the process, the source consumption of the processes, or even allow the application to actually set values in the kernel, in the OS's data structure. I've been using the term kernel and operating system interchangeably. They mean roughly the same thing in this discussion. The kernel is the real operating system layer that's implementing all these system calls. Okay, so we looked at the slash proc file system. And then we also looked at, and then we also said that, you know, we talked about processes, which are relatively isolated. And we understand the utility of having processes, because one program does, one program writer doesn't need to be worried about another program writer. And you also have trust boundaries. One program does not need to trust another program, and so on. But then, you know, inter-process communication becomes relatively expensive, because it involves system calls and going through the kernel. Often, you don't need this level of protection. And so what you could instead do was, instead of use processes, use shared address space and allow multiple threads of control within the same address space. And so these are called threads. And so threads can communicate between each other in a very fast way, because they share memory. One thread can write, another thread can read. And so it absolutely does not involve the kernel at all. Some examples of applications where you use threads are, let's say you have a browser, and you open multiple tabs inside the browser. You know, all the code of the tab, each of the code is actually doing separate things simultaneously. But this code is sort of mutually trusted, so you don't really need to have separate silos for each tab. They can actually, you know, run in the same address space. That way, they consume less memory, number one. Number two, if they need to communicate with each other, for whatever reason, let's say statistics, then they can do it in a very fast way. But what this imposes as a burden on the programmer is that the programmer should be aware that when, you know, when one thread is running, another thread could be concurrently running on the same memory. And so there should be safety in that sense. And we're going to talk about, you know, what kind of safety and how it's ensured later. And that's a very interesting topic in itself. We also said that there are two types of threads. There's user-level threads, where the operating system is completely blind to what's happening. It actually just looks at a process. But inside the process, the user can actually orchestrate his logic in such a way that it appears that there are multiple threads, and each thread can now appear to run concurrently. There may not be physical concurrency, but there's logical concurrency when you're doing user-level threads. And we also said that actually, you know, the system calls that we've discussed so far are enough for the user to be able to implement this layer of extracting one process into multiple threads. And this will be part of your homework problem next week. So you'll see how this is done, for example. All right. Then the other type of threads are kernel-level threads, where you tell the OS that, yes, I need threads. And, you know, the OS understands threads. It basically gives you one address space and allows you multiple threads of control. And that allows you full physical concurrency. If there are multiple CPUs, then these threads can be scheduled simultaneously physically. And also, you know, one thread could be waiting on the disk, and another thread could be waiting on the network, and yet another thread could be running on the CPU. All right. So we saw all this. Yes, question. Okay. When you switch off the machine, do the zombie processes get killed? Yes. Right. So the semantics of an operating system typically are that when you switch off the machine, all memory state is wiped out, all process state is wiped out. It's only the disk contents that remain. And we are going to look at, you know, even the disk contents, there are D guarantees on what remains and what doesn't remain, and we're going to discuss that when we are talking about file systems. Question. Okay. Interesting question. So in the event that the parent doesn't call wait on child, and the parent exits before the child has called exit, then what should happen? Should the child remain as a zombie process? Or should it be freed completely? Right. So, I mean, an OS typically what it does is, if a child loses its parent, then it's an orphan process. And that orphan process is actually attached to one of the default processes, which is called the init process. And so, you know, if an orphan process parent is the init process. And so the init process is, you know, the code of the init process typically would just keep calling wait over and over again to clean up all the zombies. Right. So that's, that's one way of handling it. Okay. Right. So, so far, we have looked at the OS. And we have looked at what kind of abstractions does the OS provide. Now, I'm going to change gears. And I'm going to look at how does the OS provide these abstractions. And yeah, I mean, it's going to be a relatively long discussion. But we're going to start with looking at what kind of interfaces does the hardware provide to the OS. Right. So let's look at the other side of how the OS works. You know, so let's start by looking bottom up. And let's look at what the hardware provides. Right. And I'm going to start. So we're going to have a discussion on the PC architecture, or PC hardware. And in particular, I'm going to discuss the x86 architecture. Now, there are many architectures, the many machine architectures that have evolved over the years. And Intel's x86 is one of the very popular architectures. And the reason I discuss it in this class is because your programming assignments are based on this architecture. And moreover, most of our machines are actually that we use based on Linux or Windows are usually based on this architecture. So it's also a good practical experience of what what the x86 architecture looks like. As a word of warning, the x86 architecture is very complex, much more complex than it needs to be. So we are going to slowly dig through that complexity. But but I would like you to pay attention because so that you understand, understand this stuff. And that will really help you through your programming assignments, for example. All right. Okay, so let's first understand what a machine looks like, what what a computer system looks like. So here's the CPU is the memory. CPU has a cache, let's say. Right. And let's say this is the bus, the system bus, basically. And then there are, you know, other buses attached to this bus, which are, let's say, attached to devices which are like disk, network, card. So so a CPU is basically a processor. It's like, you know, it's a silicon chip that has circuitry built into it. The nice thing about having a small silicon chip is that, you know, operations within this chip are very fast. Right. So you can read and write within the chip in a very fast way. You can execute logic inside the chip in a very fast way. So you have all these gates, all these logic gates inside the chip, which are executing things that you want to execute. And and then, you know, there's the bus. This is a this is a high speed bus. You know, that's a 10 to 100 gigabits per second. It's connecting to memory. And so the chip can only have that much storage space because, you know, that's the that the physical that's a limitation of the physical technology. And so but if you allow slightly slower storage, which is, you know, typically the kind of technology used for physical memory technologies, let's call DRAM, then you could actually so you could have larger sizes of this memory. And these sizes could range anywhere from, you know, a few hundreds of MBs to now even a few hundreds of GBs, for example, right. So memories are actually constantly increasing size in a huge way. And so the bandwidth to the memory is actually dictated by this bus. And so the CPU can can actually say that can send a message to the memory on the bus saying, give me the contents of address X. And the memory will reply on this bus saying here the contents of address X. So this is a read operation, write operation similar, the CPU can say, change the contents of address X with the with data D. And so, you know, and then they are the memory has certain semantics that if you write to address X data D and you read it later, you're going to get the same value that you wrote. So those are semantics of memory. And they are implemented in a fast way using using using different technologies. One of the popular technologies is using DRAM. All right. And similarly, there is other devices. So the CPU has ways of sending commands to device and the device has ways to send results back to the CPU. OK, so now let's look inside the CPU. So let's look at the CPU. So what does the CPU look like? The CPU has some integer registers. So these are, let's call them registers. What are registers? Registers is just storage inside the chip. It's very fast storage, much faster than DRAM. You can access it as at let's say nanosecond latency. All right. Then you have, you know, let's say these are integer registers and then you have floating point registers. And you have, you know, a memory management unit. MMU. What is an MMU? An MMU is basically an instruction. So the CPU wants to access memory. The memory management unit sits between the instruction that wanted to access memory and the actual physical memory and does some sort of bookkeeping or translation or access control checks. So, you know, an instruction wants to access certain physical memory. The MMU is going to do some checking or some translation before it actually sends an address to the bus. So some translation between what the instruction says I want to access and what the actual address goes on out to the bus for the memory to read. All right. That's MMU. All right. And then, you know, modern CPUs actually have multiple of these on a single chip. Right? So, you know, when you hear about multi-core architectures or dual core processors, et cetera, it's one chip which has multiple of these inside one. And now you also have some logic to basically be able to communicate between these processors. So I'm going to skip that for now and let's just talk about a single processor to make things simple. Of course, in this picture, you know, what else remains is basically, you know, there's a keyboard, mouse, et cetera. The way typically you would organize the system is you will look at the bandwidth requirements of that particular device. So the bandwidth requirement for memory is typically very high because you may want to read, you know, megabytes of data at once and you want that to happen as fast as possible. Bandwidth requirements for keyboard are relatively very low because, you know, how frequently can a user really press a key? It can at most be, you know, at millisecond, you know, one key per millisecond at most. So you don't need that much bandwidth to the keyboard. So you would probably have a very thin bus to connect to the keyboard or mouse for that matter. For something like a network card, you know, it really depends on what kind of network technology you're using. You know, earlier network technology used to be 100 megabytes. So the bus inside the CPU didn't need to be that fat to connect to the network card. Now you have 10 gigabits or even 40 gigabits Ethernet. So, you know, the buses inside the CPU also need to be broadened appropriately. The disk. The disk typically, you know, is a mechanical device. You know, when you actually see the disk moving, it's actually moving a spindle. And we're going to talk about how a disk works later. And some mechanical devices don't have that much throughput anyway. So, you know, typically you don't need that fat a bus to a disk, let's say. So, I mean, a hardware designer is going to do all this engineering and going to say that, OK, this is how I'm going to maximize my throughput for the devices that need throughput and not maximize the throughput for the devices that don't need throughput. OK. All right. So we looked at the state inside a CPU. Now, what is the logic of a CPU? So the way CPUs have evolved is basically that CPUs have, you know, divided the logic of computation into hardware and software. Right. And so hardware implements what are called instructions. And software implements, executes these instructions or sequences these instructions to implement its logic. So the hardware design is basically implementing instructions and the software design is writing these instructions. And the logic of a CPU is basically an infinite loop. If I was to write it in C, let's say, is just, you know, an infinite loop for forever, run the next instruction. It just says, what's the next instruction? Run it. What's the next instruction? Run it. That's the logic of a CPU. And the software is basically a sequence of instructions that needs to be done. And when it puts it in the CPU, the CPU starts running those instructions. Yes. OK. So how are these instructions actually implemented in hardware? Are they implemented using logic gates and finite state machines? Or is some other sort of software sitting inside the hardware that's actually emulating these instructions? Both are possibilities. There are time and space tradeoffs in doing one or the other. And both approaches have been tried. But let's for now assume that these instructions are implemented using logic gates and finite state machines. Right? OK. All right. OK. So now let's concentrate on the x86 architecture. So x86 architecture has four data registers. They're called AX, BX, CX, and DX. These are 16-bit in width. So I'm actually talking about the 8086 architecture. That was perhaps the first generation of these chips that we are using today. So let's say roughly in the early 1980s or something. And so this architecture had, at that time, actually operating on 16-bit values. And it was felt that operations on 16-bit values is enough for most purposes. And so this chip was basically a 16-bit chip. What does it mean for a chip to be a 16-bit chip? It basically means that the register's width is 16 bits. And the instructions that execute are also going to execute on 16-bit values. Also, they say that most, there are many instructions that only need to execute on 8-bit values. So they actually divided each register into two 8-bit halves. And they also allowed instructions to name them separately. So each of these 8-bit halves was called ALAH, BLBH, CLCH, and DLD. An instruction could choose to run on the full 16-bit value. Or an instruction could actually choose to run on an 8-bit value. And the 8-bit value could be named using one of these identifiers, DLDH. And so you could do that. So this was imagined to be primarily for integer computation, like whatever you want to do, add, subtract, multiply, et cetera. And then you also want some registers to be able to access memory, to generate addresses for memory. And so there were four more registers that were present in the 8086 architecture. And they were called BP, SP, BP, SI, and DI. These were also 16-bit. The idea was that these registers could perhaps primarily be used for providing addresses to memory. And the naming was also suggested. SP stands for Stack Pointer. BP stands for Base Pointer. I'm going to discuss what a Base Pointer means, really. And SI and DI stand for, I think, Source Index and Destination Index. In any case, these are four registers. And in fact, the way they designed the instructions, they could operate on any of these registers equally. Apart from this, there was also another register which is called the Instruction Pointer. Or let's call it, it's also commonly known as the Program Counter. Instruction Pointer or Program Counter, same thing. This is also 16-bit. This is also 16-bit. It's a register in the CPU. And the semantics are that when I showed you that loop, that it's going to run the next instruction in an infinite loop, how does it know what's the next instruction? It's just going to send the value of IP. Perhaps the value of IP is going to go through the memory management unit. The address is going to go to the bus. The memory is going to serve the contents of that address. And that's the instruction. That contents of that address is basically the instruction that gets executed. So in other words, the program itself will live in memory. This was done for simplicity. There can be other schemes where the program lives somewhere else. So there have been architectures that have separate memory for instructions and separate memory for data. And so they optimize path for instruction execution and they optimize the path for data execution separately. But here, the same memory is holding both instructions and data. What is the Wave Pointer? Let's just wait. We're going to talk about it very soon. The other semantics of IP is that it gets automatically incremented on every instruction execution. You execute an instruction, IP gets incremented appropriately to point to the next instruction. So that's also part of the logic of the CPU so that you don't have to have a separate software instruction. That would not be possible. To be able to do this for loop, you basically want that the semantics of the instruction pointer gets incremented on every fetch of the instruction. What if you wanted to access an instruction that lives inside memory, which is greater than 2 to the power 16? In the scheme we have discussed so far, it doesn't seem possible because the address is only 16 bits. And so you can't have more than 2 to the power 16 bytes in memory, assuming a byte-addressable memory. So let's for now just assume that the maximum amount of memory that you can have is 2 to the power 16. But actually it's not so. You know, 8086 actually allowed bigger memory than how it allowed so. You know, discuss later, but basically it's the MMU, the memory management unit that allows you to do greater physical memory. All right. OK, good. And also the IP or the IP gets incremented on every instruction executed on every execution of an instruction. Plus, there are special instructions that actually can manipulate the behavior of IP, like the jump instruction, or the jump conditional instruction, or the call instruction, or an indirect jump instruction, or a return instruction. So these are all different flavors of instructions that allow you to be able to manipulate the instruction pointer. There is no instruction on x86 that can actually just allow you to write to IP directly. If you want to write to IP, you should, if you want to say write value x to IP, the way to do that is to have the instruction jump x. Right. So IP is different from all the other registers in that sense. OK. One more thing. There's another register inside x86 which is called the flag register. This is also a special register. You cannot just read or write to the flag register like you can read or write to the other integer registers. But it stores information like whether the last information, last arithmetic operation you performed overflowed or not. Right. So there are semantics to instructions. For example, if you said add ax and dx, if the result overflowed, then a flag in the flag register, one bit in the flag register will get set. Right. Similarly, it stores information like whether the last computed result was positive or negative, whether it was zero or not, or whether there was a carry or a borrow in an add or subtract. It also has system level information, like whether interrupts are enabled or not. Right. So we're going to talk about that very soon. And so what happens is the way you would actually do control flow typically is that you would, let's say, execute some kind of an arithmetic instruction. Let's say you executed subtract. And you said subtract ax dx. Right. So I'm using some syntax here. This is the opcode. You must have seen this in your computer architecture class. The percentage sign here means that I'm talking about a register. And whatever follows the percentage sign is the name of the register. So I'm saying, you know, the ax register and the bx register. And we are using a syntax called the AT&T syntax for x86. Right. There are two types of syntax for x86. What we're going to use in this class and programming assignment is the AT&T syntax. There's another syntax called the Intel syntax. If you read the Intel reference manual, you're going to find the Intel syntax. But this is, you know, this is more common. And this is what we're going to use in this course. Right. So the syntax says that every register needs to be prefixed with a percentage instruction. And it also says that the first argument is the destination. The first operand is the destination operand. And the second operand is the source operand. And so this instruction means, and there are only two operands for an instruction, for an opcode typically. And this basically means that replace the value in ax with ax minus bx. Right. And now if I want to say, you know, subtract bx from ax. And then if the result is greater than zero, then jump somewhere. Right. So the way it will happen is that this instruction is going to set a bit in the flag register. And then there is an instruction which says, let's say, jump less than. It basically says, if the negative bit in the flag register is set. So if the last operand was, if the last computed value is negative, then you jump to some, you know, some other next PC. So these conditional jump instructions are a way to read the flag register. And these arithmetic instructions are a way to write these flag registers. A software cannot just directly read or write the flag register. These are the only two ways to read and write the flag register. OK. Right. So. So, you know, the other instructions like add, subtract, move, et cetera, that do the usual thing of what their name suggests. All right. OK. So now let's come to the question that, you know, do 8086 processor, did 8086 processor support only 64 KB memory? So answer is no. Actually, they supported up to one megabyte memory. So one M. That's two to the power 20 bytes. And so, so the CPU architects needed a way to be able to address the physical memory using two to the power 20 bytes. And, you know, the architecture itself is 16 bit. So what they did was, you know, the MMU would take in a 16 bit value and generate a 20 bit value with the help of certain registers which are called the segment registers. Once again, I'm talking about a particular architecture that 8086. So how does this work? Apart from all the registers that we have talked about, there are also, so let's say, you know, there's ABCD, these are the integer registers, there's IP and there are flags. You know, there are also those ESP, et cetera. I'm going to write them. And then there are these segment registers that there are six segment registers on 8086, CF, code segment, DS, data segment, SS, stack segment. And then there are, and then there was another segment which is called the ES. You know, anything else you want to do, it's typically used for strings. So it's called, you know, it's usually used for the string operations. And we're going to talk about that. These registers themselves were 16 bits. And what MMU would do is, so any, if you want to dereference memory, you cannot just say that here is a register and I want to dereference the address at SP. You have to actually address the memory using a pair of the segment register and an address. And the address could be a direct address or a register address or whatever. So I would either say, you know, percentage CS, percentage IP. This is the full address that I want to access. And the semantics of this is that the value of CS, so when I put something in brackets, it basically means you dereference that. So I actually don't dereference it here. So the value of CS, that percentage CS, into 16 plus the value of IP. So that will be your total address. That will be your complete address that will actually go on the bus to the memory. So in other words, your MMU on 8086 was a very simple MMU, the memory management unit, which would take an address named by a pair of segment register and some integer, which could be the register, one of the registers, integer registers. And it will do this computation and send the address on the physical bus. 16. So basically, it only allowed up to one megabyte of memory. That's 2 to the power 20 bits. So 16 is 2 to the power 4. So you can imagine, you can see that this totally, this can address any byte in a space of 2 to the power 20. 2 to the power 16 into 2 to the power 4 is 2 to the power 20. So you can address any of the 2 to the power 20 bytes. Right. And plus, you know, there was some default thing. So for example, any dereference through the IP will always go through the CS. Right. So, you know, for example, when I say run next instruction, the run next instruction is not only dereferencing IP, it's actually dereferencing CS colon IP. Right. So whatever you have set up in CS, that's going to be used in your MMU. So the intention must have been that, you know, the code will live in a separate segment, and you're going to a separate region of memory. It's all uniform memory, but you are basically dividing this memory artificially using these segment registers. And you will basically say that CS holds the base of the area where you're going to store your program. DS holds the base of the area which holds your data. SS holds the base of the program which holds your stack. Right. And also there were default things like whenever you dereference an instruction, it's going to go through CS. Whenever you dereference the stack using the SP or BP pointer, then you're going to go through the SS segment, and everything else by default is going to go through DS. Right. And then there was some special instruction that will go through ES. Could we have used CS as a 4-bit register and perform the same operation? Well, a 16-bit register allows you to, you know, start anywhere. Right. So the code segment could start on the top of the 1 megabyte region. But if you have a 4-bit register, then, you know, you lose some freedom in where the base will start. And from a hardware designer's point of view, all my logic is anyway 16-bit. You know, having a special 4-bit logic is actually more costly than less costly. I'm just guessing. Yeah. Is there any specific reason? No. I mean, at that time, they just thought, you know, 1M is more than enough. So every memory will have a limit. Even today's memory, you cannot, I mean, the CPU will only support a memory up to this size. Right. And so whenever, you know, in the real world, you will always have limits. You cannot just have infinite memory. And at that time, 1M was thought to be enough. Of course, we know now that's not enough. And so the architecture has evolved. And I'm really going to talk about how it's evolved. It's basically, you know, it's evolved in a backward compatible way. All right. So, I mean, this was a little, I mean, not very clean, actually, because there's no protection. I can, you know, I can reference the same byte in physical memory using many different addresses. Right. I could set up CS in some way and IP in some way, or CS something, CS lower, IP higher, because the same byte could have multiple addresses in the scheme, because, as you pointed out, there's redundancy in the number of bits that are being used to address a certain address space. Right. 32 bits are being used to address a space of 2 by 20. So there's clearly redundancy in the way things are done. Right. Okay. All right. So it's a little complex. It was also a little difficult to program this kind of a thing, because, you know, we are used to writing programs where we talk about variables. And now it's the job of a compiler to actually translate those variables into memory addresses. And now the compiler has to worry about where this memory address is going to be allocated, whether it's DS or SS or CS. And also it has to worry about, oh, there should not be any overlap. So, you know, CS plus some address should never be able to reach this variable. No. So all these things make life very difficult for the software writer, or in particular, the compiler writer, if you're using a compiler. All right. So clearly, 16 bits was painfully small. Quickly, this was realized, you know, in a few years' time that, you know, this is not going to work. We need to extend it. But, you know, what they also needed to do was provide backward compatibility. So there's some software that's been written for the 16-bit architecture. You know, there's been OS, there's some OSes that have been written for the 16-bit architecture. And, you know, they have invested in the company. They have bought the processor. They've written software for that processor. And now if the processor gets changed, then, you know, they will try. So what they do is they provide backward compatibility. And the backward compatibility means that when the processor boots, when you power it on a processor, it actually always boots, starts in the 16-bit mode. So if an OS was written for 16-bit, it will just, you know, happily run on the 16-bit mode. And then you provide certain special instructions, which were actually not present in the 16-bit processor, to switch to 32-bit mode, right? So if an OS is actually a 32-bit OS, it will start in the 16-bit mode. And very soon, it will call an instruction to switch to 32-bit mode. And now it can run in 32-bit mode. However, if there's an OS which doesn't understand 32-bit, only understands 16-bit, it can still happily run on the old processor, right? And actually, this has continued. And to this day, you know, when we are, when we have our chips, which are 64-bit, we still, I mean, it boots in the 16-bit. And it's the job of the OS writer to, you know, the OS writer is aware that it will always boot in the 16-bit. And now it will execute some code in the 16-bit mode, call a special instruction to switch to 32-bit mode. Then it will execute some code in the 32-bit mode. Then it will call a special instruction to switch to the 64-bit mode. And that's, you know, finally, I can, I can run my applications, right? So, so the architecture developed from 16-bit to 32-bit to 64-bit, and to provide backward compatibility, it will, it still boots in 16-bit. And then, and the OS has to switch it from 16 to 32 to 64 in that way, right? So that older OSs can still run. So, for example, the 32-bit OS can still run on a 64-bit processor. And the reason is basically because it, it still boots in the 16-bit mode and then 32-bit. All right. Okay. So how did the, how did the 32-bit architecture change? So the 32-bit series was, let's say, you know, 80 something, 386, 186, 286, et cetera. And they basically made all these registers 32-bit. So, you know, this was extended by another 16-bits from 32-bit. And, and the new name of this register is EAX, extended AX. Similarly, this becomes EBX, ECX, EDX, ESP, EBP, ESI, EDI. So all these registers get extended by by an extra 16-bits and they are renamed to ESP, EBP, et cetera. They still allow you to access the lower 16-bits of a register by using the old name. So you can still say AX, and that will mean the lower 16-bits of EAX, right? Or you can say SP, and that will mean the lower 16-bits of ESP, right? And now they changed all these instructions. They changed all these instructions to, with the same opcodes, to mean the same thing, except that they will now operate on 32-bit values, right? So subtract basically means subtract EAX, EBX, right? Add, move, whatever, right? They all basically become 32-bit, they've become 32-bit versions of themselves, right? So the same opcode, the same encoding of an instruction becomes a 32-bit version of itself. If for some reason you want to still access a 16-bit region, a 16-bit value inside the register, you can prefix an instruction with a special prefix. Let's say the prefix is some byte, let's say 0x66. It basically says that treat this instruction as a 16-bit instruction as opposed to a 32-bit instruction. So these are just some things that the designers did to be able to maintain both worlds. By default, everything becomes 32-bit, but if you want to use 16-bit, you can prefix an instruction with this special prefix and that instruction now behaves as though it's a 16-bit instruction, right? And similar things are actually true about 32-bit and 64-bit. So for example, if I say 0x66, add, and let's say, you know, I specified, so there are some number encoding for each register, right? For example, AX, the encoding for AX is 0, and the encoding for BX is, let's say, let's say 1, hypothetically, right? So it'll say, you know, add register 0 to register 1, right? And if I don't use this prefix, this encoding means add EAX to EBX. If you use the prefix, it basically means add AX to BX, right? That's the only difference. And there are actually two prefixes, 0x66 and 0x67, which stand for, you know, this basically says I want to change the size of the data. So data is basically the value that you're operating on, for example, AX or EAX. And 0x67 basically means I want to change the value of the address. So for example, if I wanted to dereference memory, I could actually do something like move %AX to %EBX. Oh, did I say that the first operand is the destination? Sorry, so the second operand is the destination, all right? So in the syntax of AT&T, the second operand or the last operand is the destination. Correction. All right, so when I say move EAX to EBX, it's basically going to read the value in EAX, dereference it, which means it's going to send that address to the MMU, generate a physical address, ask the memory to fetch its contents, and the contents are going to be stored in this register called EBX. So that's what's going to happen, right? If I prefix it with 0x66, what's going to happen is the address still remains 32-bit, but the data becomes 16-bit, right? So I still use 32-bit to dereference memory, to address memory through my MMU, but the number of bytes I fetch from that address is only 2 or 16 bits, right? So that's what the prefix is 66 means. It's basically referring to the size of the data. So if you prefix it with 0x66, let's say, then you're going to, you're basically changing, toggling the size of the data on which this instruction is operating. On the other hand, if I use 67, I'm toggling the size of the address, right? In this case, the address is EAX. So now this instruction is going to become move EAX, and if I don't have 66, then it's going to say move AX to EBX. So it's basically saying dereference using AX, but the data still remains 32-bit, right? And if I use both, then both become 16-bit, right? So there are two things. Every instruction will have an address component to it and a data component to it, if it accesses memory. And so these two prefixes can toggle either or both. All right. Okay, yeah, so a lot of detail, but good to know once and then, you know, be comfortable about it forever. All right. Let's also understand, you know, how an assembler works. So, for example, when you write code, when an OS is going to write code, it's going to write some assembly instructions. It's going to say move something, something, and so on. So you can tell the assembler that, look, this part of the code is, compile this code as 16-bit code. And then, you know, you can tell the assembler, so there's a directive called, let's say, code 32. And then you can tell that all future codes should be compiled as 32-bit code, right? So, for example, if there's an instruction called move AX BX here, and there's another instruction called move AX BX here, here I could just use the encoding of this instruction without having to prefix it with anything, right? But here, if I want to use the same instruction, I need to prefix it with 0866, let's say, right? Because this is 32-bit mode. So this directive is a way to tell the assembler that in what mode should you compile this code. Because the same string can mean different encodings in different modes. Okay. All right. So we'll look at 32-bit architectures for our programming assignments and most of our discussion, but, you know, seems, already seems very complex. I don't want to discuss 32-bit or 64-bit. I don't think you want me to discuss it either, right? But you can imagine it's a similar kind of complexity that's going to occur in doing that also. All right. Okay. Let's look at some instructions, some real instructions. So here is the AT&T syntax. Let's say, move L, percentage EAX to percentage EDX. Let me write the C-ish equivalent or, you know, semantics of this instruction in a language we understand, C, right? So let's say, so what this basically means is EDX is equal to EAX, right? Just to understand what this means. If you don't understand the syntax, let me just explain it to you. I could say, move $123 to EDX. $123 means it's a constant. It's an immediate value, right? It's an immediate operand. You must have seen immediate operands in your computer architecture class. And this basically means EDX is equal to 0x123, or the hexadecimal number 123. Notice the use of the dollar sign to specifically say that this is a constant that I want to move to EDX. On the other hand, if I say move, so I'm also prefixing it with a character which says what's the size of the data I'm using. So by default on a 32-bit mode, it's 32-bit, but you can also specify it using L. So L stands for 32-bit. So you're basically moving 32-bit values, right? You could also say move L 0x123 to %EDX. Notice the difference between this and the previous one is that I'm not using the dollar sign, right? And what does this mean? Yeah, so treat 123 as an address for the memory, right? So it's basically saying EDX is equal to dereference first typecast 123 as a pointer that points to a 32-bit value, because it's a 32-bit instruction, and then dereference that pointer. And whatever result you get, put it in EDX. Right? Just dereferencing EDX, or dereferencing 123. Okay, if I say move L, so this is immediate addressing. I could also say move L EDX to EDX. This is indirect addressing, basically means EDX is equal to N32 star 32, just to specify that's a 32-bit number, N32 star X, right? Basically saying, look at the value in EDX, use it as an address to memory, dereference it in memory, and fetch the contents and store it in EDX. Okay. And finally, I could also say something like move L for EDX to EDX. So x86 allows you these kind of instructions where you can specify an offset with a register. So basically means that you add 4 to EDX, and then you dereference it, and then you get the contents and put it in EDX. So basically means EDX is equal to star EDX plus 4. Question? What is the difference between the prefix 0x66, 0x67, and the suffix L in my assembly code? This is assembly code, all right? So I can use, you know, I'm specifying assembly code using strings, where I specify an opcode using a string, and I use a suffix L to indicate whether it's a 32-bit or a 60-bit. The 0x66, 0x67 is for the instruction encoding in binary, right? So the assembly, the assembler is going to convert the string into prefix or un-prefix versions of instruction encodings, right? Okay, so I'll stop here."}